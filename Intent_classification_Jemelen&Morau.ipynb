{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilienJemelen/NLP_project2023_MORAU-JEMELEN/blob/main/Intent_classification_Jemelen%26Morau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING PACKAGES"
      ],
      "metadata": {
        "id": "tUStaRWZ7xh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install xmltodict\n",
        "!pip install tokenizers\n",
        "!pip install torchinfo\n",
        "!pip install datasets\n",
        "!pip install datasets\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "from torchtext.data import get_tokenizer\n",
        "import transformers\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import pickle\n",
        "import xmltodict\n",
        "import xml.etree.ElementTree as ET\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torchtext\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from itertools import islice\n",
        "import torch.optim as opt\n",
        "from IPython.display import clear_output \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from gensim.models import FastText\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils as utils\n",
        "import re\n",
        "from nltk import tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytD6OWiP71Ea",
        "outputId": "32b4b586-0bfa-4afb-e601-93130f1dec1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.9/dist-packages (0.13.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.9/dist-packages (0.13.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.9/dist-packages (1.7.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjAP-ugmMUkB"
      },
      "source": [
        "## SECTION 1 / LOADING OPENSUBTITLES DATA AND TRAINING OUR CUSTOM TOKENIZER"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data we have are .zip formatted (from https://opus.nlpl.eu/OpenSubtitles-alt-v2018.php), and .xml at the lower level, so some formatting is required (cells below)"
      ],
      "metadata": {
        "id": "RWsvEuxA8ixK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eGbnbMprAcH"
      },
      "outputs": [],
      "source": [
        "# creating folder for data storage\n",
        "try:\n",
        "  os.mkdir('./opensbtitles_files')\n",
        "except FileExistsError:\n",
        "  print('folder exists already')\n",
        "\n",
        "\n",
        "# loading the .zip and creating a zip object\n",
        "with ZipFile('./drive/MyDrive/fr.zip', 'r') as zObject:\n",
        "    # Extracting all the .xml of the zip into a specific location.\n",
        "    zObject.extractall(\n",
        "        path=\"./opensbtitles_files\")\n",
        "    \n",
        "# making a list of all the .xml files paths\n",
        "general_folder_path = './opensbtitles_files/OpenSubtitles/raw/fr/'\n",
        "xml_files_paths = []\n",
        "for subfolder_path in os.listdir(general_folder_path):\n",
        "  for subsubfolder_path in os.listdir(general_folder_path + subfolder_path):\n",
        "    for file_name in os.listdir(general_folder_path + subfolder_path + '/' + subsubfolder_path):\n",
        "      xml_files_paths.append(general_folder_path + subfolder_path + '/' + subsubfolder_path + '/' + file_name)\n",
        "\n",
        "\n",
        "# defining function to save as pickle at path\n",
        "def save_data(data, path):\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "\n",
        "# initializing dictionaries to store the data:\n",
        "try:\n",
        "  xmltree = ET.parse(xml_files_paths[0])\n",
        "  xmlroot = xmltree.getroot()\n",
        "  #defining an xml string\n",
        "  xml_data = ET.tostring(xmlroot)\n",
        "  #converting xml to dictionary\n",
        "  doc_dict = xmltodict.parse(xml_data)\n",
        "  dictionaries = doc_dict['document']['s']\n",
        "except ET.ParseError:\n",
        "  print('corrupt file, skipping it')\n",
        "\n",
        "# Due to the size of the data and our limited storage + computation capacities,\n",
        "# We will only training tokenizer on a certain proportion of the whole data\n",
        "propor = 1/2\n",
        "for k in range(1, int(len(xml_files_paths)*propor)):\n",
        "  \n",
        "  try:\n",
        "    xmltree = ET.parse(xml_files_paths[k])\n",
        "    xmlroot = xmltree.getroot()\n",
        "    # defining an xml string\n",
        "    xml_data = ET.tostring(xmlroot)\n",
        "    # converting xml to dictionary\n",
        "    doc_dict = xmltodict.parse(xml_data)\n",
        "    dictionaries = dictionaries + doc_dict['document']['s']\n",
        "  except (ET.ParseError, TypeError) as error:\n",
        "    print('corrupt file, skipping it')\n",
        "\n",
        "  if int((k/int(len(xml_files_paths)*propor))*100) != int(((k-1)/int(len(xml_files_paths)*propor))*100):\n",
        "    print(int((k/int(len(xml_files_paths)*propor))*100), ' %')\n",
        "\n",
        "for i in range(len(dictionaries)):\n",
        "  dictionaries[i]['@id'] = i\n",
        "\n",
        "save_data(dictionaries, \"./drive/MyDrive/opensbtitles_compiled_XXL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39wQ8XQccEBB",
        "outputId": "0d4886f0-523e-49c5-b5f9-9f5e6a58f8e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'@id': 0,\n",
              "  'time': [{'@id': 'T1S', '@value': '00:00:48,640'},\n",
              "   {'@id': 'T1E', '@value': '00:00:52,085'}],\n",
              "  '#text': 'Vous pensez : une pub pour la crème Noxema.'},\n",
              " {'@id': 1,\n",
              "  'time': [{'@id': 'T2S', '@value': '00:00:52,120'},\n",
              "   {'@id': 'T2E', '@value': '00:00:56,360'}],\n",
              "  '#text': \"Pourtant, j'ai une vie hyper normale pour une ado.\"},\n",
              " {'@id': 2,\n",
              "  'time': [{'@id': 'T3S', '@value': '00:00:56,395'},\n",
              "   {'@id': 'T4E', '@value': '00:01:01,273'}],\n",
              "  '#text': 'Je me lève, me lave les dents et choisis ma tenue.'},\n",
              " {'@id': 3,\n",
              "  'time': [{'@id': 'T5S', '@value': '00:01:01,720'},\n",
              "   {'@id': 'T5E', '@value': '00:01:05,565'}],\n",
              "  '#text': 'Sélection.'},\n",
              " {'@id': 4,\n",
              "  'time': [{'@id': 'T6S', '@value': '00:01:05,600'},\n",
              "   {'@id': 'T6E', '@value': '00:01:06,635'}],\n",
              "  '#text': 'Mal coordonné !'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "with open('./drive/MyDrive/opensbtitles_compiled_XXL', 'rb') as f:\n",
        "  dictionaries = pickle.load(f)\n",
        "\n",
        "dictionaries[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mduO16qkcNj7"
      },
      "source": [
        "PREPARING A LIST OF DICTIONARIES FOR MODEL PRETRAINING /\n",
        "\n",
        "> List of lists (each sublist represents a dialog) of dictionaries (one dictionary = one utterance)\n",
        "\n",
        "> Dialogs are defined as lists of utterances with less than 6 seconds between the utterances.\n",
        "\n",
        "> Concretely, 3 types of 'time' values in each utterance :\n",
        "\n",
        "- First type / 'time': [{'@id': 'T60S', '@value': '00:05:55,780'},\n",
        "   {'@id': 'T60E', '@value': '00:05:58,408'}]\n",
        "=> in this case, we compare the utterance with the previous one with the first time value and the utterance with the following one with the second time value\n",
        "\n",
        "- Second type / no time value\n",
        "We keep the utterance, since it's often a very short one\n",
        "\n",
        "- Third type (majority) / 'time': {'@id': 'T45E', '@value': '00:05:00,615'}\n",
        "We use the time value to compare with previous and following utterances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir1ulVS7cW0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548c5a10-4c16-4326-a7ff-280ef73b74cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Vous pensez : une pub pour la crème Noxema.',\n",
              "  \"Pourtant, j'ai une vie hyper normale pour une ado.\",\n",
              "  'Je me lève, me lave les dents et choisis ma tenue.',\n",
              "  'Sélection.',\n",
              "  'Mal coordonné !'],\n",
              " ['Ce sont les plus redoutés de la profession.',\n",
              "  'Même Lucy, la bonne, est terrorisée par lui.',\n",
              "  \"Il gagne 500 dollars de l'heure à se disputer avec les gens.\",\n",
              "  \"Avec moi, c'est gratuit !\",\n",
              "  'Papa.',\n",
              "  '- Cher, arrête avec ton jus.',\n",
              "  '- Tu as besoin de vitamine C.',\n",
              "  'Où est ma mallette ?',\n",
              "  \"Depuis le temps, j'ai dit qu'on irait à Malibu.\",\n",
              "  '- Ces demeurés ont encore appelé ?',\n",
              "  '- Ce sont tes parents.',\n",
              "  'Dr Lovett passera au bureau te vacciner contre la grippe.',\n",
              "  '- Josh vient dîner.',\n",
              "  '- Pourquoi ?',\n",
              "  \"C'est le fils de mon ex.\",\n",
              "  'Un mariage éclair il y a 5 ans !',\n",
              "  '- Pourquoi je verrais Josh ?',\n",
              "  '- On divorce pas des enfants.',\n",
              "  '- Tiens.',\n",
              "  \"- N'insiste pas !\",\n",
              "  \"C'est ma jeep au moteur gonflé.\",\n",
              "  'Cadeau de papa.',\n",
              "  '4 roues motrices, double airbag, et une stéréo qui décoiffe.',\n",
              "  \"J'ai pas encore le permis mais je prends des cours.\",\n",
              "  \"J'avais pas vu.\",\n",
              "  'Dionne habite ici.'],\n",
              " ['Ma copine !',\n",
              "  'Là, chapeau pour ses efforts vestimentaires.',\n",
              "  \"On porte les noms de deux chanteuses d'autrefois aujourd'hui recyclées dans la pub.\",\n",
              "  '- Alors ?',\n",
              "  '- Cadeau du Chapelier fou ?',\n",
              "  'Moi, je ne tue pas les colleys.',\n",
              "  \"C'est de la fausse.\",\n",
              "  '- Et le stop ?',\n",
              "  \"- Je l'ai totalement respecté.\",\n",
              "  \"Ouais, d'accord !\",\n",
              "  'À peine 8 h 30 et Murray me bipe déjà.',\n",
              "  \"Qu'il est possessif.\",\n",
              "  \"Ce week-end, il m'appelle :\",\n",
              "  '\"Où étais-tu ?\"...',\n",
              "  \"Dionne et son copain, c'est l'amour tempête.\",\n",
              "  'Ils ont vu trop souvent le film sur Ike et Tina Turner.',\n",
              "  'Là, je lui dis.',\n",
              "  'Pourquoi supporter ça ?',\n",
              "  'Tu pourrais avoir mieux.',\n",
              "  'Je sais.',\n",
              "  'Le voilà.',\n",
              "  'Femme, réponds quand je te bipe.',\n",
              "  \"- Je hais qu'on m'appelle femme !\",\n",
              "  \"- T'as baisé en limousine ?\",\n",
              "  'Baisé ?',\n",
              "  'Non.',\n",
              "  'Parlant de baise motorisée, explique-moi ce que faisait ce postiche immonde dans ton auto ?',\n",
              "  \"Je ne sais pas d'où ça vient.\",\n",
              "  '- On dirait le tien.',\n",
              "  '- Je porte pas du polyester !',\n",
              "  'Pas comme une certaine Shawana !',\n",
              "  \"- Je m'éclipse.\",\n",
              "  '- Salut.',\n",
              "  'Ça suffit.',\n",
              "  \"J'en ai assez de toi !\",\n",
              "  'Encore ta semaine du mois ?',\n",
              "  'Je vois pas pourquoi Dionne sort avec un garçon du lycée.',\n",
              "  'De vrais toutous.',\n",
              "  'Il faut les toiletter et les nourrir.',\n",
              "  'Des bestioles qui vous sautent dessus en bavant.',\n",
              "  'Bas les pattes !',\n",
              "  'Non mais il rêve !',\n",
              "  \"L'Amérique doit-elle accueillir tous les opprimés ?\",\n",
              "  'Amber donnera les arguments contre, Cher les arguments pour.',\n",
              "  'Cher, deux minutes.',\n",
              "  \"D'accord.\"]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "voc = dictionaries.copy()\n",
        "dialogs = []\n",
        "dialog = []\n",
        "time_0 = voc[0]['time']\n",
        "if type(time_0) == list:\n",
        "  dialog.append(voc[0]['#text'])\n",
        "  time_previous = datetime.strptime(time_0[1]['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "if type(time_0) == dict:\n",
        "  dialog.append(voc[0]['#text'])\n",
        "  time_previous = datetime.strptime(time_0['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "\n",
        "  \n",
        "for k in range(1, len(voc)):\n",
        "\n",
        "  try:\n",
        "    time_k = voc[k]['time']\n",
        "    if type(time_k) == list:\n",
        "      time_k_value = datetime.strptime(time_k[0]['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "      if abs((time_k_value - time_previous).total_seconds()) <= 6:\n",
        "        dialog.append(voc[k]['#text'])\n",
        "      else:\n",
        "        dialogs.append(dialog)\n",
        "        dialog = []\n",
        "      time_previous = datetime.strptime(time_k[1]['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "\n",
        "    if type(time_k) == dict:\n",
        "      time_k_value = datetime.strptime(time_k['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "      if abs((time_k_value - time_previous).total_seconds()) <= 6:\n",
        "        dialog.append(voc[k]['#text'])\n",
        "      else:\n",
        "        dialogs.append(dialog)\n",
        "        dialog = []\n",
        "      time_previous = datetime.strptime(time_k['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "\n",
        "  except :\n",
        "    try:\n",
        "      dialog.append(voc[k]['#text'])\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "\n",
        "dialogs = [l for l in dialogs if l != []]\n",
        "with open(\"./drive/MyDrive/dialogs_XXL\", \"wb\") as f:\n",
        "        pickle.dump(dialogs, f)\n",
        "\n",
        "dialogs[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUjgDqw44yLM"
      },
      "source": [
        "MAKING A TOKENIZER SPECIFIC TO OPENSUBTITLES DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3de82a3ca99e44728071fadb22f8951c",
            "cfb2bed0049d474bbf4d67d256dda461",
            "a19b695adce140dd9feec386b04117fa",
            "15b18c9b05df4814ac4066ae4057e21b",
            "f0db876de8664c85a7f551f5fe9be03c",
            "e852bc1bafd246debf415d3e05364ba8",
            "d1c4540c55af40799b77b634e4cb8789",
            "e7e5fb29c6eb43f28c1a9bc51eaea4f0",
            "9c2077a27a1c4ddba981cb6c8e7e0e5c",
            "96d4b9dd59fd4efaa117fb3afb489eb1",
            "5e3368b5b602408ebb2515526483920c"
          ]
        },
        "id": "VBuiRLBb5ezL",
        "outputId": "452d7a13-339b-46af-859d-e856246303ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10576129 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3de82a3ca99e44728071fadb22f8951c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# in this cell we compile all the text we have and split it in smaller files to facilitate tokenizer training\n",
        "try:\n",
        "  os.mkdir('./textFiles4tokenizer')\n",
        "except FileExistsError:\n",
        "  print('folder exists already')\n",
        "\n",
        "text_data = []\n",
        "file_count = 0\n",
        "\n",
        "for sample in tqdm(dictionaries):\n",
        "  try:\n",
        "    sample = sample['#text'].replace('\\n', ' ')\n",
        "    text_data.append(sample)\n",
        "  except KeyError:\n",
        "    pass\n",
        "  \n",
        "  if len(text_data) == 5_000:\n",
        "    with open(f'./textFiles4tokenizer/file_{file_count}.txt', 'w') as fp:\n",
        "      fp.write('\\n'.join(text_data))\n",
        "    text_data = []\n",
        "    file_count += 1\n",
        "with open(f'./textFiles4tokenizer/file_{file_count}.txt', 'w') as fp:\n",
        "  fp.write('\\n'.join(text_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqRHl2mX9EL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1da500-e594-4a98-a2e7-974d4e8f6207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "folder exists already\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining/vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text = True, # removes unwanted characters\n",
        "    strip_accents = False, # Accents are important in French\n",
        "    lowercase = True # upper case letters are not really important in our case\n",
        ")\n",
        "\n",
        "general_folder_path = './textFiles4tokenizer/'\n",
        "paths = []\n",
        "for file_name in os.listdir(general_folder_path):\n",
        "  paths.append(general_folder_path + file_name)\n",
        "\n",
        "# training the tokenizer\n",
        "tokenizer.train(files = paths,\n",
        "                vocab_size = 50_000, # number of tokens that we can have\n",
        "                min_frequency = 2, # min frequency so that a sequence be taken into account as a token\n",
        "                special_tokens = ['[PAD]', # \"padding\" token\n",
        "                                  '[UNK]', # \"unknown\" token\n",
        "                                  '[CLS]', # \"classifier\" token\n",
        "                                  '[SEP]', # \"separator\" token\n",
        "                                  '[MASK]'], # \"mask\" token\n",
        "                limit_alphabet = 5000,\n",
        "                wordpieces_prefix = \"##\")\n",
        "\n",
        "try:\n",
        "  os.mkdir('./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining')\n",
        "except FileExistsError:\n",
        "  print('folder exists already')\n",
        "\n",
        "tokenizer.save_model('./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DDIS3CyAMAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "905d106b-fc8f-4b8d-a793-7f298f00109b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [2, 373, 488, 18571, 5, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining')\n",
        "tokenizer('Je suis enthousiaste !')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kijzgqGvCF4x"
      },
      "source": [
        "## SECTION 2 / DEFINING THE ARCHITECTURE OF THE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Creating our FastText model (compared to word 2 vec, FastText has the advantage for us of taking into account punctuation marks)"
      ],
      "metadata": {
        "id": "2Zhca8D5GMm5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLvrwmAF2aOf"
      },
      "outputs": [],
      "source": [
        "# only run this cell the first time ! then just LOAD the FastText model from Drive (next cell)\n",
        "sentences = []\n",
        "for dialog in dialogs:\n",
        "  for sentence in dialog:\n",
        "    sentences.append(sentence.split(' '))\n",
        "\n",
        "model = FastText(sentences, min_count = 3)\n",
        "model.save('./drive/MyDrive/FastTextModel/custom_FastTextModel')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading our FastText model"
      ],
      "metadata": {
        "id": "b2vgFAzCF3q9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l87CXMWyvfL1"
      },
      "outputs": [],
      "source": [
        "fastText_model = FastText.load('./drive/MyDrive/FastTextModel/custom_FastTextModel')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the embedder to feed input as embeddings of size 100 (small because of limited computation capacity)"
      ],
      "metadata": {
        "id": "GznM0NVtGgV5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSfMqjF4BJul"
      },
      "outputs": [],
      "source": [
        "def embedder(token_ids):\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_ids)    \n",
        "    return torch.stack([torch.tensor(fastText_model.wv[w]) for w in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming our tokenized data by padding the utterances and conv and building Dataset class"
      ],
      "metadata": {
        "id": "aLz-wo3Q6a3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenSubtitlesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      item = {\n",
        "          \"conversation\": self.data[idx]['conversation']\n",
        "      }\n",
        "   \n",
        "      return item"
      ],
      "metadata": {
        "id": "DvW3zalJysL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only run that cell the first time to pad dialogs ! then just run the next cell\n",
        "\n",
        "Padding dialogs (as token ids lists for the moment):"
      ],
      "metadata": {
        "id": "QyrUFch0VGo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/opensbtitles_dialogs4pretraining', 'rb') as f:\n",
        "  dialogs4pretraining = pickle.load(f)\n",
        "\n",
        "# we initialize max_size_of_utterance and max_dialog_size values:\n",
        "max_size_of_utterance = 0\n",
        "max_dialog_size = 0\n",
        "\n",
        "dialogs4pretraining_padded = dialogs4pretraining.copy()\n",
        "\n",
        "# computing maximum sizes for padding (dialogs WILL be sparse with many utterances set to [])\n",
        "for d in range(len(dialogs4pretraining_padded)):\n",
        "  if max_dialog_size < len(dialogs4pretraining_padded[d]):\n",
        "    max_dialog_size = len(dialogs4pretraining_padded[d])\n",
        "\n",
        "  for u in range(len(dialogs4pretraining_padded[d])):\n",
        "    if max_size_of_utterance < len(dialogs4pretraining_padded[d][u]):\n",
        "      max_size_of_utterance = len(dialogs4pretraining_padded[d][u])\n",
        "\n",
        "\n",
        "print('max dialog size : ', max_dialog_size, 'max size of utterance : ', max_size_of_utterance)\n",
        "# Comments : \n",
        "    # max_dialog_size is way too big for the RAM : we are going to have to cut longer dialogs\n",
        "    # max_size_of_utterance is reasonable, but since except a few utterances most of them are below 30 we cut to 30"
      ],
      "metadata": {
        "id": "X8e_uBe_7Fs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d844a730-14c6-466d-b733-65ccd33dff36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max dialog size :  1400 max size of utterance :  140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell only if padded dialogs do not already exist in Drive (else go to next cell)"
      ],
      "metadata": {
        "id": "RYmuQxnfrkYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_dialog_size = 10\n",
        "max_size_of_utterance = 30\n",
        "\n",
        "dialogs4pretraining_padded = dialogs4pretraining.copy()\n",
        "padding_utterance = [0 for i in range(max_size_of_utterance)]\n",
        "\n",
        "for d in range(len(dialogs4pretraining_padded)):\n",
        "\n",
        "  if len(dialogs4pretraining_padded[d]) < max_dialog_size:\n",
        "\n",
        "    # <PAD> token is n°0 for utterances\n",
        "    for u in range(len(dialogs4pretraining_padded[d])):\n",
        "      if len(dialogs4pretraining_padded[d][u]) < max_size_of_utterance:\n",
        "        dialogs4pretraining_padded[d][u] = dialogs4pretraining_padded[d][u] + [0 for i in range(len(dialogs4pretraining_padded[d][u]), max_size_of_utterance)]\n",
        "    # <PAD> token is [padding_utterance, padding_utterance, ...] for dialogs\n",
        "    dialogs4pretraining_padded[d] = dialogs4pretraining_padded[d] + [padding_utterance for i in range(len(dialogs4pretraining_padded[d]), max_dialog_size)] \n",
        "  \n",
        "  elif len(dialogs4pretraining_padded[d]) > max_dialog_size: \n",
        "    \n",
        "    # in this case we cut the dialog to max_dialog_size # of utterances\n",
        "    dialogs4pretraining_padded[d] = dialogs4pretraining_padded[d][:max_dialog_size]\n",
        "    # <PAD> token is n°0 for remaining utterances\n",
        "    for u in range(len(dialogs4pretraining_padded[d])):\n",
        "      if len(dialogs4pretraining_padded[d][u]) < max_size_of_utterance:\n",
        "        dialogs4pretraining_padded[d][u] = dialogs4pretraining_padded[d][u] + [0 for i in range(len(dialogs4pretraining_padded[d][u]), max_size_of_utterance)]\n",
        "\n",
        "with open(\"./drive/MyDrive/opensbtitles_dialogs4pretraining_padded_small\", \"wb\") as f:\n",
        "        pickle.dump(dialogs4pretraining_padded, f)"
      ],
      "metadata": {
        "id": "9WFw_vPDXe4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the autoencoder layers model"
      ],
      "metadata": {
        "id": "G4Db0MhuJDRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwWRNJimGZo5"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, nhead, dim_feedforward, vocab_size):\n",
        "\n",
        "        super(Model, self).__init__()   \n",
        "        self.first_layer_encoders = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)   \n",
        "        self.second_layer_encoder = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.decoder = nn.TransformerDecoderLayer(d_model, nhead)\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "\n",
        "  def forward(self, conversation, tgt):\n",
        "\n",
        "      conversation = [embedder(token_ids).to(device) for token_ids in conversation]                  #First each token of each utterance is embedded with word2vec\n",
        "      tgt = embedder(tgt).to(device)                                                                 #We do the same for the target, which corresponds to one given utterance\n",
        "\n",
        "      sentence_hidden_states = [self.first_layer_encoders(sentence) for sentence in conversation]    #Then each utterance is feeded into seperate transformer encoders in the first layer of the NN\n",
        "      corpus_hidden_states = torch.cat(sentence_hidden_states, dim=0)                                #We retrieve and concatenate the outputs\n",
        "      second_layer = self.second_layer_encoder(corpus_hidden_states)                                 #The hidden representation of the conversation is then feeded into one last transformer encoder\n",
        "      output = self.decoder(tgt, second_layer)                                                       #We decode the output of the previous layer using the target\n",
        "      pre_logits = self.linear(output)                                                               #Finally, we train a Dense layer to perform classification\n",
        "\n",
        "      return pre_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qZFPXMKcYk4u"
      },
      "outputs": [],
      "source": [
        "model_not_pretrained = Model(100, 10, 100, tokenizer.vocab_size).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a function which is going to mask part of conversation to allow the autoencoder to learn to predict parts of conversations"
      ],
      "metadata": {
        "id": "uEua7traJMna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxzPEbA6LjR_"
      },
      "outputs": [],
      "source": [
        "def masker(conversation, mask_ratio):\n",
        "  \"\"\" \n",
        "  This function is used to return a masked version of the conversation at the token level as well as a masked version at the utterance level\n",
        "\n",
        "  Input: \n",
        "       - conversation: the conversation to mask, as a list of a list of tokens (a list of tokenized utterances)\n",
        "       - mask_ratio: the probability to mask a given token / utterance\n",
        "  \n",
        "  Output:\n",
        "       - masked_input is the conversation masked at the tokens level (at least one word per utterance)\n",
        "       - masked_dialogue is the conversation masked at the utterance level (at least one utterance)\n",
        "       - target_conv is the original conversation\n",
        "       - masks is a list of the indices of the masked tokens for each utterance\n",
        "       - n_masked is the total number of tokens (including the tokens from the masked utterance) that were masked\n",
        "  \"\"\"\n",
        "  target_conv = []\n",
        "  masked_input = []\n",
        "  masks = []\n",
        "  n_masked = 0\n",
        "\n",
        "\n",
        "  #First, we mask conversations at the utterance level\n",
        "\n",
        "  #We do not want to mask padded utterance (it wouldn't be interested to decode them), so we start to identify relevant utterance\n",
        "  non_padded_indexes = [i for i in range(len(conversation)) if conversation[i] != padding_utterance]\n",
        "\n",
        "  #Then, using the mask_ratio as a probability, we pick utterances to mask fully\n",
        "  mask_uterance = torch.rand(len(non_padded_indexes)) < mask_ratio  \n",
        "  mask_uterance = [non_padded_indexes[i] for i in range(len(non_padded_indexes)) if i in torch.nonzero(mask_uterance)]\n",
        "\n",
        "  #If no utterance was picked (it can happen when mask_ratio is low), we make sur to mask at least one by picking it randomly\n",
        "  if len(mask_uterance) == 0:\n",
        "    mask_uterance = torch.randint(len(non_padded_indexes), (1, 1))\n",
        "    mask_uterance = [non_padded_indexes[mask_uterance]]\n",
        "\n",
        "  #Finally, we fully mask the tokens of the selected utterances\n",
        "  masked_dialogue = conversation.copy()\n",
        "  for index in mask_uterance:\n",
        "    masked_dialogue[index] = [tokenizer.mask_token_id for uterance in masked_dialogue[index]]\n",
        "    n_masked += len(masked_dialogue[index])\n",
        "\n",
        "  #Then, we mask conversations at the token level\n",
        "\n",
        "  for uterance in conversation:\n",
        "\n",
        "    #We do not want to mask padded tokens (it wouldn't be interested to decode them), so we start to identify relevant tokens for each utterance\n",
        "    non_padded_indexes = [i for i in range(len(uterance)) if uterance[i] != 0]\n",
        "\n",
        "    #Then, using the mask_ratio as a probability, we pick tokens to mask fully for each utterance\n",
        "    mask_indices = torch.rand(len(non_padded_indexes)) < mask_ratio  \n",
        "    mask_indices = [non_padded_indexes[i] for i in range(len(non_padded_indexes)) if i in torch.nonzero(mask_indices)]\n",
        "\n",
        "    #If no tokens was picked (it can happen when mask_ratio is low), we make sur to mask at least one by picking it randomly\n",
        "    if len(mask_indices) == 0 and uterance != padding_utterance:\n",
        "      mask_indices = torch.randint(len(non_padded_indexes), (1, 1))\n",
        "      mask_indices = [non_padded_indexes[mask_indices]]\n",
        "    n_masked += len(mask_indices)\n",
        "    input_tensor = torch.tensor(uterance)\n",
        "    masks += [torch.tensor(mask_indices).to(device)]\n",
        "\n",
        "    #Finally, we mask the selected tokens for each utterance\n",
        "    masked_input_tensor = input_tensor.clone()\n",
        "    masked_input_tensor[mask_indices] = tokenizer.mask_token_id\n",
        "    masked_input += [masked_input_tensor.to(device)]\n",
        "\n",
        "    target_tensor = input_tensor.clone()\n",
        "    target_conv += [target_tensor.to(device)]\n",
        "  \n",
        "  \n",
        "  return masked_input, target_conv, masks, masked_dialogue, n_masked\n",
        "\n",
        "\n",
        "def base_mlm_loss(logits, target, mask):\n",
        "  \"\"\" \n",
        "  This function defines the base MLM loss for masked parts prediction. It is an implementation of the usual MLM loss for one given utterance (the one corresponding to the target)\n",
        "\n",
        "  Input: \n",
        "    - logits: the logits of the model when it's called with tgt = target\n",
        "    - target: the unmasked utterance of interest for which we want to compute the MLM loss\n",
        "    - mask: the indices of the masked tokens for this utterance of interest\n",
        "\n",
        "  Ouput: \n",
        "    - mlm_loss_val: the MLM loss on this utterance\n",
        "  \"\"\"\n",
        "\n",
        "  #The usual MLM loss can be seen as a cross-entropy loss that focuses on the masked tokens. We use reduction = sum because we will normalize it later\n",
        "  #We filter the target on the masked tokens and compute the cross-entropy loss on these tokens\n",
        "  mlm_loss = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "  logits = torch.squeeze(logits[mask], dim=1)\n",
        "  target = target[mask]\n",
        "  if target.shape[0] != 1:\n",
        "    target = torch.squeeze(target, dim=-1)\n",
        "  if len(logits) > 0:\n",
        "    mlm_loss_val = mlm_loss(logits, target)\n",
        "  else: \n",
        "    print('this should no longer be possible')\n",
        "  return mlm_loss_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hZs2-stXKoQ"
      },
      "outputs": [],
      "source": [
        "# That part of the code is inspired by the hierarchical loss defined in the paper\n",
        "# Hierarchical Pre-training for Sequence Labelling in Spoken Dialog, Chapuis, Colombo et al. \n",
        "\n",
        "# defining hyperparameters. \n",
        "#mask_ratio is the proportion of tokens/utterances we want to mask, lamb_u and lamb_c weight the contribution of the loss for masked tokens and the loss for masked utterances\n",
        "mask_ratio = 0.2\n",
        "lamb_u = 0.5\n",
        "lamb_c = 0.5\n",
        "\n",
        "# using parameters used for padding the opensubtitles data previously\n",
        "max_dialog_size = 10\n",
        "max_size_of_utterance = 30\n",
        "padding_utterance = [0 for i in range(max_size_of_utterance)]\n",
        "padding_dialog = [padding_utterance for k in range(max_dialog_size)]\n",
        "\n",
        "\n",
        "def mlm_loss(model, conversation, mask_ratio, lamb_u, lamb_c ):\n",
        "  \"\"\"\n",
        "  This function is in charge of computing a weighted mean between the usual MLM loss at the masked tokens level and its generalization to the masked utterance level. \n",
        "  It is the loss that our model will use for its training.\n",
        "\n",
        "  Input: \n",
        "    - model: an instance of our hierarchical autoencoder Model\n",
        "    - conversation: a sample of conversation from our database, under the format of a list of list of tokens_ids (a list of tokenized utterances)\n",
        "    - mask_ratio: the proportion of tokens/utterances that we want to mask\n",
        "    - lamb_u: the importance of the MLM loss computed at the masked tokens level\n",
        "    - lamb_c: the importance of the MLM loss computed at the masked utterance level\n",
        "  \n",
        "  Output: \n",
        "    - a weighted mean of the MLM loss computed at the masked tokens level and the MLM loss computed at the masked utterance level\n",
        "  \"\"\"\n",
        "\n",
        "  if conversation == padding_dialog:\n",
        "    return 0\n",
        "\n",
        "\n",
        "  #First, we compute the MLM loss at the masked tokens level. \n",
        "  #This is the usual defintion of the MLM loss and we just need to apply the base MLM loss function we defined to all the utterances\n",
        "\n",
        "  #We begin by calling our masker function in order to have a version of the conversation where at least one tokens per utterance was masked\n",
        "  masked_input, target_conv, masks, masked_dialogue, n_masked = masker(conversation, 0.15)\n",
        "  losses = []\n",
        "\n",
        "  for index in range(len(conversation)):\n",
        "    #We iterate over all the utterances to focus on different targets and we identify the tokens that were masked to each utterance\n",
        "    target = target_conv[index]\n",
        "    mask = masks[index]\n",
        "    \n",
        "    #We train the model for each target of interest and get the probabilites associated with each token\n",
        "    logits = model(masked_input, target.to(device))   # shape: number of words of uterance of interest * vocab => a prediction is available for each word\n",
        "    logits = logits.view(-1, tokenizer.vocab_size)\n",
        "    target = target.view(-1).to(device)\n",
        "\n",
        "    #We feed the logits, the indices of the masked utterance and the original words into our MLM loss function to get the error\n",
        "    try:\n",
        "      mlm_loss_val = base_mlm_loss(logits, target, mask) # cross entropy loss while only considering the masked word compared to true value\n",
        "      losses += [mlm_loss_val]\n",
        "    except IndexError:\n",
        "      pass\n",
        "  \n",
        "  # the MLM loss at the masked token level is the sum of the losses for each utterance.\n",
        "  base_MLM = sum(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Second, we compute the MLM loss at the masked utterance level. In this scenario, we'll try to predict all the tokens from fully masked utterances\n",
        "  \n",
        "  #In that scenario, we'll consider the conversation as one single utterance where some tokens (all the tokens from some given utterances were masked)\n",
        "  target_conv_full = torch.cat(target_conv)\n",
        "\n",
        "  #We train the model on the full dialogue and get the logits\n",
        "  logits_full = model(masked_dialogue, target_conv_full.to(device))\n",
        "  logits_full = logits_full.view(-1, tokenizer.vocab_size)\n",
        "  target_conv_full = target_conv_full.view(-1).to(device)\n",
        "\n",
        "  #We get the indices of the masked tokens (all the tokens from some given utterances) and compute the MLM loss on it\n",
        "  mask_conv = (torch.cat([torch.tensor(el) for el in masked_dialogue]) == tokenizer.mask_token_id)\n",
        "  mlm_loss_full = base_mlm_loss(logits_full, target_conv_full, mask_conv)\n",
        "\n",
        "  #We compute the weighted loss between the two\n",
        "  mlm_loss = lamb_u * base_MLM + lamb_c * mlm_loss_full\n",
        "  return mlm_loss/n_masked"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [[2054, 2003, 2605, 3007], [1045, 2228, 2009, 2003, 3000]]\n",
        "mlm_loss(model_not_pretrained, conversation, mask_ratio, lamb_u, lamb_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSom7lKnZ6yA",
        "outputId": "e4b11e2a-f32b-4960-9f20-9122e6212052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.3638, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 3 / PRETRAINING OF THE AUTOENCODER"
      ],
      "metadata": {
        "id": "BMJyKFVey_-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we define a few functions to be able to handle batches of conversations:"
      ],
      "metadata": {
        "id": "IVtIy9b27ttv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converts conversation as list of strings to list of token ids, as required by the model\n",
        "def conv_converter(conversation_):\n",
        "\n",
        "  conversation = []\n",
        "  for sentence in conversation_ :\n",
        "    conversation.append(tokenizer(sentence)['input_ids'])\n",
        "    \n",
        "  return conversation\n",
        "\n",
        "\n",
        "# batch treatment to feed into the model\n",
        "def from_batch_to_conv(batch, max_dialog_size = max_dialog_size, max_size_of_utterance = max_size_of_utterance):\n",
        "\n",
        "  batch_size = len(batch[0][0])\n",
        "  conversations = []\n",
        "\n",
        "  for k in range(batch_size):\n",
        "    conversation = []\n",
        "    for u in range(max_dialog_size):\n",
        "      utterance = []\n",
        "      for n in range(max_size_of_utterance):\n",
        "        utterance.append(int(batch[u][n][k]))\n",
        "\n",
        "      conversation.append(utterance)\n",
        "    conversations.append(conversation)\n",
        "\n",
        "  return conversations\n",
        "\n",
        "\n",
        "# wrapped up computation of MLM loss for a whole batch, we sum the losses obtained on each conversation\n",
        "def batch_loss(batch):\n",
        "\n",
        "  loss = 0\n",
        "  for conversation in batch:\n",
        "    loss += mlm_loss(model, conversation, mask_ratio, lamb_u, lamb_c)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "m0AxTJrjbCY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define the function in charge of the training of the model.\n",
        "\n",
        "We only do the pretraining on one epoch at once because colab will never allow us to do multiple epochs without interrupting the execution\n",
        "\n",
        "We configure the function to be able to get back to pretraining after interruption thanks to intermediate savings"
      ],
      "metadata": {
        "id": "QneAM25ZOwxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_epoch_pretrain(model, optimizer, begin = 0, epoch_loss = 0):\n",
        "\n",
        "  # set the model into a training mode : the model's weights and parameters WILL BE updated!\n",
        "  model.train()\n",
        "\n",
        "  # define the iterator over batches defined by train_loader\n",
        "  it = iter(train_loader)\n",
        "\n",
        "  # bringing the iterator to where we where before shutdown (thanks colab)\n",
        "  if begin > 0:\n",
        "    try:\n",
        "      next(islice(it, begin, begin + 1))\n",
        "    except RuntimeError:\n",
        "      pass\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "\n",
        "\n",
        "  # for each valid batch (very rare RunTimeError so try condition)\n",
        "  for i in range(begin, train_loader.__len__()):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    if int((i/train_loader.__len__())*100) != int(((i-1)/train_loader.__len__())*100):\n",
        "      # intermediate saving ! colab is unstable...\n",
        "      with open(\"./drive/MyDrive/pretraining_params_saving\", \"wb\") as f:\n",
        "        pickle.dump({'epoch_loss' : epoch_loss, 'iter' : i}, f)\n",
        "      torch.save(model, './drive/MyDrive/model')\n",
        "      clear_output()\n",
        "      print(int((i/train_loader.__len__())*100), ' % of epoch')\n",
        "\n",
        "    try:\n",
        "\n",
        "      batch = next(it)\n",
        "      batch = from_batch_to_conv(batch['conversation'])\n",
        "      train_loss = batch_loss(batch)\n",
        "        \n",
        "      # compute accumulated gradients\n",
        "      train_loss.backward()\n",
        "      # perform parameter update based on current gradients\n",
        "      optimizer.step()\n",
        "      # add the mini-batch training loss to epoch_loss  \n",
        "      epoch_loss += train_loss.item()\n",
        "\n",
        "      print(train_loss.item())\n",
        "\n",
        "    # sometimes RuntimeError happen due to batch_size parameter in train_loader ! \n",
        "    # Rarely with batch_size = 3, and all the time with batch_size = 10 for instance\n",
        "    except RuntimeError:\n",
        "      pass\n",
        "      # KeyError skipped on this batch (very special character)\n",
        "    except KeyError:\n",
        "      pass"
      ],
      "metadata": {
        "id": "CDNSnrO8MFmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the pretraining data and convert it to a Dataloader object for batch-wise training:"
      ],
      "metadata": {
        "id": "3X_AEZPMPO4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/opensbtitles_dialogs4pretraining_padded_small', 'rb') as f:\n",
        "  dialogs4pretraining_padded = pickle.load(f)\n",
        "\n",
        "dialogs4pretraining_padded = pd.DataFrame({\n",
        "     \"conversation\" : dialogs4pretraining_padded\n",
        "})\n",
        "\n",
        "# only training on 1/3 of dialogs, too big otherwise\n",
        "dialogs4pretraining_padded = dialogs4pretraining_padded[dialogs4pretraining_padded.index < \n",
        "                                                        np.percentile(dialogs4pretraining_padded.index, 33)]\n",
        "\n",
        "from datasets import Dataset\n",
        "dialogs4pretraining_padded = OpenSubtitlesDataset(Dataset.from_dict(dialogs4pretraining_padded))\n",
        "\n",
        "\n",
        "# Creating DataLoader for OpenSubtitles dataset \n",
        "# loading train_loader, please define batch_size!\n",
        "train_loader = DataLoader(dataset = dialogs4pretraining_padded, batch_size = 3,\n",
        "                          num_workers=2, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "JW2a1N__chsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If an intermediary saved version exist, we load it with its weights:"
      ],
      "metadata": {
        "id": "mwfcnLqoSw6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('./drive/MyDrive/model')\n",
        "\n",
        "with open('./drive/MyDrive/pretraining_params_saving', 'rb') as f:\n",
        "  params_b4_stop = pickle.load(f)\n",
        "loss = params_b4_stop['epoch_loss']\n",
        "begin_mark = params_b4_stop['iter']"
      ],
      "metadata": {
        "id": "EDFa0z7PaZxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we run the training, using the stochastic gradient descent as an optimizer:"
      ],
      "metadata": {
        "id": "8E-yx9i-9B5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an optimizer object for SGD\n",
        "optimizer = opt.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# getting back to where we where before colab decided to stop the session\n",
        "one_epoch_pretrain(model, optimizer, begin = begin_mark, epoch_loss = loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mXi9iQygeb6E",
        "outputId": "c041ae20-a914-4555-dd06-cea80ee1dfff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68  % of epoch\n",
            "0.4377881586551666\n",
            "0.22375909984111786\n",
            "0.6407569646835327\n",
            "1.269823431968689\n",
            "1.1751580238342285\n",
            "0.11328204721212387\n",
            "0.05332847684621811\n",
            "1.236427903175354\n",
            "1.4482359886169434\n",
            "0.9583795666694641\n",
            "0.17977654933929443\n",
            "0.31546124815940857\n",
            "0.03450876474380493\n",
            "0.6464394927024841\n",
            "0.6029733419418335\n",
            "0.9643459320068359\n",
            "0.316817045211792\n",
            "0.5652420520782471\n",
            "0.6068803668022156\n",
            "0.37333065271377563\n",
            "0.6150548458099365\n",
            "0.3170987665653229\n",
            "0.9817095994949341\n",
            "0.02667429856956005\n",
            "0.3036181330680847\n",
            "0.5334111452102661\n",
            "0.40665116906166077\n",
            "0.163399800658226\n",
            "0.056358613073825836\n",
            "0.4366093873977661\n",
            "0.5773667097091675\n",
            "0.33691757917404175\n",
            "0.0719754695892334\n",
            "0.4259120225906372\n",
            "1.3154850006103516\n",
            "0.6769728064537048\n",
            "0.33807939291000366\n",
            "0.5562328696250916\n",
            "0.04853591322898865\n",
            "0.20967531204223633\n",
            "1.1756051778793335\n",
            "0.5940706133842468\n",
            "0.7167482376098633\n",
            "0.3838765025138855\n",
            "0.76789391040802\n",
            "0.8960665464401245\n",
            "0.2677481472492218\n",
            "1.178426742553711\n",
            "0.10034087300300598\n",
            "0.19804908335208893\n",
            "0.644828200340271\n",
            "0.11839587241411209\n",
            "0.6246451139450073\n",
            "0.42489901185035706\n",
            "0.4038825035095215\n",
            "0.15340732038021088\n",
            "0.26069459319114685\n",
            "0.9168685078620911\n",
            "0.6275079250335693\n",
            "0.26379862427711487\n",
            "0.1404830664396286\n",
            "0.35908475518226624\n",
            "0.5003756880760193\n",
            "0.5315443277359009\n",
            "0.6462786197662354\n",
            "0.1330503523349762\n",
            "0.07280703634023666\n",
            "0.031607840210199356\n",
            "0.3920850157737732\n",
            "0.48775506019592285\n",
            "0.4448471963405609\n",
            "0.40532582998275757\n",
            "0.7225769758224487\n",
            "0.12705788016319275\n",
            "0.4039730727672577\n",
            "0.6903330087661743\n",
            "0.596924364566803\n",
            "0.7607995271682739\n",
            "0.9420310854911804\n",
            "0.5261998772621155\n",
            "0.46256518363952637\n",
            "0.2185547947883606\n",
            "0.3690396249294281\n",
            "0.8111213445663452\n",
            "0.2950872778892517\n",
            "0.2794210910797119\n",
            "0.5964074730873108\n",
            "0.10497240722179413\n",
            "0.4253009855747223\n",
            "0.47581714391708374\n",
            "0.6543401479721069\n",
            "0.09534593671560287\n",
            "0.70269775390625\n",
            "0.09023381024599075\n",
            "0.6160100698471069\n",
            "0.7006017565727234\n",
            "0.6490616202354431\n",
            "0.29046326875686646\n",
            "0.3301502466201782\n",
            "0.18395176529884338\n",
            "0.13092422485351562\n",
            "0.27814149856567383\n",
            "0.5329816937446594\n",
            "0.22584092617034912\n",
            "0.37839263677597046\n",
            "0.7964152097702026\n",
            "0.9166088700294495\n",
            "0.5109622478485107\n",
            "0.7639452815055847\n",
            "0.04944702237844467\n",
            "1.013556957244873\n",
            "0.32499948143959045\n",
            "0.4567078948020935\n",
            "0.16720515489578247\n",
            "0.15143975615501404\n",
            "0.5323167443275452\n",
            "0.5328953266143799\n",
            "0.7689633369445801\n",
            "0.68415367603302\n",
            "0.9338311553001404\n",
            "0.6192110180854797\n",
            "0.38651856780052185\n",
            "0.23424500226974487\n",
            "0.25713613629341125\n",
            "0.2623952329158783\n",
            "0.7935794591903687\n",
            "1.1391711235046387\n",
            "0.5852213501930237\n",
            "0.046956200152635574\n",
            "0.7272678017616272\n",
            "0.7851331830024719\n",
            "0.5332022309303284\n",
            "0.24847468733787537\n",
            "1.0113906860351562\n",
            "1.0844839811325073\n",
            "0.6538081765174866\n",
            "0.9046584367752075\n",
            "0.6036697030067444\n",
            "0.6611411571502686\n",
            "0.06437193602323532\n",
            "0.16651950776576996\n",
            "0.0539201982319355\n",
            "0.6333747506141663\n",
            "0.4328436851501465\n",
            "0.3825984001159668\n",
            "0.26282232999801636\n",
            "0.6782738566398621\n",
            "0.451781690120697\n",
            "0.40328994393348694\n",
            "0.16222529113292694\n",
            "0.3316938579082489\n",
            "0.1332046240568161\n",
            "1.0211777687072754\n",
            "0.20494666695594788\n",
            "0.061875633895397186\n",
            "0.45280003547668457\n",
            "0.7492762207984924\n",
            "0.06748514622449875\n",
            "0.2363215982913971\n",
            "0.6591245532035828\n",
            "0.3953082859516144\n",
            "0.11387625336647034\n",
            "0.5365655422210693\n",
            "0.7839425802230835\n",
            "0.4352948069572449\n",
            "0.28720343112945557\n",
            "0.23810026049613953\n",
            "0.47391191124916077\n",
            "0.849666953086853\n",
            "0.3111242651939392\n",
            "0.6272732615470886\n",
            "0.50774747133255\n",
            "1.0350818634033203\n",
            "0.2579362392425537\n",
            "0.6063669919967651\n",
            "0.7255895137786865\n",
            "0.27076005935668945\n",
            "0.7933728098869324\n",
            "0.9841665029525757\n",
            "0.48473167419433594\n",
            "0.9440761208534241\n",
            "1.1412537097930908\n",
            "0.6097661256790161\n",
            "0.24510061740875244\n",
            "0.528221607208252\n",
            "0.371390700340271\n",
            "0.2762259542942047\n",
            "0.6226698756217957\n",
            "0.49564987421035767\n",
            "0.16972549259662628\n",
            "0.7905784845352173\n",
            "0.24589550495147705\n",
            "0.2447851002216339\n",
            "0.36719658970832825\n",
            "0.25906699895858765\n",
            "0.5337523818016052\n",
            "0.6801374554634094\n",
            "0.20670470595359802\n",
            "0.3577216863632202\n",
            "0.17274488508701324\n",
            "0.8597678542137146\n",
            "0.5880372524261475\n",
            "0.8895882368087769\n",
            "0.7371100187301636\n",
            "0.2599225640296936\n",
            "0.38259193301200867\n",
            "0.22674284875392914\n",
            "0.08196339756250381\n",
            "0.5710412859916687\n",
            "0.08343393355607986\n",
            "0.7437798976898193\n",
            "0.3089996576309204\n",
            "0.41280436515808105\n",
            "0.8839077949523926\n",
            "0.27349209785461426\n",
            "0.9331496357917786\n",
            "0.6096813082695007\n",
            "0.29975828528404236\n",
            "0.7812882661819458\n",
            "0.3833378255367279\n",
            "1.7898403406143188\n",
            "0.7967636585235596\n",
            "0.4054279029369354\n",
            "0.3518122136592865\n",
            "0.1996469348669052\n",
            "0.4925679564476013\n",
            "0.5050153136253357\n",
            "0.33952683210372925\n",
            "0.5034658908843994\n",
            "0.4698063135147095\n",
            "0.6536579132080078\n",
            "0.202183336019516\n",
            "0.1924586296081543\n",
            "0.20086455345153809\n",
            "0.052148353308439255\n",
            "0.8411067128181458\n",
            "0.5518909692764282\n",
            "0.010208358988165855\n",
            "1.0771715641021729\n",
            "0.6355478763580322\n",
            "0.001771828276105225\n",
            "0.19454312324523926\n",
            "0.567017138004303\n",
            "0.34842172265052795\n",
            "1.2297765016555786\n",
            "0.4610174894332886\n",
            "0.35816532373428345\n",
            "0.5610826015472412\n",
            "0.15572071075439453\n",
            "0.19810138642787933\n",
            "0.2874622344970703\n",
            "0.5195788145065308\n",
            "0.5642072558403015\n",
            "0.7017248868942261\n",
            "0.01933489739894867\n",
            "1.0155017375946045\n",
            "0.17568187415599823\n",
            "0.5887498259544373\n",
            "0.6822057962417603\n",
            "0.34685301780700684\n",
            "0.38146018981933594\n",
            "0.5853277444839478\n",
            "0.6636760830879211\n",
            "0.029974138364195824\n",
            "0.2622365355491638\n",
            "0.6071224808692932\n",
            "0.003574055153876543\n",
            "0.019528308883309364\n",
            "0.6983656883239746\n",
            "0.7904989123344421\n",
            "0.6346444487571716\n",
            "0.6298918724060059\n",
            "0.5601567029953003\n",
            "0.4281482994556427\n",
            "0.25206613540649414\n",
            "0.6078245043754578\n",
            "0.09473521262407303\n",
            "0.1716558337211609\n",
            "0.0252541471272707\n",
            "0.015857761725783348\n",
            "0.5253347158432007\n",
            "0.3259928524494171\n",
            "0.3998197019100189\n",
            "0.6142891645431519\n",
            "0.7776117920875549\n",
            "0.5274174809455872\n",
            "0.428574800491333\n",
            "0.3680056035518646\n",
            "0.6346784234046936\n",
            "0.7383392453193665\n",
            "0.5221883654594421\n",
            "0.44682514667510986\n",
            "1.8685259819030762\n",
            "0.507091760635376\n",
            "0.01327891368418932\n",
            "1.2155466079711914\n",
            "0.39199525117874146\n",
            "0.6297140121459961\n",
            "0.3014434576034546\n",
            "0.8382886052131653\n",
            "0.4631482660770416\n",
            "0.01126745156943798\n",
            "0.3646153509616852\n",
            "0.422096848487854\n",
            "0.5589718818664551\n",
            "0.4843584895133972\n",
            "1.1727389097213745\n",
            "0.6734830737113953\n",
            "0.19821414351463318\n",
            "0.5800321102142334\n",
            "0.5356931090354919\n",
            "0.6235077977180481\n",
            "1.7596012353897095\n",
            "0.02006661891937256\n",
            "0.198944091796875\n",
            "0.6048182249069214\n",
            "0.21905411779880524\n",
            "0.37886545062065125\n",
            "0.541165292263031\n",
            "0.1857507824897766\n",
            "0.0690893679857254\n",
            "0.4331907629966736\n",
            "0.8124292492866516\n",
            "0.3806281089782715\n",
            "0.10238265991210938\n",
            "0.24260938167572021\n",
            "1.1599090099334717\n",
            "0.8882735967636108\n",
            "0.3961634635925293\n",
            "0.6984394788742065\n",
            "0.4089345335960388\n",
            "0.7020604014396667\n",
            "0.18636763095855713\n",
            "0.4378468990325928\n",
            "1.0226831436157227\n",
            "1.3290045261383057\n",
            "0.32885876297950745\n",
            "1.397681474685669\n",
            "0.2875720262527466\n",
            "0.27807697653770447\n",
            "0.8988745212554932\n",
            "0.8434276580810547\n",
            "0.967018723487854\n",
            "0.5461070537567139\n",
            "0.2695380747318268\n",
            "0.41902825236320496\n",
            "0.19081389904022217\n",
            "0.32547780871391296\n",
            "0.4008730351924896\n",
            "0.6047658920288086\n",
            "0.5336529016494751\n",
            "0.47648072242736816\n",
            "0.08582828938961029\n",
            "0.10765062272548676\n",
            "1.1330434083938599\n",
            "0.2514570653438568\n",
            "0.048074863851070404\n",
            "0.47003185749053955\n",
            "0.7894774079322815\n",
            "0.2692137360572815\n",
            "0.6726040840148926\n",
            "0.3775649666786194\n",
            "1.5826034545898438\n",
            "0.22717300057411194\n",
            "0.34187743067741394\n",
            "1.0342066287994385\n",
            "0.46065977215766907\n",
            "0.5292019844055176\n",
            "0.24804934859275818\n",
            "0.3242035508155823\n",
            "0.25919491052627563\n",
            "0.8956799507141113\n",
            "1.3383262157440186\n",
            "0.28411000967025757\n",
            "0.5034206509590149\n",
            "0.07637672126293182\n",
            "0.5564131736755371\n",
            "0.4446379244327545\n",
            "0.4187362790107727\n",
            "0.09827294945716858\n",
            "0.3990493714809418\n",
            "0.7993391752243042\n",
            "0.4166789948940277\n",
            "0.7196251153945923\n",
            "0.41775017976760864\n",
            "0.7462689876556396\n",
            "0.19355401396751404\n",
            "0.8367873430252075\n",
            "0.6464237570762634\n",
            "0.5405244827270508\n",
            "0.40737685561180115\n",
            "0.20101964473724365\n",
            "0.3137362003326416\n",
            "0.7580119371414185\n",
            "0.2097267359495163\n",
            "0.6558713912963867\n",
            "0.3488602936267853\n",
            "0.47600167989730835\n",
            "0.31215816736221313\n",
            "0.28301990032196045\n",
            "0.24377784132957458\n",
            "0.01915157027542591\n",
            "0.38000866770744324\n",
            "0.2802141606807709\n",
            "0.3969881236553192\n",
            "0.7368776798248291\n",
            "0.5376987457275391\n",
            "0.03477620705962181\n",
            "0.32017287611961365\n",
            "0.35595402121543884\n",
            "0.9380557537078857\n",
            "0.5155519843101501\n",
            "0.05258442461490631\n",
            "0.5886102914810181\n",
            "0.08532675355672836\n",
            "1.0106515884399414\n",
            "0.2796260416507721\n",
            "0.20713891088962555\n",
            "0.572625458240509\n",
            "0.3247150182723999\n",
            "0.30343687534332275\n",
            "0.9497398138046265\n",
            "0.4904193878173828\n",
            "0.7173644304275513\n",
            "0.17488375306129456\n",
            "0.2007676362991333\n",
            "0.24105770885944366\n",
            "0.8239529728889465\n",
            "0.5058569312095642\n",
            "0.8892470002174377\n",
            "0.015918297693133354\n",
            "0.08789948374032974\n",
            "0.31412437558174133\n",
            "0.43303418159484863\n",
            "0.4706382155418396\n",
            "1.0058375597000122\n",
            "0.610764741897583\n",
            "0.40285223722457886\n",
            "0.34609419107437134\n",
            "0.35206952691078186\n",
            "0.3011939525604248\n",
            "0.29551950097084045\n",
            "0.22557681798934937\n",
            "0.8948894143104553\n",
            "0.5162719488143921\n",
            "0.8920156359672546\n",
            "0.4177563786506653\n",
            "0.6219925880432129\n",
            "0.015963880345225334\n",
            "0.5411273241043091\n",
            "0.6174131035804749\n",
            "0.3849640488624573\n",
            "0.15115727484226227\n",
            "0.22640061378479004\n",
            "0.24896283447742462\n",
            "0.17805469036102295\n",
            "0.39913347363471985\n",
            "0.7829767465591431\n",
            "0.22628240287303925\n",
            "0.890605628490448\n",
            "0.9072585701942444\n",
            "0.9127416610717773\n",
            "0.5870095491409302\n",
            "0.005223415791988373\n",
            "0.546697199344635\n",
            "0.26542943716049194\n",
            "0.5405869483947754\n",
            "0.4269755184650421\n",
            "0.43322956562042236\n",
            "0.47259068489074707\n",
            "0.9512649178504944\n",
            "1.6870245933532715\n",
            "0.2957037389278412\n",
            "0.4107204079627991\n",
            "0.6866610646247864\n",
            "0.6489734649658203\n",
            "0.44371944665908813\n",
            "0.7968429327011108\n",
            "0.21765504777431488\n",
            "0.5849131345748901\n",
            "0.9173664450645447\n",
            "0.34437859058380127\n",
            "0.1807609498500824\n",
            "0.3373924493789673\n",
            "0.3582015633583069\n",
            "1.1752057075500488\n",
            "0.31604132056236267\n",
            "0.5766148567199707\n",
            "0.9245057106018066\n",
            "0.010280930437147617\n",
            "0.14247743785381317\n",
            "0.4592497944831848\n",
            "0.11235713958740234\n",
            "0.24295774102210999\n",
            "1.3068653345108032\n",
            "1.5200912952423096\n",
            "0.13879725337028503\n",
            "0.4992753267288208\n",
            "0.6948529481887817\n",
            "0.22011427581310272\n",
            "0.8647316694259644\n",
            "0.3479437828063965\n",
            "1.1921225786209106\n",
            "0.8095569610595703\n",
            "0.5589615702629089\n",
            "0.5505446195602417\n",
            "0.33690908551216125\n",
            "0.1268404871225357\n",
            "0.5137447118759155\n",
            "0.44096970558166504\n",
            "0.36840641498565674\n",
            "0.9188918471336365\n",
            "0.19153587520122528\n",
            "0.4780503213405609\n",
            "0.3489857614040375\n",
            "0.5352595448493958\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-c49fef4b508b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# getting back to where we where before colab decided to stop the session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mone_epoch_pretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbegin_mark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-a8149c9ae2da>\u001b[0m in \u001b[0;36mone_epoch_pretrain\u001b[0;34m(model, optimizer, begin, epoch_loss)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m# compute accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0;31m# perform parameter update based on current gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1st epoch cumul. MLM loss : __119 397__\n",
        "* 2nd epoch cumul. MLM loss : __52 462__\n",
        "* 3rd epoch cumul. MLM loss : __34 029__\n",
        "* Estimated cumul. MLM loss for epoch 4 (not enough computation units to run the epoch, estimation based on 5% of pretrain set) : __25 180__"
      ],
      "metadata": {
        "id": "rBNc0jQkTpiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 4 / TRAINING OF THE MODEL FOR MOVIES REVIEWS CLASSIFICATION"
      ],
      "metadata": {
        "id": "KvaX0tUH9IQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we fine-tune the model on movie reviews to see how well the pretraining on dialogues generalizes to intent classification on written paragraphs.\n",
        "\n",
        "We use data from allociné since our pretraining was made on French dialogues. This dataset is available on the PyTorch library."
      ],
      "metadata": {
        "id": "LCS6g-h49LgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cinema_reviews = load_dataset(\"allocine\")\n",
        "cinema_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "63f96d2001154a5c8cdc1d120e08232c",
            "12c48669f71f421990374157ea0ade2e",
            "cef68fc041af4902bf115a538bae6a1c",
            "7b92c3608ea046c9a21a12039902e4ea",
            "c5879c3753a644ec964dfb8e35f7c796",
            "0e63bbb439e14dffbf4e6fa987534dbe",
            "dff231d5300e45bb8c44201cc7b2cb64",
            "a95572802c6343369a8b7a12e618bc74",
            "f9859b9b925e43ca82abe0372e85432f",
            "9e35b4a667aa4a9ab3d214228157fb53",
            "93c92597b7a84aedaee49549a617a0f3"
          ]
        },
        "id": "Uvv1svnG7oB9",
        "outputId": "ef63d4c1-6c18-41bb-efa1-730749c2332d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset allocine (/root/.cache/huggingface/datasets/allocine/allocine/1.0.0/ea86b1dc05eae3a45a07b6281f2d4033b5fe7927b1008d06aa457ca1eae660d0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63f96d2001154a5c8cdc1d120e08232c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['review', 'label'],\n",
              "        num_rows: 160000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['review', 'label'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['review', 'label'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset = cinema_reviews['train'], batch_size = 10,\n",
        "                          num_workers=2, shuffle=True, drop_last=True)\n",
        "\n",
        "example_batch = next(iter(train_loader))\n",
        "example_batch['review'][0]"
      ],
      "metadata": {
        "id": "5gbwXtdDKp5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d0399432-fd60-4128-f291-c60523d46e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Très bon film d\\'Alain Corneau. Sur une trame simpliste, la recherche d\\'un autre soi-même, un voyage dans une Inde fascinante, très bien filmée (photographie de Angelo), avec une belle musique, un Jean-Hughes Anglade habité par son rôle, des rencontres incroyables, une fin assez belle avec \"la femme\". C\\'est très beau, poétique, philosophique, et très cinématographique. Certes, cela demande des efforts d\\'attention, car c\\'est assez lent, mais malgré cela, se laisse voir sans ennui.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to preprocess this new data. We use the same techniques as for the pretraining: tokenization with the same pretrained tokenizer, padding to have consistent sizes in the reviews and storing of the reviews as lists of lists of tokens (list of tokenized sentences)"
      ],
      "metadata": {
        "id": "DhdQ6iXn-L6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining')\n",
        "max_dialog_size = 5\n",
        "max_size_of_utterance = 30\n",
        "\n",
        "def preprocessing(batch, max_dialog_size, max_size_of_utterance):\n",
        "\n",
        "  batch_preprocessed = []\n",
        "  for review in batch['review']:\n",
        "\n",
        "    # We begin by splitting the reviews into a list of sentences, using the nltk library\n",
        "    correct_spacing_after_dot = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', review)                           # making sure a whitespace separate sentences\n",
        "    list_of_sentences = nltk.tokenize.sent_tokenize(correct_spacing_after_dot, language='french')\n",
        "\n",
        "    # We tokenize each sentence using our pretrained tokenizer\n",
        "    tokenized_list = [tokenizer(el.lower())['input_ids'] for el in list_of_sentences]\n",
        "    padding_utterance = [0 for i in range(max_size_of_utterance)]\n",
        "    for i in range(len(tokenized_list)):\n",
        "      if len(tokenized_list[i]) >= max_size_of_utterance:\n",
        "        tokenized_list[i] = tokenized_list[i][:max_size_of_utterance]\n",
        "      else: \n",
        "        tokenized_list[i] = tokenized_list[i] + [0 for i in range(len(tokenized_list[i]), max_size_of_utterance)]\n",
        "        \n",
        "    # Finally, we use padding to have the same number of sentences in each review and the same number of tokens in each sentences\n",
        "    padded_list = tokenized_list[:max_dialog_size] if len(tokenized_list) >= max_dialog_size else tokenized_list + [padding_utterance for i in range(len(tokenized_list), max_dialog_size)]\n",
        "    batch_preprocessed += [padded_list]\n",
        "\n",
        "  return batch_preprocessed"
      ],
      "metadata": {
        "id": "12JoXz9cJJgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's now time to define our new model to perform classification on movie reviews. We reuse the encoding layers from the pretrained model and add a MLP on top of it:"
      ],
      "metadata": {
        "id": "G8hFlXNS_XOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Version of the pretrained model with classification layer\n",
        "\n",
        "class FinalModel(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(FinalModel, self).__init__()\n",
        "\n",
        "    self.first_layer_encoders = model.first_layer_encoders\n",
        "    self.second_layer_encoder = model.second_layer_encoder\n",
        "    self.classif_layer = nn.Linear(100 * max_size_of_utterance, 2)\n",
        "  \n",
        "  def forward(self, conversation):\n",
        "    \n",
        "    #We feed our new data into the pretrained model\n",
        "    conversation = [embedder(token_ids).to(device) for token_ids in conversation]\n",
        "    sentence_hidden_states = [self.first_layer_encoders(sentence) for sentence in conversation]\n",
        "    corpus_hidden_states = torch.cat(sentence_hidden_states, dim=0)\n",
        "    second_layer = self.second_layer_encoder(corpus_hidden_states)\n",
        "\n",
        "    #We finetune the model with an additionnal MLP\n",
        "    second_layer = second_layer.reshape(max_dialog_size, max_size_of_utterance*100)    #After retrieving the hidden embedding produced by the pretrain model, we reshape it to have one line per sentence\n",
        "    output = self.classif_layer(second_layer)                                          #Then we use a Dense layer to have an output of dimension \"number of sentences x 2\" so that each sentence can contribute to the probability\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "fBTfnZrUKlSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "final_model = FinalModel(model)\n",
        "final_model.to(device)"
      ],
      "metadata": {
        "id": "NutqYxObNKku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f92ee5-8998-4503-fb50-2ce2006e3e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalModel(\n",
              "  (first_layer_encoders): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (second_layer_encoder): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (classif_layer): Linear(in_features=3000, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We finally define our loss and the training function to optimize the model"
      ],
      "metadata": {
        "id": "S9G1pdf1_4j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def batch_loss_classif(batch):\n",
        "  \"\"\"\n",
        "  A function to define our loss at the batch level.\n",
        "  It is defined as the sum of the losses obtained on each review. \n",
        "\n",
        "  The model outputs a matrix of size \"number of sentences x 2\" that contains the votes of each sentence for the opinion of the review (0 for the first column, 1 in the second)\n",
        "  We turn the votes into probabilities, sum them and apply the cross-entropy loss\n",
        "  \"\"\"\n",
        "\n",
        "  target = batch['label']\n",
        "\n",
        "  # We turn the reviews from teh batches into the format expected by the model\n",
        "  x = preprocessing(batch, max_dialog_size, max_size_of_utterance)\n",
        "\n",
        "  # initialize batch loss\n",
        "  loss = 0\n",
        "\n",
        "  # We iterate over each review in the batch\n",
        "  for el, targ in zip(x, target): \n",
        "    \n",
        "    try:\n",
        "      torch.manual_seed(0)\n",
        "\n",
        "      #We run the model with each review\n",
        "      out = final_model(el)\n",
        "\n",
        "      #We make the columns sum to one (each line / sentence gives a proba for 0 and a proba for 1)\n",
        "      probas = F.softmax(out, dim=1)                \n",
        "\n",
        "      #We sum the votes, use the cross-entropy loss (the PyTorch version takes care of applying the softmax) and add it to batch loss\n",
        "      votes = torch.sum(probas, axis=0).to(device)\n",
        "      loss += loss_fn(votes, torch.tensor(targ).to(device)) \n",
        "\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "7lVwsqIttQ4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimization of parameters, using SGD and the loss defined\n",
        "\n",
        "def classif_trainer(final_model, num_epoch = 10, max_norm = 1, epoch_b4_stopping = 0, epoch_losses = [], classif_layer_only = True, clipping = True):\n",
        "\n",
        "  if epoch_b4_stopping != 0:\n",
        "    epoch_losses = epoch_losses\n",
        "  else:\n",
        "    epoch_losses = [0 for i in range(num_epoch)]\n",
        "\n",
        "  if classif_layer_only == True:\n",
        "    for param in final_model.parameters():\n",
        "      param.requires_grad = False\n",
        "    for param in final_model.classif_layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "  optimizer = opt.SGD(final_model.parameters(), lr=0.001, momentum=0.9)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  # training mode \"train\" only changes the \"dropout\" or \"batchnorm\" layers\n",
        "  final_model.train()\n",
        "\n",
        "  # main loop (train+test)\n",
        "  for epoch in tqdm(range(epoch_b4_stopping, num_epoch)):\n",
        "\n",
        "      if epoch == epoch_b4_stopping:\n",
        "        epoch_losses[epoch] = 0 \n",
        "\n",
        "      loss_100_batches = 0\n",
        "      \n",
        "      for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "          # Clear previous gradients in the graph\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # compute batch loss\n",
        "          loss = batch_loss_classif(batch)\n",
        "\n",
        "          # Compute clipped gradients\n",
        "          loss.backward()\n",
        "          if clipping == True:\n",
        "            utils.clip_grad_norm(final_model.parameters(), max_norm)\n",
        "\n",
        "          epoch_losses[epoch] += loss.item()\n",
        "          loss_100_batches += loss.item()\n",
        "\n",
        "          # Apply Gradient descent\n",
        "          optimizer.step()\n",
        "          \n",
        "          if batch_idx %100 ==0:\n",
        "              print('epoch {} batch {} [{}/{}] training loss: {}'.format(epoch,batch_idx,batch_idx*10,\n",
        "                      len(train_loader.dataset),loss_100_batches))\n",
        "              loss_100_batches = 0\n",
        "              \n",
        "          if batch_idx %1000 == 0:\n",
        "            with open(\"./drive/MyDrive/final_training_params_saving\", \"wb\") as f:\n",
        "              pickle.dump({'epoch_losses': epoch_losses, 'epoch_b4_stopping' : epoch}, f)\n",
        "            torch.save(final_model, './drive/MyDrive/final_model')\n",
        "\n",
        "  with open(\"./drive/MyDrive/final_training_epoch_losses\", \"wb\") as f:\n",
        "              pickle.dump(epoch_losses, f)\n",
        "  torch.save(final_model, './drive/MyDrive/final_model')"
      ],
      "metadata": {
        "id": "EDBC9cMRrSaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "final_model = torch.load('./drive/MyDrive/final_model')"
      ],
      "metadata": {
        "id": "9qZpJBw_nyfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classif_trainer(final_model, num_epoch = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHfsUVPPHVAe",
        "outputId": "248d3ed3-e8dd-4e2a-bf31-257d71150d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 batch 0 [0/160000] training loss: 4.973075866699219\n",
            "epoch 0 batch 100 [1000/160000] training loss: 561.0885152816772\n",
            "epoch 0 batch 200 [2000/160000] training loss: 598.1758968830109\n",
            "epoch 0 batch 300 [3000/160000] training loss: 593.3617217540741\n",
            "epoch 0 batch 400 [4000/160000] training loss: 555.1356732845306\n",
            "epoch 0 batch 500 [5000/160000] training loss: 609.7186354398727\n",
            "epoch 0 batch 600 [6000/160000] training loss: 597.5041286945343\n",
            "epoch 0 batch 700 [7000/160000] training loss: 568.6174657344818\n",
            "epoch 0 batch 800 [8000/160000] training loss: 619.4961135387421\n",
            "epoch 0 batch 900 [9000/160000] training loss: 572.446617603302\n",
            "epoch 0 batch 1000 [10000/160000] training loss: 591.2948250770569\n",
            "epoch 0 batch 1100 [11000/160000] training loss: 602.0693011283875\n",
            "epoch 0 batch 1200 [12000/160000] training loss: 601.2670428752899\n",
            "epoch 0 batch 1300 [13000/160000] training loss: 574.6419415473938\n",
            "epoch 0 batch 1400 [14000/160000] training loss: 572.5380162000656\n",
            "epoch 0 batch 1500 [15000/160000] training loss: 533.439455986023\n",
            "epoch 0 batch 1600 [16000/160000] training loss: 576.394362449646\n",
            "epoch 0 batch 1700 [17000/160000] training loss: 566.9380375146866\n",
            "epoch 0 batch 1800 [18000/160000] training loss: 555.4498207569122\n",
            "epoch 0 batch 1900 [19000/160000] training loss: 594.0148785114288\n",
            "epoch 0 batch 2000 [20000/160000] training loss: 560.6425311565399\n",
            "epoch 0 batch 2100 [21000/160000] training loss: 569.1373379230499\n",
            "epoch 0 batch 2200 [22000/160000] training loss: 602.7828135490417\n",
            "epoch 0 batch 2300 [23000/160000] training loss: 586.324006319046\n",
            "epoch 0 batch 2400 [24000/160000] training loss: 585.1445000171661\n",
            "epoch 0 batch 2500 [25000/160000] training loss: 574.3421056270599\n",
            "epoch 0 batch 2600 [26000/160000] training loss: 558.332709312439\n",
            "epoch 0 batch 2700 [27000/160000] training loss: 578.9082448482513\n",
            "epoch 0 batch 2800 [28000/160000] training loss: 584.4145970344543\n",
            "epoch 0 batch 2900 [29000/160000] training loss: 639.727668762207\n",
            "epoch 0 batch 3000 [30000/160000] training loss: 558.5380663871765\n",
            "epoch 0 batch 3100 [31000/160000] training loss: 588.2898890972137\n",
            "epoch 0 batch 3200 [32000/160000] training loss: 569.6862344741821\n",
            "epoch 0 batch 3300 [33000/160000] training loss: 565.8755848407745\n",
            "epoch 0 batch 3400 [34000/160000] training loss: 562.13929438591\n",
            "epoch 0 batch 3500 [35000/160000] training loss: 571.1385204792023\n",
            "epoch 0 batch 3600 [36000/160000] training loss: 610.0697045326233\n",
            "epoch 0 batch 3700 [37000/160000] training loss: 547.9784542322159\n",
            "epoch 0 batch 3800 [38000/160000] training loss: 573.5332370996475\n",
            "epoch 0 batch 3900 [39000/160000] training loss: 615.5414617061615\n",
            "epoch 0 batch 4000 [40000/160000] training loss: 572.9908615350723\n",
            "epoch 0 batch 4100 [41000/160000] training loss: 577.9856164455414\n",
            "epoch 0 batch 4200 [42000/160000] training loss: 600.2918455600739\n",
            "epoch 0 batch 4300 [43000/160000] training loss: 573.3074685335159\n",
            "epoch 0 batch 4400 [44000/160000] training loss: 559.5448451042175\n",
            "epoch 0 batch 4500 [45000/160000] training loss: 584.9729616641998\n",
            "epoch 0 batch 4600 [46000/160000] training loss: 564.9929747581482\n",
            "epoch 0 batch 4700 [47000/160000] training loss: 538.2543976306915\n",
            "epoch 0 batch 4800 [48000/160000] training loss: 578.9953492879868\n",
            "epoch 0 batch 4900 [49000/160000] training loss: 578.1329457759857\n",
            "epoch 0 batch 5000 [50000/160000] training loss: 548.9574459791183\n",
            "epoch 0 batch 5100 [51000/160000] training loss: 573.6981906890869\n",
            "epoch 0 batch 5200 [52000/160000] training loss: 588.4000709056854\n",
            "epoch 0 batch 5300 [53000/160000] training loss: 599.8860754966736\n",
            "epoch 0 batch 5400 [54000/160000] training loss: 567.3358268737793\n",
            "epoch 0 batch 5500 [55000/160000] training loss: 555.9363882541656\n",
            "epoch 0 batch 5600 [56000/160000] training loss: 563.2709152698517\n",
            "epoch 0 batch 5700 [57000/160000] training loss: 580.147337436676\n",
            "epoch 0 batch 5800 [58000/160000] training loss: 566.3193125724792\n",
            "epoch 0 batch 5900 [59000/160000] training loss: 563.0021679401398\n",
            "epoch 0 batch 6000 [60000/160000] training loss: 571.8654799461365\n",
            "epoch 0 batch 6100 [61000/160000] training loss: 595.1904652118683\n",
            "epoch 0 batch 6200 [62000/160000] training loss: 619.1923813819885\n",
            "epoch 0 batch 6300 [63000/160000] training loss: 593.0718712806702\n",
            "epoch 0 batch 6400 [64000/160000] training loss: 613.7181761264801\n",
            "epoch 0 batch 6500 [65000/160000] training loss: 575.3501374721527\n",
            "epoch 0 batch 6600 [66000/160000] training loss: 572.0600967407227\n",
            "epoch 0 batch 6700 [67000/160000] training loss: 590.9119353294373\n",
            "epoch 0 batch 6800 [68000/160000] training loss: 581.4303534030914\n",
            "epoch 0 batch 6900 [69000/160000] training loss: 601.0925331115723\n",
            "epoch 0 batch 7000 [70000/160000] training loss: 589.7442677021027\n",
            "epoch 0 batch 7100 [71000/160000] training loss: 567.2972419261932\n",
            "epoch 0 batch 7200 [72000/160000] training loss: 585.6011136770248\n",
            "epoch 0 batch 7300 [73000/160000] training loss: 564.6857922077179\n",
            "epoch 0 batch 7400 [74000/160000] training loss: 566.8386571407318\n",
            "epoch 0 batch 7500 [75000/160000] training loss: 544.2702499628067\n",
            "epoch 0 batch 7600 [76000/160000] training loss: 572.4774420261383\n",
            "epoch 0 batch 7700 [77000/160000] training loss: 575.5705962181091\n",
            "epoch 0 batch 7800 [78000/160000] training loss: 622.4453077316284\n",
            "epoch 0 batch 7900 [79000/160000] training loss: 580.064367055893\n",
            "epoch 0 batch 8000 [80000/160000] training loss: 565.2315833568573\n",
            "epoch 0 batch 8100 [81000/160000] training loss: 553.0552216768265\n",
            "epoch 0 batch 8200 [82000/160000] training loss: 612.7924115657806\n",
            "epoch 0 batch 8300 [83000/160000] training loss: 589.2604351043701\n",
            "epoch 0 batch 8400 [84000/160000] training loss: 562.1858496665955\n",
            "epoch 0 batch 8500 [85000/160000] training loss: 604.1983978748322\n",
            "epoch 0 batch 8600 [86000/160000] training loss: 577.4031836986542\n",
            "epoch 0 batch 8700 [87000/160000] training loss: 588.3698290586472\n",
            "epoch 0 batch 8800 [88000/160000] training loss: 597.4690682888031\n",
            "epoch 0 batch 8900 [89000/160000] training loss: 585.9686827659607\n",
            "epoch 0 batch 9000 [90000/160000] training loss: 591.8281754255295\n",
            "epoch 0 batch 9100 [91000/160000] training loss: 585.0400396585464\n",
            "epoch 0 batch 9200 [92000/160000] training loss: 573.8101791143417\n",
            "epoch 0 batch 9300 [93000/160000] training loss: 581.7875237464905\n",
            "epoch 0 batch 9400 [94000/160000] training loss: 558.3746573925018\n",
            "epoch 0 batch 9500 [95000/160000] training loss: 571.9357883930206\n",
            "epoch 0 batch 9600 [96000/160000] training loss: 595.1625652313232\n",
            "epoch 0 batch 9700 [97000/160000] training loss: 575.0845236778259\n",
            "epoch 0 batch 9800 [98000/160000] training loss: 545.2843618392944\n",
            "epoch 0 batch 9900 [99000/160000] training loss: 577.4075870513916\n",
            "epoch 0 batch 10000 [100000/160000] training loss: 598.9634420871735\n",
            "epoch 0 batch 10100 [101000/160000] training loss: 576.1242034435272\n",
            "epoch 0 batch 10200 [102000/160000] training loss: 535.4944105148315\n",
            "epoch 0 batch 10300 [103000/160000] training loss: 574.8428947925568\n",
            "epoch 0 batch 10400 [104000/160000] training loss: 554.1702995300293\n",
            "epoch 0 batch 10500 [105000/160000] training loss: 623.6014528274536\n",
            "epoch 0 batch 10600 [106000/160000] training loss: 602.1005308628082\n",
            "epoch 0 batch 10700 [107000/160000] training loss: 565.2334598302841\n",
            "epoch 0 batch 10800 [108000/160000] training loss: 574.9319794178009\n",
            "epoch 0 batch 10900 [109000/160000] training loss: 541.259067773819\n",
            "epoch 0 batch 11000 [110000/160000] training loss: 579.676432132721\n",
            "epoch 0 batch 11100 [111000/160000] training loss: 607.0331330299377\n",
            "epoch 0 batch 11200 [112000/160000] training loss: 568.7160608768463\n",
            "epoch 0 batch 11300 [113000/160000] training loss: 574.0536227226257\n",
            "epoch 0 batch 11400 [114000/160000] training loss: 626.6838582754135\n",
            "epoch 0 batch 11500 [115000/160000] training loss: 573.9268202781677\n",
            "epoch 0 batch 11600 [116000/160000] training loss: 571.9053134918213\n",
            "epoch 0 batch 11700 [117000/160000] training loss: 565.7131702899933\n",
            "epoch 0 batch 11800 [118000/160000] training loss: 562.8216975927353\n",
            "epoch 0 batch 11900 [119000/160000] training loss: 551.9180483818054\n",
            "epoch 0 batch 12000 [120000/160000] training loss: 578.8262453079224\n",
            "epoch 0 batch 12100 [121000/160000] training loss: 552.5917472839355\n",
            "epoch 0 batch 12200 [122000/160000] training loss: 554.5270636081696\n",
            "epoch 0 batch 12300 [123000/160000] training loss: 584.7858616113663\n",
            "epoch 0 batch 12400 [124000/160000] training loss: 583.2615706920624\n",
            "epoch 0 batch 12500 [125000/160000] training loss: 564.4911153316498\n",
            "epoch 0 batch 12600 [126000/160000] training loss: 569.8962795734406\n",
            "epoch 0 batch 12700 [127000/160000] training loss: 609.1530240774155\n",
            "epoch 0 batch 12800 [128000/160000] training loss: 594.3150081634521\n",
            "epoch 0 batch 12900 [129000/160000] training loss: 585.8211581707001\n",
            "epoch 0 batch 13000 [130000/160000] training loss: 567.2417285442352\n",
            "epoch 0 batch 13100 [131000/160000] training loss: 600.896734714508\n",
            "epoch 0 batch 13200 [132000/160000] training loss: 610.3509514331818\n",
            "epoch 0 batch 13300 [133000/160000] training loss: 579.7420008182526\n",
            "epoch 0 batch 13400 [134000/160000] training loss: 564.3960573673248\n",
            "epoch 0 batch 13500 [135000/160000] training loss: 578.9458231925964\n",
            "epoch 0 batch 13600 [136000/160000] training loss: 536.60076212883\n",
            "epoch 0 batch 13700 [137000/160000] training loss: 616.957245349884\n",
            "epoch 0 batch 13800 [138000/160000] training loss: 574.9492456912994\n",
            "epoch 0 batch 13900 [139000/160000] training loss: 587.7133988142014\n",
            "epoch 0 batch 14000 [140000/160000] training loss: 583.4961533546448\n",
            "epoch 0 batch 14100 [141000/160000] training loss: 605.710371017456\n",
            "epoch 0 batch 14200 [142000/160000] training loss: 574.29119181633\n",
            "epoch 0 batch 14300 [143000/160000] training loss: 577.0178070068359\n",
            "epoch 0 batch 14400 [144000/160000] training loss: 562.7232813835144\n",
            "epoch 0 batch 14500 [145000/160000] training loss: 574.5511567592621\n",
            "epoch 0 batch 14600 [146000/160000] training loss: 578.3510093688965\n",
            "epoch 0 batch 14700 [147000/160000] training loss: 584.9082281589508\n",
            "epoch 0 batch 14800 [148000/160000] training loss: 586.9974894523621\n",
            "epoch 0 batch 14900 [149000/160000] training loss: 606.9097023010254\n",
            "epoch 0 batch 15000 [150000/160000] training loss: 603.2995989322662\n",
            "epoch 0 batch 15100 [151000/160000] training loss: 574.9688532352448\n",
            "epoch 0 batch 15200 [152000/160000] training loss: 590.9916665554047\n",
            "epoch 0 batch 15300 [153000/160000] training loss: 604.3205187320709\n",
            "epoch 0 batch 15400 [154000/160000] training loss: 570.2796087265015\n",
            "epoch 0 batch 15500 [155000/160000] training loss: 595.767147064209\n",
            "epoch 0 batch 15600 [156000/160000] training loss: 564.6577355861664\n",
            "epoch 0 batch 15700 [157000/160000] training loss: 577.6780441999435\n",
            "epoch 0 batch 15800 [158000/160000] training loss: 536.4892852306366\n",
            "epoch 0 batch 15900 [159000/160000] training loss: 558.9100859165192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 1/30 [33:12<16:02:55, 1992.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 batch 0 [0/160000] training loss: 4.966423988342285\n",
            "epoch 1 batch 100 [1000/160000] training loss: 561.2135145664215\n",
            "epoch 1 batch 200 [2000/160000] training loss: 597.2158207893372\n",
            "epoch 1 batch 300 [3000/160000] training loss: 593.5213496685028\n",
            "epoch 1 batch 400 [4000/160000] training loss: 555.3676036596298\n",
            "epoch 1 batch 500 [5000/160000] training loss: 609.4263695478439\n",
            "epoch 1 batch 600 [6000/160000] training loss: 595.9689656496048\n",
            "epoch 1 batch 700 [7000/160000] training loss: 568.5092234611511\n",
            "epoch 1 batch 800 [8000/160000] training loss: 618.8671262264252\n",
            "epoch 1 batch 900 [9000/160000] training loss: 572.9425468444824\n",
            "epoch 1 batch 1000 [10000/160000] training loss: 591.1758124828339\n",
            "epoch 1 batch 1100 [11000/160000] training loss: 602.2452881336212\n",
            "epoch 1 batch 1200 [12000/160000] training loss: 600.9229584932327\n",
            "epoch 1 batch 1300 [13000/160000] training loss: 574.4912250041962\n",
            "epoch 1 batch 1400 [14000/160000] training loss: 571.6438583135605\n",
            "epoch 1 batch 1500 [15000/160000] training loss: 533.5709521770477\n",
            "epoch 1 batch 1600 [16000/160000] training loss: 575.9657118320465\n",
            "epoch 1 batch 1700 [17000/160000] training loss: 566.4541635513306\n",
            "epoch 1 batch 1800 [18000/160000] training loss: 555.0068607330322\n",
            "epoch 1 batch 1900 [19000/160000] training loss: 593.6948409080505\n",
            "epoch 1 batch 2000 [20000/160000] training loss: 560.4250020980835\n",
            "epoch 1 batch 2100 [21000/160000] training loss: 569.0972564220428\n",
            "epoch 1 batch 2200 [22000/160000] training loss: 602.7004683017731\n",
            "epoch 1 batch 2300 [23000/160000] training loss: 586.313178062439\n",
            "epoch 1 batch 2400 [24000/160000] training loss: 584.8864192962646\n",
            "epoch 1 batch 2500 [25000/160000] training loss: 574.1075142621994\n",
            "epoch 1 batch 2600 [26000/160000] training loss: 558.6267092227936\n",
            "epoch 1 batch 2700 [27000/160000] training loss: 579.3372156620026\n",
            "epoch 1 batch 2800 [28000/160000] training loss: 584.7080609798431\n",
            "epoch 1 batch 2900 [29000/160000] training loss: 640.3108355998993\n",
            "epoch 1 batch 3000 [30000/160000] training loss: 558.3638978004456\n",
            "epoch 1 batch 3100 [31000/160000] training loss: 588.2204880714417\n",
            "epoch 1 batch 3200 [32000/160000] training loss: 568.8321559429169\n",
            "epoch 1 batch 3300 [33000/160000] training loss: 565.4719533920288\n",
            "epoch 1 batch 3400 [34000/160000] training loss: 562.2230923175812\n",
            "epoch 1 batch 3500 [35000/160000] training loss: 570.7444280385971\n",
            "epoch 1 batch 3600 [36000/160000] training loss: 609.9423007965088\n",
            "epoch 1 batch 3700 [37000/160000] training loss: 548.0377982854843\n",
            "epoch 1 batch 3800 [38000/160000] training loss: 572.8766130208969\n",
            "epoch 1 batch 3900 [39000/160000] training loss: 615.6144022941589\n",
            "epoch 1 batch 4000 [40000/160000] training loss: 572.4460661411285\n",
            "epoch 1 batch 4100 [41000/160000] training loss: 577.4630267620087\n",
            "epoch 1 batch 4200 [42000/160000] training loss: 600.3575067520142\n",
            "epoch 1 batch 4300 [43000/160000] training loss: 573.1423001289368\n",
            "epoch 1 batch 4400 [44000/160000] training loss: 559.5066635608673\n",
            "epoch 1 batch 4500 [45000/160000] training loss: 585.0531868934631\n",
            "epoch 1 batch 4600 [46000/160000] training loss: 564.664736032486\n",
            "epoch 1 batch 4700 [47000/160000] training loss: 537.6523015499115\n",
            "epoch 1 batch 4800 [48000/160000] training loss: 581.3799312114716\n",
            "epoch 1 batch 4900 [49000/160000] training loss: 577.9945778846741\n",
            "epoch 1 batch 5000 [50000/160000] training loss: 548.8669897317886\n",
            "epoch 1 batch 5100 [51000/160000] training loss: 572.8130605220795\n",
            "epoch 1 batch 5200 [52000/160000] training loss: 588.1386601924896\n",
            "epoch 1 batch 5300 [53000/160000] training loss: 599.5216784477234\n",
            "epoch 1 batch 5400 [54000/160000] training loss: 567.3852393627167\n",
            "epoch 1 batch 5500 [55000/160000] training loss: 560.0248682498932\n",
            "epoch 1 batch 5600 [56000/160000] training loss: 565.3584532737732\n",
            "epoch 1 batch 5700 [57000/160000] training loss: 580.0965087413788\n",
            "epoch 1 batch 5800 [58000/160000] training loss: 566.4657683372498\n",
            "epoch 1 batch 5900 [59000/160000] training loss: 563.2965416908264\n",
            "epoch 1 batch 6000 [60000/160000] training loss: 571.6642906665802\n",
            "epoch 1 batch 6100 [61000/160000] training loss: 595.4704728126526\n",
            "epoch 1 batch 6200 [62000/160000] training loss: 618.8607774972916\n",
            "epoch 1 batch 6300 [63000/160000] training loss: 593.4355635643005\n",
            "epoch 1 batch 6400 [64000/160000] training loss: 614.806295633316\n",
            "epoch 1 batch 6500 [65000/160000] training loss: 575.1258366107941\n",
            "epoch 1 batch 6600 [66000/160000] training loss: 572.3117160797119\n",
            "epoch 1 batch 6700 [67000/160000] training loss: 590.4166419506073\n",
            "epoch 1 batch 6800 [68000/160000] training loss: 580.0362038612366\n",
            "epoch 1 batch 6900 [69000/160000] training loss: 600.9412651062012\n",
            "epoch 1 batch 7000 [70000/160000] training loss: 589.8239796161652\n",
            "epoch 1 batch 7100 [71000/160000] training loss: 569.2272744178772\n",
            "epoch 1 batch 7200 [72000/160000] training loss: 584.5987454652786\n",
            "epoch 1 batch 7300 [73000/160000] training loss: 564.1627632379532\n",
            "epoch 1 batch 7400 [74000/160000] training loss: 566.6326115131378\n",
            "epoch 1 batch 7500 [75000/160000] training loss: 544.651638507843\n",
            "epoch 1 batch 7600 [76000/160000] training loss: 572.9921016693115\n",
            "epoch 1 batch 7700 [77000/160000] training loss: 576.615035533905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /!\\ run this cell if training was interrupted by colab /\n",
        "with open('./drive/MyDrive/final_training_params_saving', 'rb') as f:\n",
        "  final_training_params_saving = pickle.load(f)\n",
        "\n",
        "final_model = torch.load('./drive/MyDrive/final_model')\n",
        "classif_trainer(final_model, num_epoch = 30, epoch_b4_stopping = final_training_params_saving['epoch_b4_stopping'],\n",
        "                epoch_losses = final_training_params_saving['epoch_losses'], classif_layer_only = True)"
      ],
      "metadata": {
        "id": "QVZz1xEcmKsW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dc9f2fb4f098428db16136f2937a6015",
            "43d86c5d283f4c7b9d9e1bd6cda18d40",
            "2ce2cfb9a9ad42b58192f3bc596cbfd0",
            "d75e3266bd09473c95183b11bf78c98b",
            "bce500f1fd2047c098385b163334805d",
            "4cec64f065a540a69cab0d097d2276a5",
            "7a5a3f2e67464e939e19006fbf86d8b8",
            "0a0e0480aa5c46b2968bf0cac17a4110",
            "015e9ff1cc24471f861985627618bb68",
            "91bf7c332ece4936bf205e56eeed1b82",
            "63ce71453d1d4054999c2e00a0aaaf96"
          ]
        },
        "outputId": "58907099-4343-4e90-8ec1-126af6b36dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc9f2fb4f098428db16136f2937a6015"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 23 batch 0 [0/160000] training loss: 5.353320121765137\n",
            "epoch 23 batch 100 [1000/160000] training loss: 554.9440817832947\n",
            "epoch 23 batch 200 [2000/160000] training loss: 571.2627465724945\n",
            "epoch 23 batch 300 [3000/160000] training loss: 583.6008839607239\n",
            "epoch 23 batch 400 [4000/160000] training loss: 545.4663650989532\n",
            "epoch 23 batch 500 [5000/160000] training loss: 608.2859929800034\n",
            "epoch 23 batch 600 [6000/160000] training loss: 578.2488363981247\n",
            "epoch 23 batch 700 [7000/160000] training loss: 558.5797343254089\n",
            "epoch 23 batch 800 [8000/160000] training loss: 602.5063028335571\n",
            "epoch 23 batch 900 [9000/160000] training loss: 570.9003794193268\n",
            "epoch 23 batch 1000 [10000/160000] training loss: 577.5220693349838\n",
            "epoch 23 batch 1100 [11000/160000] training loss: 588.2168145179749\n",
            "epoch 23 batch 1200 [12000/160000] training loss: 583.1911407709122\n",
            "epoch 23 batch 1300 [13000/160000] training loss: 565.6407499313354\n",
            "epoch 23 batch 1400 [14000/160000] training loss: 560.2698872089386\n",
            "epoch 23 batch 1500 [15000/160000] training loss: 530.4617733955383\n",
            "epoch 23 batch 1600 [16000/160000] training loss: 566.4670634269714\n",
            "epoch 23 batch 1700 [17000/160000] training loss: 560.522780418396\n",
            "epoch 23 batch 1800 [18000/160000] training loss: 545.9320688247681\n",
            "epoch 23 batch 1900 [19000/160000] training loss: 581.5862219333649\n",
            "epoch 23 batch 2000 [20000/160000] training loss: 544.164097070694\n",
            "epoch 23 batch 2100 [21000/160000] training loss: 556.291631937027\n",
            "epoch 23 batch 2200 [22000/160000] training loss: 597.0542174577713\n",
            "epoch 23 batch 2300 [23000/160000] training loss: 582.034769654274\n",
            "epoch 23 batch 2400 [24000/160000] training loss: 581.4816851615906\n",
            "epoch 23 batch 2500 [25000/160000] training loss: 576.6266214847565\n",
            "epoch 23 batch 2600 [26000/160000] training loss: 548.2546696662903\n",
            "epoch 23 batch 2700 [27000/160000] training loss: 578.7455779314041\n",
            "epoch 23 batch 2800 [28000/160000] training loss: 583.1619372367859\n",
            "epoch 23 batch 2900 [29000/160000] training loss: 628.3556251525879\n",
            "epoch 23 batch 3000 [30000/160000] training loss: 555.6167678833008\n",
            "epoch 23 batch 3100 [31000/160000] training loss: 589.2788031101227\n",
            "epoch 23 batch 3200 [32000/160000] training loss: 561.6810624599457\n",
            "epoch 23 batch 3300 [33000/160000] training loss: 562.3660879135132\n",
            "epoch 23 batch 3400 [34000/160000] training loss: 558.7682883739471\n",
            "epoch 23 batch 3500 [35000/160000] training loss: 568.9940023422241\n",
            "epoch 23 batch 3600 [36000/160000] training loss: 603.8285903930664\n",
            "epoch 23 batch 3700 [37000/160000] training loss: 546.6554661989212\n",
            "epoch 23 batch 3800 [38000/160000] training loss: 568.9909174442291\n",
            "epoch 23 batch 3900 [39000/160000] training loss: 600.8502042293549\n",
            "epoch 23 batch 4000 [40000/160000] training loss: 564.2656359672546\n",
            "epoch 23 batch 4100 [41000/160000] training loss: 573.6910665035248\n",
            "epoch 23 batch 4200 [42000/160000] training loss: 613.4368689060211\n",
            "epoch 23 batch 4300 [43000/160000] training loss: 573.1177431344986\n",
            "epoch 23 batch 4400 [44000/160000] training loss: 555.581340432167\n",
            "epoch 23 batch 4500 [45000/160000] training loss: 585.4723222255707\n",
            "epoch 23 batch 4600 [46000/160000] training loss: 564.4878447055817\n",
            "epoch 23 batch 4700 [47000/160000] training loss: 537.0251239538193\n",
            "epoch 23 batch 4800 [48000/160000] training loss: 576.668545126915\n",
            "epoch 23 batch 4900 [49000/160000] training loss: 575.6513378620148\n",
            "epoch 23 batch 5000 [50000/160000] training loss: 548.5796920061111\n",
            "epoch 23 batch 5100 [51000/160000] training loss: 556.4591541290283\n",
            "epoch 23 batch 5200 [52000/160000] training loss: 575.0769805908203\n",
            "epoch 23 batch 5300 [53000/160000] training loss: 597.7972683906555\n",
            "epoch 23 batch 5400 [54000/160000] training loss: 583.1340489387512\n",
            "epoch 23 batch 5500 [55000/160000] training loss: 559.2046649456024\n",
            "epoch 23 batch 5600 [56000/160000] training loss: 562.149608373642\n",
            "epoch 23 batch 5700 [57000/160000] training loss: 574.2041900157928\n",
            "epoch 23 batch 5800 [58000/160000] training loss: 562.6296436786652\n",
            "epoch 23 batch 5900 [59000/160000] training loss: 556.4947439432144\n",
            "epoch 23 batch 6000 [60000/160000] training loss: 566.8338403701782\n",
            "epoch 23 batch 6100 [61000/160000] training loss: 606.8001635074615\n",
            "epoch 23 batch 6200 [62000/160000] training loss: 611.0180668830872\n",
            "epoch 23 batch 6300 [63000/160000] training loss: 597.6883080005646\n",
            "epoch 23 batch 6400 [64000/160000] training loss: 609.3601438999176\n",
            "epoch 23 batch 6500 [65000/160000] training loss: 568.5937142372131\n",
            "epoch 23 batch 6600 [66000/160000] training loss: 564.8606729507446\n",
            "epoch 23 batch 6700 [67000/160000] training loss: 586.6119771003723\n",
            "epoch 23 batch 6800 [68000/160000] training loss: 572.8597726821899\n",
            "epoch 23 batch 6900 [69000/160000] training loss: 600.3210015296936\n",
            "epoch 23 batch 7000 [70000/160000] training loss: 585.8139872550964\n",
            "epoch 23 batch 7100 [71000/160000] training loss: 573.2950830459595\n",
            "epoch 23 batch 7200 [72000/160000] training loss: 576.1820456981659\n",
            "epoch 23 batch 7300 [73000/160000] training loss: 550.0639944076538\n",
            "epoch 23 batch 7400 [74000/160000] training loss: 563.5636358261108\n",
            "epoch 23 batch 7500 [75000/160000] training loss: 546.5186021327972\n",
            "epoch 23 batch 7600 [76000/160000] training loss: 565.7330362796783\n",
            "epoch 23 batch 7700 [77000/160000] training loss: 587.5958073139191\n",
            "epoch 23 batch 7800 [78000/160000] training loss: 615.6344799995422\n",
            "epoch 23 batch 7900 [79000/160000] training loss: 570.4059345722198\n",
            "epoch 23 batch 8000 [80000/160000] training loss: 566.3091855049133\n",
            "epoch 23 batch 8100 [81000/160000] training loss: 555.1025441884995\n",
            "epoch 23 batch 8200 [82000/160000] training loss: 598.4522159099579\n",
            "epoch 23 batch 8300 [83000/160000] training loss: 600.0182156562805\n",
            "epoch 23 batch 8400 [84000/160000] training loss: 548.165154337883\n",
            "epoch 23 batch 8500 [85000/160000] training loss: 612.1695038080215\n",
            "epoch 23 batch 8600 [86000/160000] training loss: 573.8600261211395\n",
            "epoch 23 batch 8700 [87000/160000] training loss: 580.461817741394\n",
            "epoch 23 batch 8800 [88000/160000] training loss: 598.9404909610748\n",
            "epoch 23 batch 8900 [89000/160000] training loss: 599.3635993003845\n",
            "epoch 23 batch 9000 [90000/160000] training loss: 583.9741277694702\n",
            "epoch 23 batch 9100 [91000/160000] training loss: 573.836786031723\n",
            "epoch 23 batch 9200 [92000/160000] training loss: 573.7449414730072\n",
            "epoch 23 batch 9300 [93000/160000] training loss: 578.0879168510437\n",
            "epoch 23 batch 9400 [94000/160000] training loss: 560.1603193283081\n",
            "epoch 23 batch 9500 [95000/160000] training loss: 577.8542002439499\n",
            "epoch 23 batch 9600 [96000/160000] training loss: 590.1288797855377\n",
            "epoch 23 batch 9700 [97000/160000] training loss: 576.4115145206451\n",
            "epoch 23 batch 9800 [98000/160000] training loss: 540.1496958732605\n",
            "epoch 23 batch 9900 [99000/160000] training loss: 577.0400099754333\n",
            "epoch 23 batch 10000 [100000/160000] training loss: 604.4388630390167\n",
            "epoch 23 batch 10100 [101000/160000] training loss: 579.4262185096741\n",
            "epoch 23 batch 10200 [102000/160000] training loss: 532.7222969532013\n",
            "epoch 23 batch 10300 [103000/160000] training loss: 569.7481002807617\n",
            "epoch 23 batch 10400 [104000/160000] training loss: 555.9153739213943\n",
            "epoch 23 batch 10500 [105000/160000] training loss: 635.9356939792633\n",
            "epoch 23 batch 10600 [106000/160000] training loss: 610.293984413147\n",
            "epoch 23 batch 10700 [107000/160000] training loss: 566.955729842186\n",
            "epoch 23 batch 10800 [108000/160000] training loss: 569.4930040836334\n",
            "epoch 23 batch 10900 [109000/160000] training loss: 542.038339138031\n",
            "epoch 23 batch 11000 [110000/160000] training loss: 586.0811977386475\n",
            "epoch 23 batch 11100 [111000/160000] training loss: 584.2184092998505\n",
            "epoch 23 batch 11200 [112000/160000] training loss: 570.4074859619141\n",
            "epoch 23 batch 11300 [113000/160000] training loss: 576.2164845466614\n",
            "epoch 23 batch 11400 [114000/160000] training loss: 599.6903164386749\n",
            "epoch 23 batch 11500 [115000/160000] training loss: 567.5614727735519\n",
            "epoch 23 batch 11600 [116000/160000] training loss: 576.2141964435577\n",
            "epoch 23 batch 11700 [117000/160000] training loss: 562.7738661766052\n",
            "epoch 23 batch 11800 [118000/160000] training loss: 571.4567371606827\n",
            "epoch 23 batch 11900 [119000/160000] training loss: 556.4334796667099\n",
            "epoch 23 batch 12000 [120000/160000] training loss: 573.4277760982513\n",
            "epoch 23 batch 12100 [121000/160000] training loss: 560.7064354419708\n",
            "epoch 23 batch 12200 [122000/160000] training loss: 548.0319525003433\n",
            "epoch 23 batch 12300 [123000/160000] training loss: 581.8112970590591\n",
            "epoch 23 batch 12400 [124000/160000] training loss: 581.5980439186096\n",
            "epoch 23 batch 12500 [125000/160000] training loss: 556.977970957756\n",
            "epoch 23 batch 12600 [126000/160000] training loss: 561.352735042572\n",
            "epoch 23 batch 12700 [127000/160000] training loss: 599.1107362508774\n",
            "epoch 23 batch 12800 [128000/160000] training loss: 601.8606852293015\n",
            "epoch 23 batch 12900 [129000/160000] training loss: 587.0244492292404\n",
            "epoch 23 batch 13000 [130000/160000] training loss: 556.353670835495\n",
            "epoch 23 batch 13100 [131000/160000] training loss: 594.8173689842224\n",
            "epoch 23 batch 13200 [132000/160000] training loss: 636.1967542171478\n",
            "epoch 23 batch 13300 [133000/160000] training loss: 601.2551097869873\n",
            "epoch 23 batch 13400 [134000/160000] training loss: 556.5602012872696\n",
            "epoch 23 batch 13500 [135000/160000] training loss: 578.4143013954163\n",
            "epoch 23 batch 13600 [136000/160000] training loss: 537.7199194431305\n",
            "epoch 23 batch 13700 [137000/160000] training loss: 616.8066010475159\n",
            "epoch 23 batch 13800 [138000/160000] training loss: 575.4655184745789\n",
            "epoch 23 batch 13900 [139000/160000] training loss: 608.2086198329926\n",
            "epoch 23 batch 14000 [140000/160000] training loss: 587.2683579921722\n",
            "epoch 23 batch 14100 [141000/160000] training loss: 598.0553719997406\n",
            "epoch 23 batch 14200 [142000/160000] training loss: 561.9579424858093\n",
            "epoch 23 batch 14300 [143000/160000] training loss: 579.0448002815247\n",
            "epoch 23 batch 14400 [144000/160000] training loss: 568.7786366939545\n",
            "epoch 23 batch 14500 [145000/160000] training loss: 570.072671175003\n",
            "epoch 23 batch 14600 [146000/160000] training loss: 578.9449080228806\n",
            "epoch 23 batch 14700 [147000/160000] training loss: 594.5475697517395\n",
            "epoch 23 batch 14800 [148000/160000] training loss: 587.1575651168823\n",
            "epoch 23 batch 14900 [149000/160000] training loss: 604.2815787792206\n",
            "epoch 23 batch 15000 [150000/160000] training loss: 600.1318504810333\n",
            "epoch 23 batch 15100 [151000/160000] training loss: 568.8615684509277\n",
            "epoch 23 batch 15200 [152000/160000] training loss: 594.6820871829987\n",
            "epoch 23 batch 15300 [153000/160000] training loss: 599.949310541153\n",
            "epoch 23 batch 15400 [154000/160000] training loss: 573.6771347522736\n",
            "epoch 23 batch 15500 [155000/160000] training loss: 594.5324983596802\n",
            "epoch 23 batch 15600 [156000/160000] training loss: 565.6470022201538\n",
            "epoch 23 batch 15700 [157000/160000] training loss: 575.4390815496445\n",
            "epoch 23 batch 15800 [158000/160000] training loss: 544.0794043540955\n",
            "epoch 23 batch 15900 [159000/160000] training loss: 550.3382411003113\n",
            "epoch 24 batch 0 [0/160000] training loss: 4.801905632019043\n",
            "epoch 24 batch 100 [1000/160000] training loss: 559.7611360549927\n",
            "epoch 24 batch 200 [2000/160000] training loss: 576.9736279249191\n",
            "epoch 24 batch 300 [3000/160000] training loss: 589.5131344795227\n",
            "epoch 24 batch 400 [4000/160000] training loss: 550.4896746873856\n",
            "epoch 24 batch 500 [5000/160000] training loss: 610.2997374534607\n",
            "epoch 24 batch 600 [6000/160000] training loss: 584.9313621520996\n",
            "epoch 24 batch 700 [7000/160000] training loss: 562.031614780426\n",
            "epoch 24 batch 800 [8000/160000] training loss: 602.2855136394501\n",
            "epoch 24 batch 900 [9000/160000] training loss: 572.2942571640015\n",
            "epoch 24 batch 1000 [10000/160000] training loss: 580.3818242549896\n",
            "epoch 24 batch 1100 [11000/160000] training loss: 590.4934039115906\n",
            "epoch 24 batch 1200 [12000/160000] training loss: 576.3158416748047\n",
            "epoch 24 batch 1300 [13000/160000] training loss: 567.4641876220703\n",
            "epoch 24 batch 1400 [14000/160000] training loss: 565.8645853996277\n",
            "epoch 24 batch 1500 [15000/160000] training loss: 531.7204253673553\n",
            "epoch 24 batch 1600 [16000/160000] training loss: 567.5375220775604\n",
            "epoch 24 batch 1700 [17000/160000] training loss: 558.5835065841675\n",
            "epoch 24 batch 1800 [18000/160000] training loss: 550.5322299003601\n",
            "epoch 24 batch 1900 [19000/160000] training loss: 584.7193596363068\n",
            "epoch 24 batch 2000 [20000/160000] training loss: 547.4459102153778\n",
            "epoch 24 batch 2100 [21000/160000] training loss: 560.7493057250977\n",
            "epoch 24 batch 2200 [22000/160000] training loss: 593.2697672843933\n",
            "epoch 24 batch 2300 [23000/160000] training loss: 582.5700542926788\n",
            "epoch 24 batch 2400 [24000/160000] training loss: 583.116354227066\n",
            "epoch 24 batch 2500 [25000/160000] training loss: 576.1249604225159\n",
            "epoch 24 batch 2600 [26000/160000] training loss: 549.7628393173218\n",
            "epoch 24 batch 2700 [27000/160000] training loss: 581.578855752945\n",
            "epoch 24 batch 2800 [28000/160000] training loss: 582.6548433303833\n",
            "epoch 24 batch 2900 [29000/160000] training loss: 629.4799134731293\n",
            "epoch 24 batch 3000 [30000/160000] training loss: 554.501945734024\n",
            "epoch 24 batch 3100 [31000/160000] training loss: 591.4850342273712\n",
            "epoch 24 batch 3200 [32000/160000] training loss: 563.6650838851929\n",
            "epoch 24 batch 3300 [33000/160000] training loss: 561.1998059749603\n",
            "epoch 24 batch 3400 [34000/160000] training loss: 560.4606540203094\n",
            "epoch 24 batch 3500 [35000/160000] training loss: 567.7067950963974\n",
            "epoch 24 batch 3600 [36000/160000] training loss: 602.2084031105042\n",
            "epoch 24 batch 3700 [37000/160000] training loss: 548.9616202116013\n",
            "epoch 24 batch 3800 [38000/160000] training loss: 566.5521807670593\n",
            "epoch 24 batch 3900 [39000/160000] training loss: 602.6636381149292\n",
            "epoch 24 batch 4000 [40000/160000] training loss: 564.2947580814362\n",
            "epoch 24 batch 4100 [41000/160000] training loss: 574.1241822242737\n",
            "epoch 24 batch 4200 [42000/160000] training loss: 613.3077208995819\n",
            "epoch 24 batch 4300 [43000/160000] training loss: 574.3797501325607\n",
            "epoch 24 batch 4400 [44000/160000] training loss: 554.456046462059\n",
            "epoch 24 batch 4500 [45000/160000] training loss: 587.9407434463501\n",
            "epoch 24 batch 4600 [46000/160000] training loss: 565.3572108745575\n",
            "epoch 24 batch 4700 [47000/160000] training loss: 537.2038612365723\n",
            "epoch 24 batch 4800 [48000/160000] training loss: 578.3032466173172\n",
            "epoch 24 batch 4900 [49000/160000] training loss: 574.5611064434052\n",
            "epoch 24 batch 5000 [50000/160000] training loss: 547.5817601680756\n",
            "epoch 24 batch 5100 [51000/160000] training loss: 558.1664206981659\n",
            "epoch 24 batch 5200 [52000/160000] training loss: 577.4620740413666\n",
            "epoch 24 batch 5300 [53000/160000] training loss: 599.0474677085876\n",
            "epoch 24 batch 5400 [54000/160000] training loss: 584.829262971878\n",
            "epoch 24 batch 5500 [55000/160000] training loss: 560.2238295078278\n",
            "epoch 24 batch 5600 [56000/160000] training loss: 561.1251039505005\n",
            "epoch 24 batch 5700 [57000/160000] training loss: 575.7265810966492\n",
            "epoch 24 batch 5800 [58000/160000] training loss: 566.1037168502808\n",
            "epoch 24 batch 5900 [59000/160000] training loss: 557.1446067094803\n",
            "epoch 24 batch 6000 [60000/160000] training loss: 566.24928855896\n",
            "epoch 24 batch 6100 [61000/160000] training loss: 605.9888236522675\n",
            "epoch 24 batch 6200 [62000/160000] training loss: 608.1878643035889\n",
            "epoch 24 batch 6300 [63000/160000] training loss: 598.5764548778534\n",
            "epoch 24 batch 6400 [64000/160000] training loss: 608.5558514595032\n",
            "epoch 24 batch 6500 [65000/160000] training loss: 568.0254442691803\n",
            "epoch 24 batch 6600 [66000/160000] training loss: 566.9742293357849\n",
            "epoch 24 batch 6700 [67000/160000] training loss: 585.3049240112305\n",
            "epoch 24 batch 6800 [68000/160000] training loss: 571.8345301151276\n",
            "epoch 24 batch 6900 [69000/160000] training loss: 599.4516415596008\n",
            "epoch 24 batch 7000 [70000/160000] training loss: 585.7063846588135\n",
            "epoch 24 batch 7100 [71000/160000] training loss: 571.5434849262238\n",
            "epoch 24 batch 7200 [72000/160000] training loss: 576.1089016199112\n",
            "epoch 24 batch 7300 [73000/160000] training loss: 550.1954283714294\n",
            "epoch 24 batch 7400 [74000/160000] training loss: 563.8326723575592\n",
            "epoch 24 batch 7500 [75000/160000] training loss: 546.3644471168518\n",
            "epoch 24 batch 7600 [76000/160000] training loss: 563.2595992088318\n",
            "epoch 24 batch 7700 [77000/160000] training loss: 588.6880226135254\n",
            "epoch 24 batch 7800 [78000/160000] training loss: 614.4948799610138\n",
            "epoch 24 batch 7900 [79000/160000] training loss: 571.4759941101074\n",
            "epoch 24 batch 8000 [80000/160000] training loss: 567.5095596313477\n",
            "epoch 24 batch 8100 [81000/160000] training loss: 556.1049225330353\n",
            "epoch 24 batch 8200 [82000/160000] training loss: 597.4210143089294\n",
            "epoch 24 batch 8300 [83000/160000] training loss: 599.6464738845825\n",
            "epoch 24 batch 8400 [84000/160000] training loss: 550.5690568685532\n",
            "epoch 24 batch 8500 [85000/160000] training loss: 611.6695183515549\n",
            "epoch 24 batch 8600 [86000/160000] training loss: 573.2762250900269\n",
            "epoch 24 batch 8700 [87000/160000] training loss: 580.2793724536896\n",
            "epoch 24 batch 8800 [88000/160000] training loss: 599.4898014068604\n",
            "epoch 24 batch 8900 [89000/160000] training loss: 603.2501324415207\n",
            "epoch 24 batch 9000 [90000/160000] training loss: 585.2483413219452\n",
            "epoch 24 batch 9100 [91000/160000] training loss: 572.3045923709869\n",
            "epoch 24 batch 9200 [92000/160000] training loss: 573.5950019359589\n",
            "epoch 24 batch 9300 [93000/160000] training loss: 575.4378094673157\n",
            "epoch 24 batch 9400 [94000/160000] training loss: 560.0624532699585\n",
            "epoch 24 batch 9500 [95000/160000] training loss: 577.3327497243881\n",
            "epoch 24 batch 9600 [96000/160000] training loss: 589.1335399150848\n",
            "epoch 24 batch 9700 [97000/160000] training loss: 574.9984663724899\n",
            "epoch 24 batch 9800 [98000/160000] training loss: 539.7380497455597\n",
            "epoch 24 batch 9900 [99000/160000] training loss: 577.61763215065\n",
            "epoch 24 batch 10000 [100000/160000] training loss: 604.5171978473663\n",
            "epoch 24 batch 10100 [101000/160000] training loss: 578.7030959129333\n",
            "epoch 24 batch 10200 [102000/160000] training loss: 531.809369802475\n",
            "epoch 24 batch 10300 [103000/160000] training loss: 568.7665994167328\n",
            "epoch 24 batch 10400 [104000/160000] training loss: 553.9005160331726\n",
            "epoch 24 batch 10500 [105000/160000] training loss: 637.4448385238647\n",
            "epoch 24 batch 10600 [106000/160000] training loss: 610.0081174373627\n",
            "epoch 24 batch 10700 [107000/160000] training loss: 567.9930200576782\n",
            "epoch 24 batch 10800 [108000/160000] training loss: 568.6523644924164\n",
            "epoch 24 batch 10900 [109000/160000] training loss: 541.0724995136261\n",
            "epoch 24 batch 11000 [110000/160000] training loss: 584.8943511247635\n",
            "epoch 24 batch 11100 [111000/160000] training loss: 583.6989500522614\n",
            "epoch 24 batch 11200 [112000/160000] training loss: 569.6847199201584\n",
            "epoch 24 batch 11300 [113000/160000] training loss: 575.7713882923126\n",
            "epoch 24 batch 11400 [114000/160000] training loss: 598.3373832702637\n",
            "epoch 24 batch 11500 [115000/160000] training loss: 566.4573183059692\n",
            "epoch 24 batch 11600 [116000/160000] training loss: 576.2464437484741\n",
            "epoch 24 batch 11700 [117000/160000] training loss: 562.0983303785324\n",
            "epoch 24 batch 11800 [118000/160000] training loss: 572.0552245378494\n",
            "epoch 24 batch 11900 [119000/160000] training loss: 556.287585735321\n",
            "epoch 24 batch 12000 [120000/160000] training loss: 572.8860149383545\n",
            "epoch 24 batch 12100 [121000/160000] training loss: 557.9552417993546\n",
            "epoch 24 batch 12200 [122000/160000] training loss: 546.9031355381012\n",
            "epoch 24 batch 12300 [123000/160000] training loss: 580.4949027299881\n",
            "epoch 24 batch 12400 [124000/160000] training loss: 581.086442232132\n",
            "epoch 24 batch 12500 [125000/160000] training loss: 557.4632881879807\n",
            "epoch 24 batch 12600 [126000/160000] training loss: 560.8955185413361\n",
            "epoch 24 batch 12700 [127000/160000] training loss: 597.3072754144669\n",
            "epoch 24 batch 12800 [128000/160000] training loss: 602.5715386867523\n",
            "epoch 24 batch 12900 [129000/160000] training loss: 585.3857893943787\n",
            "epoch 24 batch 13000 [130000/160000] training loss: 555.6235105991364\n",
            "epoch 24 batch 13100 [131000/160000] training loss: 593.5136382579803\n",
            "epoch 24 batch 13200 [132000/160000] training loss: 631.687362909317\n",
            "epoch 24 batch 13300 [133000/160000] training loss: 602.7391194105148\n",
            "epoch 24 batch 13400 [134000/160000] training loss: 556.2887569665909\n",
            "epoch 24 batch 13500 [135000/160000] training loss: 579.345221042633\n",
            "epoch 24 batch 13600 [136000/160000] training loss: 537.6687707901001\n",
            "epoch 24 batch 13700 [137000/160000] training loss: 616.4114253520966\n",
            "epoch 24 batch 13800 [138000/160000] training loss: 574.1150681972504\n",
            "epoch 24 batch 13900 [139000/160000] training loss: 610.8669662475586\n",
            "epoch 24 batch 14000 [140000/160000] training loss: 585.9255990982056\n",
            "epoch 24 batch 14100 [141000/160000] training loss: 597.5784850120544\n",
            "epoch 24 batch 14200 [142000/160000] training loss: 560.4225168228149\n",
            "epoch 24 batch 14300 [143000/160000] training loss: 579.0758287906647\n",
            "epoch 24 batch 14400 [144000/160000] training loss: 568.0506570339203\n",
            "epoch 24 batch 14500 [145000/160000] training loss: 570.050431728363\n",
            "epoch 24 batch 14600 [146000/160000] training loss: 578.737995505333\n",
            "epoch 24 batch 14700 [147000/160000] training loss: 595.0837621688843\n",
            "epoch 24 batch 14800 [148000/160000] training loss: 589.5571343898773\n",
            "epoch 24 batch 14900 [149000/160000] training loss: 603.289510011673\n",
            "epoch 24 batch 15000 [150000/160000] training loss: 599.603137254715\n",
            "epoch 24 batch 15100 [151000/160000] training loss: 568.454008102417\n",
            "epoch 24 batch 15200 [152000/160000] training loss: 593.2860708236694\n",
            "epoch 24 batch 15300 [153000/160000] training loss: 601.0612943172455\n",
            "epoch 24 batch 15400 [154000/160000] training loss: 572.8408718109131\n",
            "epoch 24 batch 15500 [155000/160000] training loss: 592.2817525863647\n",
            "epoch 24 batch 15600 [156000/160000] training loss: 565.4869441986084\n",
            "epoch 24 batch 15700 [157000/160000] training loss: 574.1813850402832\n",
            "epoch 24 batch 15800 [158000/160000] training loss: 543.970103263855\n",
            "epoch 24 batch 15900 [159000/160000] training loss: 549.0037877559662\n",
            "epoch 25 batch 0 [0/160000] training loss: 4.804863929748535\n",
            "epoch 25 batch 100 [1000/160000] training loss: 561.7307922840118\n",
            "epoch 25 batch 200 [2000/160000] training loss: 577.889680147171\n",
            "epoch 25 batch 300 [3000/160000] training loss: 591.8412189483643\n",
            "epoch 25 batch 400 [4000/160000] training loss: 553.1979956626892\n",
            "epoch 25 batch 500 [5000/160000] training loss: 611.1062560081482\n",
            "epoch 25 batch 600 [6000/160000] training loss: 586.633651137352\n",
            "epoch 25 batch 700 [7000/160000] training loss: 563.1456542015076\n",
            "epoch 25 batch 800 [8000/160000] training loss: 602.2511339187622\n",
            "epoch 25 batch 900 [9000/160000] training loss: 574.3147959709167\n",
            "epoch 25 batch 1000 [10000/160000] training loss: 581.9252744913101\n",
            "epoch 25 batch 1100 [11000/160000] training loss: 591.6176810264587\n",
            "epoch 25 batch 1200 [12000/160000] training loss: 575.6514892578125\n",
            "epoch 25 batch 1300 [13000/160000] training loss: 566.8464221954346\n",
            "epoch 25 batch 1400 [14000/160000] training loss: 568.1175246238708\n",
            "epoch 25 batch 1500 [15000/160000] training loss: 531.9660930633545\n",
            "epoch 25 batch 1600 [16000/160000] training loss: 568.8484032154083\n",
            "epoch 25 batch 1700 [17000/160000] training loss: 559.1635172367096\n",
            "epoch 25 batch 1800 [18000/160000] training loss: 551.0370655059814\n",
            "epoch 25 batch 1900 [19000/160000] training loss: 585.4871082305908\n",
            "epoch 25 batch 2000 [20000/160000] training loss: 548.9223766326904\n",
            "epoch 25 batch 2100 [21000/160000] training loss: 561.7552292346954\n",
            "epoch 25 batch 2200 [22000/160000] training loss: 593.870847940445\n",
            "epoch 25 batch 2300 [23000/160000] training loss: 582.4787658452988\n",
            "epoch 25 batch 2400 [24000/160000] training loss: 584.0175530910492\n",
            "epoch 25 batch 2500 [25000/160000] training loss: 576.9537699222565\n",
            "epoch 25 batch 2600 [26000/160000] training loss: 549.8620743751526\n",
            "epoch 25 batch 2700 [27000/160000] training loss: 582.626247882843\n",
            "epoch 25 batch 2800 [28000/160000] training loss: 581.9385249614716\n",
            "epoch 25 batch 2900 [29000/160000] training loss: 630.0053777694702\n",
            "epoch 25 batch 3000 [30000/160000] training loss: 554.5847635269165\n",
            "epoch 25 batch 3100 [31000/160000] training loss: 592.2342598438263\n",
            "epoch 25 batch 3200 [32000/160000] training loss: 563.6814732551575\n",
            "epoch 25 batch 3300 [33000/160000] training loss: 560.5093386173248\n",
            "epoch 25 batch 3400 [34000/160000] training loss: 561.2175755500793\n",
            "epoch 25 batch 3500 [35000/160000] training loss: 566.9541466236115\n",
            "epoch 25 batch 3600 [36000/160000] training loss: 602.3497612476349\n",
            "epoch 25 batch 3700 [37000/160000] training loss: 550.436688542366\n",
            "epoch 25 batch 3800 [38000/160000] training loss: 566.4365032911301\n",
            "epoch 25 batch 3900 [39000/160000] training loss: 602.5416429042816\n",
            "epoch 25 batch 4000 [40000/160000] training loss: 564.620852470398\n",
            "epoch 25 batch 4100 [41000/160000] training loss: 574.6210486888885\n",
            "epoch 25 batch 4200 [42000/160000] training loss: 615.374303817749\n",
            "epoch 25 batch 4300 [43000/160000] training loss: 574.5777155160904\n",
            "epoch 25 batch 4400 [44000/160000] training loss: 553.6498186588287\n",
            "epoch 25 batch 4500 [45000/160000] training loss: 588.5038504600525\n",
            "epoch 25 batch 4600 [46000/160000] training loss: 565.0012674331665\n",
            "epoch 25 batch 4700 [47000/160000] training loss: 536.9078731536865\n",
            "epoch 25 batch 4800 [48000/160000] training loss: 578.9824279546738\n",
            "epoch 25 batch 4900 [49000/160000] training loss: 574.478547334671\n",
            "epoch 25 batch 5000 [50000/160000] training loss: 548.3007609844208\n",
            "epoch 25 batch 5100 [51000/160000] training loss: 558.8318085670471\n",
            "epoch 25 batch 5200 [52000/160000] training loss: 577.6547565460205\n",
            "epoch 25 batch 5300 [53000/160000] training loss: 600.2480683326721\n",
            "epoch 25 batch 5400 [54000/160000] training loss: 586.8185510635376\n",
            "epoch 25 batch 5500 [55000/160000] training loss: 560.4277317523956\n",
            "epoch 25 batch 5600 [56000/160000] training loss: 561.0431318283081\n",
            "epoch 25 batch 5700 [57000/160000] training loss: 576.710018157959\n",
            "epoch 25 batch 5800 [58000/160000] training loss: 566.1865606307983\n",
            "epoch 25 batch 5900 [59000/160000] training loss: 556.8923279047012\n",
            "epoch 25 batch 6000 [60000/160000] training loss: 566.6923277378082\n",
            "epoch 25 batch 6100 [61000/160000] training loss: 605.9448871612549\n",
            "epoch 25 batch 6200 [62000/160000] training loss: 607.9113562107086\n",
            "epoch 25 batch 6300 [63000/160000] training loss: 599.161890745163\n",
            "epoch 25 batch 6400 [64000/160000] training loss: 608.2355043888092\n",
            "epoch 25 batch 6500 [65000/160000] training loss: 568.0789835453033\n",
            "epoch 25 batch 6600 [66000/160000] training loss: 567.1183774471283\n",
            "epoch 25 batch 6700 [67000/160000] training loss: 584.7002336978912\n",
            "epoch 25 batch 6800 [68000/160000] training loss: 571.9883085489273\n",
            "epoch 25 batch 6900 [69000/160000] training loss: 599.030764579773\n",
            "epoch 25 batch 7000 [70000/160000] training loss: 585.7114527225494\n",
            "epoch 25 batch 7100 [71000/160000] training loss: 570.7679264545441\n",
            "epoch 25 batch 7200 [72000/160000] training loss: 575.9621362686157\n",
            "epoch 25 batch 7300 [73000/160000] training loss: 550.3949782848358\n",
            "epoch 25 batch 7400 [74000/160000] training loss: 563.8171827793121\n",
            "epoch 25 batch 7500 [75000/160000] training loss: 546.6772768497467\n",
            "epoch 25 batch 7600 [76000/160000] training loss: 564.5045490264893\n",
            "epoch 25 batch 7700 [77000/160000] training loss: 589.585039138794\n",
            "epoch 25 batch 7800 [78000/160000] training loss: 614.1595709323883\n",
            "epoch 25 batch 7900 [79000/160000] training loss: 571.1817650794983\n",
            "epoch 25 batch 8000 [80000/160000] training loss: 567.6905555725098\n",
            "epoch 25 batch 8100 [81000/160000] training loss: 557.8863425254822\n",
            "epoch 25 batch 8200 [82000/160000] training loss: 597.1782550811768\n",
            "epoch 25 batch 8300 [83000/160000] training loss: 599.9561183452606\n",
            "epoch 25 batch 8400 [84000/160000] training loss: 552.2397149801254\n",
            "epoch 25 batch 8500 [85000/160000] training loss: 611.7484619617462\n",
            "epoch 25 batch 8600 [86000/160000] training loss: 573.1543343067169\n",
            "epoch 25 batch 8700 [87000/160000] training loss: 579.8535244464874\n",
            "epoch 25 batch 8800 [88000/160000] training loss: 600.1510815620422\n",
            "epoch 25 batch 8900 [89000/160000] training loss: 605.4177694320679\n",
            "epoch 25 batch 9000 [90000/160000] training loss: 585.6515655517578\n",
            "epoch 25 batch 9100 [91000/160000] training loss: 571.8731424808502\n",
            "epoch 25 batch 9200 [92000/160000] training loss: 573.8018803596497\n",
            "epoch 25 batch 9300 [93000/160000] training loss: 574.2225205898285\n",
            "epoch 25 batch 9400 [94000/160000] training loss: 559.6521347761154\n",
            "epoch 25 batch 9500 [95000/160000] training loss: 577.0568690299988\n",
            "epoch 25 batch 9600 [96000/160000] training loss: 588.225227355957\n",
            "epoch 25 batch 9700 [97000/160000] training loss: 574.1792621612549\n",
            "epoch 25 batch 9800 [98000/160000] training loss: 539.3764531612396\n",
            "epoch 25 batch 9900 [99000/160000] training loss: 578.0894951820374\n",
            "epoch 25 batch 10000 [100000/160000] training loss: 604.4783306121826\n",
            "epoch 25 batch 10100 [101000/160000] training loss: 578.2310843467712\n",
            "epoch 25 batch 10200 [102000/160000] training loss: 531.9487862586975\n",
            "epoch 25 batch 10300 [103000/160000] training loss: 568.9508919715881\n",
            "epoch 25 batch 10400 [104000/160000] training loss: 553.1758596897125\n",
            "epoch 25 batch 10500 [105000/160000] training loss: 638.0475435256958\n",
            "epoch 25 batch 10600 [106000/160000] training loss: 609.9029397964478\n",
            "epoch 25 batch 10700 [107000/160000] training loss: 568.1559488773346\n",
            "epoch 25 batch 10800 [108000/160000] training loss: 568.5141708850861\n",
            "epoch 25 batch 10900 [109000/160000] training loss: 541.0635669231415\n",
            "epoch 25 batch 11000 [110000/160000] training loss: 584.830849647522\n",
            "epoch 25 batch 11100 [111000/160000] training loss: 583.4246022701263\n",
            "epoch 25 batch 11200 [112000/160000] training loss: 569.0510280132294\n",
            "epoch 25 batch 11300 [113000/160000] training loss: 575.6391181945801\n",
            "epoch 25 batch 11400 [114000/160000] training loss: 598.293345451355\n",
            "epoch 25 batch 11500 [115000/160000] training loss: 565.6680595874786\n",
            "epoch 25 batch 11600 [116000/160000] training loss: 576.401025056839\n",
            "epoch 25 batch 11700 [117000/160000] training loss: 561.6052026748657\n",
            "epoch 25 batch 11800 [118000/160000] training loss: 572.5543129444122\n",
            "epoch 25 batch 11900 [119000/160000] training loss: 555.4844920635223\n",
            "epoch 25 batch 12000 [120000/160000] training loss: 572.4893136024475\n",
            "epoch 25 batch 12100 [121000/160000] training loss: 557.0193245410919\n",
            "epoch 25 batch 12200 [122000/160000] training loss: 546.6998815536499\n",
            "epoch 25 batch 12300 [123000/160000] training loss: 580.2840826511383\n",
            "epoch 25 batch 12400 [124000/160000] training loss: 580.5608999729156\n",
            "epoch 25 batch 12500 [125000/160000] training loss: 557.5173736810684\n",
            "epoch 25 batch 12600 [126000/160000] training loss: 560.6322846412659\n",
            "epoch 25 batch 12700 [127000/160000] training loss: 596.2788450717926\n",
            "epoch 25 batch 12800 [128000/160000] training loss: 602.1464918851852\n",
            "epoch 25 batch 12900 [129000/160000] training loss: 584.3504824638367\n",
            "epoch 25 batch 13000 [130000/160000] training loss: 555.1836540699005\n",
            "epoch 25 batch 13100 [131000/160000] training loss: 592.6985530853271\n",
            "epoch 25 batch 13200 [132000/160000] training loss: 625.5390315055847\n",
            "epoch 25 batch 13300 [133000/160000] training loss: 604.336327791214\n",
            "epoch 25 batch 13400 [134000/160000] training loss: 555.8746534585953\n",
            "epoch 25 batch 13500 [135000/160000] training loss: 579.2193584442139\n",
            "epoch 25 batch 13600 [136000/160000] training loss: 537.5316016674042\n",
            "epoch 25 batch 13700 [137000/160000] training loss: 616.2181663513184\n",
            "epoch 25 batch 13800 [138000/160000] training loss: 573.4517152309418\n",
            "epoch 25 batch 13900 [139000/160000] training loss: 610.7155653238297\n",
            "epoch 25 batch 14000 [140000/160000] training loss: 582.9178493022919\n",
            "epoch 25 batch 14100 [141000/160000] training loss: 597.5617346763611\n",
            "epoch 25 batch 14200 [142000/160000] training loss: 559.2312500476837\n",
            "epoch 25 batch 14300 [143000/160000] training loss: 579.0591762065887\n",
            "epoch 25 batch 14400 [144000/160000] training loss: 567.9069607257843\n",
            "epoch 25 batch 14500 [145000/160000] training loss: 570.3318195343018\n",
            "epoch 25 batch 14600 [146000/160000] training loss: 578.6049673557281\n",
            "epoch 25 batch 14700 [147000/160000] training loss: 595.8688642978668\n",
            "epoch 25 batch 14800 [148000/160000] training loss: 590.7574071884155\n",
            "epoch 25 batch 14900 [149000/160000] training loss: 602.4435374736786\n",
            "epoch 25 batch 15000 [150000/160000] training loss: 599.5742239952087\n",
            "epoch 25 batch 15100 [151000/160000] training loss: 568.0764448642731\n",
            "epoch 25 batch 15200 [152000/160000] training loss: 592.8271582126617\n",
            "epoch 25 batch 15300 [153000/160000] training loss: 601.3731341362\n",
            "epoch 25 batch 15400 [154000/160000] training loss: 572.1390504837036\n",
            "epoch 25 batch 15500 [155000/160000] training loss: 591.7871382236481\n",
            "epoch 25 batch 15600 [156000/160000] training loss: 566.0783030986786\n",
            "epoch 25 batch 15700 [157000/160000] training loss: 573.7982906103134\n",
            "epoch 25 batch 15800 [158000/160000] training loss: 544.3436517715454\n",
            "epoch 25 batch 15900 [159000/160000] training loss: 548.4599289894104\n",
            "epoch 26 batch 0 [0/160000] training loss: 4.8390703201293945\n",
            "epoch 26 batch 100 [1000/160000] training loss: 562.9389193058014\n",
            "epoch 26 batch 200 [2000/160000] training loss: 577.850751876831\n",
            "epoch 26 batch 300 [3000/160000] training loss: 593.2772135734558\n",
            "epoch 26 batch 400 [4000/160000] training loss: 554.470276594162\n",
            "epoch 26 batch 500 [5000/160000] training loss: 611.4866387844086\n",
            "epoch 26 batch 600 [6000/160000] training loss: 587.6389851570129\n",
            "epoch 26 batch 700 [7000/160000] training loss: 563.7274675369263\n",
            "epoch 26 batch 800 [8000/160000] training loss: 602.8847789764404\n",
            "epoch 26 batch 900 [9000/160000] training loss: 575.4426801204681\n",
            "epoch 26 batch 1000 [10000/160000] training loss: 583.092881321907\n",
            "epoch 26 batch 1100 [11000/160000] training loss: 592.5853335857391\n",
            "epoch 26 batch 1200 [12000/160000] training loss: 575.9055552482605\n",
            "epoch 26 batch 1300 [13000/160000] training loss: 567.1160262823105\n",
            "epoch 26 batch 1400 [14000/160000] training loss: 569.1624662876129\n",
            "epoch 26 batch 1500 [15000/160000] training loss: 532.6480758190155\n",
            "epoch 26 batch 1600 [16000/160000] training loss: 569.6090185642242\n",
            "epoch 26 batch 1700 [17000/160000] training loss: 559.5807874202728\n",
            "epoch 26 batch 1800 [18000/160000] training loss: 551.3843240737915\n",
            "epoch 26 batch 1900 [19000/160000] training loss: 585.6471846103668\n",
            "epoch 26 batch 2000 [20000/160000] training loss: 549.64568567276\n",
            "epoch 26 batch 2100 [21000/160000] training loss: 562.2262151241302\n",
            "epoch 26 batch 2200 [22000/160000] training loss: 594.5461469888687\n",
            "epoch 26 batch 2300 [23000/160000] training loss: 582.1848176717758\n",
            "epoch 26 batch 2400 [24000/160000] training loss: 584.4783842563629\n",
            "epoch 26 batch 2500 [25000/160000] training loss: 577.4933216571808\n",
            "epoch 26 batch 2600 [26000/160000] training loss: 549.9486334323883\n",
            "epoch 26 batch 2700 [27000/160000] training loss: 583.3330497741699\n",
            "epoch 26 batch 2800 [28000/160000] training loss: 581.6513707637787\n",
            "epoch 26 batch 2900 [29000/160000] training loss: 630.0266118049622\n",
            "epoch 26 batch 3000 [30000/160000] training loss: 554.7096967697144\n",
            "epoch 26 batch 3100 [31000/160000] training loss: 592.7640254497528\n",
            "epoch 26 batch 3200 [32000/160000] training loss: 563.5018494129181\n",
            "epoch 26 batch 3300 [33000/160000] training loss: 560.4114744663239\n",
            "epoch 26 batch 3400 [34000/160000] training loss: 561.4354450702667\n",
            "epoch 26 batch 3500 [35000/160000] training loss: 566.4674592018127\n",
            "epoch 26 batch 3600 [36000/160000] training loss: 602.388002872467\n",
            "epoch 26 batch 3700 [37000/160000] training loss: 551.3498983383179\n",
            "epoch 26 batch 3800 [38000/160000] training loss: 566.5751029253006\n",
            "epoch 26 batch 3900 [39000/160000] training loss: 602.6780033111572\n",
            "epoch 26 batch 4000 [40000/160000] training loss: 564.7195979356766\n",
            "epoch 26 batch 4100 [41000/160000] training loss: 575.5795269012451\n",
            "epoch 26 batch 4200 [42000/160000] training loss: 616.3234469890594\n",
            "epoch 26 batch 4300 [43000/160000] training loss: 575.0450482368469\n",
            "epoch 26 batch 4400 [44000/160000] training loss: 553.1242153644562\n",
            "epoch 26 batch 4500 [45000/160000] training loss: 588.8583886623383\n",
            "epoch 26 batch 4600 [46000/160000] training loss: 564.9895887374878\n",
            "epoch 26 batch 4700 [47000/160000] training loss: 536.6431350708008\n",
            "epoch 26 batch 4800 [48000/160000] training loss: 579.2207434177399\n",
            "epoch 26 batch 4900 [49000/160000] training loss: 574.9947235584259\n",
            "epoch 26 batch 5000 [50000/160000] training loss: 548.6682674884796\n",
            "epoch 26 batch 5100 [51000/160000] training loss: 558.9811294078827\n",
            "epoch 26 batch 5200 [52000/160000] training loss: 577.0084042549133\n",
            "epoch 26 batch 5300 [53000/160000] training loss: 600.9999351501465\n",
            "epoch 26 batch 5400 [54000/160000] training loss: 588.6078164577484\n",
            "epoch 26 batch 5500 [55000/160000] training loss: 560.5589914321899\n",
            "epoch 26 batch 5600 [56000/160000] training loss: 561.2392711639404\n",
            "epoch 26 batch 5700 [57000/160000] training loss: 577.0039029121399\n",
            "epoch 26 batch 5800 [58000/160000] training loss: 565.8542168140411\n",
            "epoch 26 batch 5900 [59000/160000] training loss: 556.6952910423279\n",
            "epoch 26 batch 6000 [60000/160000] training loss: 566.9981048107147\n",
            "epoch 26 batch 6100 [61000/160000] training loss: 605.6820313930511\n",
            "epoch 26 batch 6200 [62000/160000] training loss: 607.506495475769\n",
            "epoch 26 batch 6300 [63000/160000] training loss: 599.4805362224579\n",
            "epoch 26 batch 6400 [64000/160000] training loss: 607.9021632671356\n",
            "epoch 26 batch 6500 [65000/160000] training loss: 568.1118144989014\n",
            "epoch 26 batch 6600 [66000/160000] training loss: 566.9039881229401\n",
            "epoch 26 batch 6700 [67000/160000] training loss: 584.4929525852203\n",
            "epoch 26 batch 6800 [68000/160000] training loss: 572.0672554969788\n",
            "epoch 26 batch 6900 [69000/160000] training loss: 598.7772898674011\n",
            "epoch 26 batch 7000 [70000/160000] training loss: 585.6969718933105\n",
            "epoch 26 batch 7100 [71000/160000] training loss: 570.3018698692322\n",
            "epoch 26 batch 7200 [72000/160000] training loss: 575.7963252067566\n",
            "epoch 26 batch 7300 [73000/160000] training loss: 550.4500653743744\n",
            "epoch 26 batch 7400 [74000/160000] training loss: 563.7270259857178\n",
            "epoch 26 batch 7500 [75000/160000] training loss: 546.9556770324707\n",
            "epoch 26 batch 7600 [76000/160000] training loss: 566.2962808609009\n",
            "epoch 26 batch 7700 [77000/160000] training loss: 590.5349645614624\n",
            "epoch 26 batch 7800 [78000/160000] training loss: 614.1007707118988\n",
            "epoch 26 batch 7900 [79000/160000] training loss: 570.744252204895\n",
            "epoch 26 batch 8000 [80000/160000] training loss: 567.6821429729462\n",
            "epoch 26 batch 8100 [81000/160000] training loss: 559.6732505559921\n",
            "epoch 26 batch 8200 [82000/160000] training loss: 597.2083842754364\n",
            "epoch 26 batch 8300 [83000/160000] training loss: 600.4263386726379\n",
            "epoch 26 batch 8400 [84000/160000] training loss: 552.8236684799194\n",
            "epoch 26 batch 8500 [85000/160000] training loss: 611.3734568357468\n",
            "epoch 26 batch 8600 [86000/160000] training loss: 573.2318894863129\n",
            "epoch 26 batch 8700 [87000/160000] training loss: 579.4136190414429\n",
            "epoch 26 batch 8800 [88000/160000] training loss: 600.5290021896362\n",
            "epoch 26 batch 8900 [89000/160000] training loss: 606.1824713945389\n",
            "epoch 26 batch 9000 [90000/160000] training loss: 585.3254606723785\n",
            "epoch 26 batch 9100 [91000/160000] training loss: 571.7675724029541\n",
            "epoch 26 batch 9200 [92000/160000] training loss: 573.9997508525848\n",
            "epoch 26 batch 9300 [93000/160000] training loss: 573.4644441604614\n",
            "epoch 26 batch 9400 [94000/160000] training loss: 559.195515871048\n",
            "epoch 26 batch 9500 [95000/160000] training loss: 576.9007153511047\n",
            "epoch 26 batch 9600 [96000/160000] training loss: 587.6319551467896\n",
            "epoch 26 batch 9700 [97000/160000] training loss: 573.6194703578949\n",
            "epoch 26 batch 9800 [98000/160000] training loss: 538.9702544212341\n",
            "epoch 26 batch 9900 [99000/160000] training loss: 578.353432893753\n",
            "epoch 26 batch 10000 [100000/160000] training loss: 604.3219308853149\n",
            "epoch 26 batch 10100 [101000/160000] training loss: 577.9008047580719\n",
            "epoch 26 batch 10200 [102000/160000] training loss: 532.1152102947235\n",
            "epoch 26 batch 10300 [103000/160000] training loss: 569.1220989227295\n",
            "epoch 26 batch 10400 [104000/160000] training loss: 552.7266943454742\n",
            "epoch 26 batch 10500 [105000/160000] training loss: 638.1180326938629\n",
            "epoch 26 batch 10600 [106000/160000] training loss: 610.0402722358704\n",
            "epoch 26 batch 10700 [107000/160000] training loss: 568.1417173147202\n",
            "epoch 26 batch 10800 [108000/160000] training loss: 568.3909919261932\n",
            "epoch 26 batch 10900 [109000/160000] training loss: 541.1818796396255\n",
            "epoch 26 batch 11000 [110000/160000] training loss: 585.4217901229858\n",
            "epoch 26 batch 11100 [111000/160000] training loss: 583.1165177822113\n",
            "epoch 26 batch 11200 [112000/160000] training loss: 568.6229672431946\n",
            "epoch 26 batch 11300 [113000/160000] training loss: 575.2327871322632\n",
            "epoch 26 batch 11400 [114000/160000] training loss: 598.5347913503647\n",
            "epoch 26 batch 11500 [115000/160000] training loss: 565.0108134746552\n",
            "epoch 26 batch 11600 [116000/160000] training loss: 576.6179043054581\n",
            "epoch 26 batch 11700 [117000/160000] training loss: 561.0068007707596\n",
            "epoch 26 batch 11800 [118000/160000] training loss: 572.7560673952103\n",
            "epoch 26 batch 11900 [119000/160000] training loss: 554.8571681976318\n",
            "epoch 26 batch 12000 [120000/160000] training loss: 572.153048992157\n",
            "epoch 26 batch 12100 [121000/160000] training loss: 556.7487025260925\n",
            "epoch 26 batch 12200 [122000/160000] training loss: 546.747586607933\n",
            "epoch 26 batch 12300 [123000/160000] training loss: 580.2802786827087\n",
            "epoch 26 batch 12400 [124000/160000] training loss: 580.0002636909485\n",
            "epoch 26 batch 12500 [125000/160000] training loss: 557.1503888368607\n",
            "epoch 26 batch 12600 [126000/160000] training loss: 560.4234893321991\n",
            "epoch 26 batch 12700 [127000/160000] training loss: 595.7990950345993\n",
            "epoch 26 batch 12800 [128000/160000] training loss: 601.4284255504608\n",
            "epoch 26 batch 12900 [129000/160000] training loss: 583.6998965740204\n",
            "epoch 26 batch 13000 [130000/160000] training loss: 554.8761718273163\n",
            "epoch 26 batch 13100 [131000/160000] training loss: 591.9720687866211\n",
            "epoch 26 batch 13200 [132000/160000] training loss: 619.1694071292877\n",
            "epoch 26 batch 13300 [133000/160000] training loss: 604.9622609615326\n",
            "epoch 26 batch 13400 [134000/160000] training loss: 555.5401906967163\n",
            "epoch 26 batch 13500 [135000/160000] training loss: 578.789044380188\n",
            "epoch 26 batch 13600 [136000/160000] training loss: 537.3137068748474\n",
            "epoch 26 batch 13700 [137000/160000] training loss: 615.9993646144867\n",
            "epoch 26 batch 13800 [138000/160000] training loss: 573.052496433258\n",
            "epoch 26 batch 13900 [139000/160000] training loss: 613.7546987533569\n",
            "epoch 26 batch 14000 [140000/160000] training loss: 582.3608064651489\n",
            "epoch 26 batch 14100 [141000/160000] training loss: 596.9824941158295\n",
            "epoch 26 batch 14200 [142000/160000] training loss: 558.3246476650238\n",
            "epoch 26 batch 14300 [143000/160000] training loss: 578.9671821594238\n",
            "epoch 26 batch 14400 [144000/160000] training loss: 567.994304895401\n",
            "epoch 26 batch 14500 [145000/160000] training loss: 570.5019087791443\n",
            "epoch 26 batch 14600 [146000/160000] training loss: 578.283322930336\n",
            "epoch 26 batch 14700 [147000/160000] training loss: 596.1571960449219\n",
            "epoch 26 batch 14800 [148000/160000] training loss: 591.5044500827789\n",
            "epoch 26 batch 14900 [149000/160000] training loss: 601.7655873298645\n",
            "epoch 26 batch 15000 [150000/160000] training loss: 599.6222627162933\n",
            "epoch 26 batch 15100 [151000/160000] training loss: 567.800286769867\n",
            "epoch 26 batch 15200 [152000/160000] training loss: 592.7035689353943\n",
            "epoch 26 batch 15300 [153000/160000] training loss: 601.5748851299286\n",
            "epoch 26 batch 15400 [154000/160000] training loss: 571.7501909732819\n",
            "epoch 26 batch 15500 [155000/160000] training loss: 591.431113243103\n",
            "epoch 26 batch 15600 [156000/160000] training loss: 566.7750070095062\n",
            "epoch 26 batch 15700 [157000/160000] training loss: 573.5751001834869\n",
            "epoch 26 batch 15800 [158000/160000] training loss: 544.6412110328674\n",
            "epoch 26 batch 15900 [159000/160000] training loss: 548.1839981079102\n",
            "epoch 27 batch 0 [0/160000] training loss: 4.875844955444336\n",
            "epoch 27 batch 100 [1000/160000] training loss: 563.7348921298981\n",
            "epoch 27 batch 200 [2000/160000] training loss: 577.5480751991272\n",
            "epoch 27 batch 300 [3000/160000] training loss: 593.9668562412262\n",
            "epoch 27 batch 400 [4000/160000] training loss: 555.1212130784988\n",
            "epoch 27 batch 500 [5000/160000] training loss: 611.6947400569916\n",
            "epoch 27 batch 600 [6000/160000] training loss: 588.4007444381714\n",
            "epoch 27 batch 700 [7000/160000] training loss: 563.9531116485596\n",
            "epoch 27 batch 800 [8000/160000] training loss: 603.3829851150513\n",
            "epoch 27 batch 900 [9000/160000] training loss: 576.4941816329956\n",
            "epoch 27 batch 1000 [10000/160000] training loss: 584.047384262085\n",
            "epoch 27 batch 1100 [11000/160000] training loss: 593.2375938892365\n",
            "epoch 27 batch 1200 [12000/160000] training loss: 575.9702543020248\n",
            "epoch 27 batch 1300 [13000/160000] training loss: 567.5605634450912\n",
            "epoch 27 batch 1400 [14000/160000] training loss: 569.8778266906738\n",
            "epoch 27 batch 1500 [15000/160000] training loss: 533.2646706104279\n",
            "epoch 27 batch 1600 [16000/160000] training loss: 570.0983512401581\n",
            "epoch 27 batch 1700 [17000/160000] training loss: 559.9666304588318\n",
            "epoch 27 batch 1800 [18000/160000] training loss: 551.494282245636\n",
            "epoch 27 batch 1900 [19000/160000] training loss: 585.6677865982056\n",
            "epoch 27 batch 2000 [20000/160000] training loss: 550.0663638114929\n",
            "epoch 27 batch 2100 [21000/160000] training loss: 562.4832022190094\n",
            "epoch 27 batch 2200 [22000/160000] training loss: 594.9275195598602\n",
            "epoch 27 batch 2300 [23000/160000] training loss: 581.9036650657654\n",
            "epoch 27 batch 2400 [24000/160000] training loss: 584.7142112255096\n",
            "epoch 27 batch 2500 [25000/160000] training loss: 577.8054366111755\n",
            "epoch 27 batch 2600 [26000/160000] training loss: 549.9086692333221\n",
            "epoch 27 batch 2700 [27000/160000] training loss: 583.8399815559387\n",
            "epoch 27 batch 2800 [28000/160000] training loss: 581.5464651584625\n",
            "epoch 27 batch 2900 [29000/160000] training loss: 629.8189935684204\n",
            "epoch 27 batch 3000 [30000/160000] training loss: 554.7270107269287\n",
            "epoch 27 batch 3100 [31000/160000] training loss: 593.124737739563\n",
            "epoch 27 batch 3200 [32000/160000] training loss: 563.1769473552704\n",
            "epoch 27 batch 3300 [33000/160000] training loss: 560.606906414032\n",
            "epoch 27 batch 3400 [34000/160000] training loss: 561.4361534118652\n",
            "epoch 27 batch 3500 [35000/160000] training loss: 566.1916879415512\n",
            "epoch 27 batch 3600 [36000/160000] training loss: 602.2904047966003\n",
            "epoch 27 batch 3700 [37000/160000] training loss: 552.0495784282684\n",
            "epoch 27 batch 3800 [38000/160000] training loss: 566.7103897333145\n",
            "epoch 27 batch 3900 [39000/160000] training loss: 602.7331693172455\n",
            "epoch 27 batch 4000 [40000/160000] training loss: 564.755202293396\n",
            "epoch 27 batch 4100 [41000/160000] training loss: 577.1295701265335\n",
            "epoch 27 batch 4200 [42000/160000] training loss: 616.863893032074\n",
            "epoch 27 batch 4300 [43000/160000] training loss: 575.6735180616379\n",
            "epoch 27 batch 4400 [44000/160000] training loss: 552.8240830898285\n",
            "epoch 27 batch 4500 [45000/160000] training loss: 589.1120908260345\n",
            "epoch 27 batch 4600 [46000/160000] training loss: 565.0721499919891\n",
            "epoch 27 batch 4700 [47000/160000] training loss: 536.6510838270187\n",
            "epoch 27 batch 4800 [48000/160000] training loss: 579.4711869955063\n",
            "epoch 27 batch 4900 [49000/160000] training loss: 575.6326892375946\n",
            "epoch 27 batch 5000 [50000/160000] training loss: 548.9003902673721\n",
            "epoch 27 batch 5100 [51000/160000] training loss: 558.9506039619446\n",
            "epoch 27 batch 5200 [52000/160000] training loss: 576.3715870380402\n",
            "epoch 27 batch 5300 [53000/160000] training loss: 601.0846228599548\n",
            "epoch 27 batch 5400 [54000/160000] training loss: 590.3194246292114\n",
            "epoch 27 batch 5500 [55000/160000] training loss: 560.5943109989166\n",
            "epoch 27 batch 5600 [56000/160000] training loss: 561.3781037330627\n",
            "epoch 27 batch 5700 [57000/160000] training loss: 577.0690732002258\n",
            "epoch 27 batch 5800 [58000/160000] training loss: 565.5107090473175\n",
            "epoch 27 batch 5900 [59000/160000] training loss: 556.5196928977966\n",
            "epoch 27 batch 6000 [60000/160000] training loss: 567.2007155418396\n",
            "epoch 27 batch 6100 [61000/160000] training loss: 605.4591791629791\n",
            "epoch 27 batch 6200 [62000/160000] training loss: 606.9282472133636\n",
            "epoch 27 batch 6300 [63000/160000] training loss: 599.8325097560883\n",
            "epoch 27 batch 6400 [64000/160000] training loss: 607.5760250091553\n",
            "epoch 27 batch 6500 [65000/160000] training loss: 568.1613059043884\n",
            "epoch 27 batch 6600 [66000/160000] training loss: 566.6340363025665\n",
            "epoch 27 batch 6700 [67000/160000] training loss: 584.4822850227356\n",
            "epoch 27 batch 6800 [68000/160000] training loss: 572.2471438646317\n",
            "epoch 27 batch 6900 [69000/160000] training loss: 598.5100150108337\n",
            "epoch 27 batch 7000 [70000/160000] training loss: 585.7915260791779\n",
            "epoch 27 batch 7100 [71000/160000] training loss: 569.8625774383545\n",
            "epoch 27 batch 7200 [72000/160000] training loss: 575.609154343605\n",
            "epoch 27 batch 7300 [73000/160000] training loss: 550.4242650270462\n",
            "epoch 27 batch 7400 [74000/160000] training loss: 563.6964933872223\n",
            "epoch 27 batch 7500 [75000/160000] training loss: 547.8424534797668\n",
            "epoch 27 batch 7600 [76000/160000] training loss: 566.9661257266998\n",
            "epoch 27 batch 7700 [77000/160000] training loss: 591.3070712089539\n",
            "epoch 27 batch 7800 [78000/160000] training loss: 614.1226277351379\n",
            "epoch 27 batch 7900 [79000/160000] training loss: 570.5669968128204\n",
            "epoch 27 batch 8000 [80000/160000] training loss: 567.6807198524475\n",
            "epoch 27 batch 8100 [81000/160000] training loss: 560.7974268198013\n",
            "epoch 27 batch 8200 [82000/160000] training loss: 597.2371444702148\n",
            "epoch 27 batch 8300 [83000/160000] training loss: 600.6610045433044\n",
            "epoch 27 batch 8400 [84000/160000] training loss: 553.0358673334122\n",
            "epoch 27 batch 8500 [85000/160000] training loss: 611.0266848802567\n",
            "epoch 27 batch 8600 [86000/160000] training loss: 573.5300779342651\n",
            "epoch 27 batch 8700 [87000/160000] training loss: 579.1312220096588\n",
            "epoch 27 batch 8800 [88000/160000] training loss: 600.7006547451019\n",
            "epoch 27 batch 8900 [89000/160000] training loss: 606.374708533287\n",
            "epoch 27 batch 9000 [90000/160000] training loss: 584.8461313247681\n",
            "epoch 27 batch 9100 [91000/160000] training loss: 571.8266860246658\n",
            "epoch 27 batch 9200 [92000/160000] training loss: 574.1910328865051\n",
            "epoch 27 batch 9300 [93000/160000] training loss: 572.9774935245514\n",
            "epoch 27 batch 9400 [94000/160000] training loss: 558.777222275734\n",
            "epoch 27 batch 9500 [95000/160000] training loss: 576.7469847202301\n",
            "epoch 27 batch 9600 [96000/160000] training loss: 587.2197093963623\n",
            "epoch 27 batch 9700 [97000/160000] training loss: 573.2267210483551\n",
            "epoch 27 batch 9800 [98000/160000] training loss: 538.6815638542175\n",
            "epoch 27 batch 9900 [99000/160000] training loss: 578.4739718437195\n",
            "epoch 27 batch 10000 [100000/160000] training loss: 604.1330497264862\n",
            "epoch 27 batch 10100 [101000/160000] training loss: 577.6491842269897\n",
            "epoch 27 batch 10200 [102000/160000] training loss: 532.1602897644043\n",
            "epoch 27 batch 10300 [103000/160000] training loss: 569.2234013080597\n",
            "epoch 27 batch 10400 [104000/160000] training loss: 552.4088842868805\n",
            "epoch 27 batch 10500 [105000/160000] training loss: 637.8909289836884\n",
            "epoch 27 batch 10600 [106000/160000] training loss: 610.3452742099762\n",
            "epoch 27 batch 10700 [107000/160000] training loss: 568.0837763547897\n",
            "epoch 27 batch 10800 [108000/160000] training loss: 568.3084344863892\n",
            "epoch 27 batch 10900 [109000/160000] training loss: 541.3055679798126\n",
            "epoch 27 batch 11000 [110000/160000] training loss: 586.1095976829529\n",
            "epoch 27 batch 11100 [111000/160000] training loss: 582.8372514247894\n",
            "epoch 27 batch 11200 [112000/160000] training loss: 568.2650833129883\n",
            "epoch 27 batch 11300 [113000/160000] training loss: 574.8620548248291\n",
            "epoch 27 batch 11400 [114000/160000] training loss: 598.775198340416\n",
            "epoch 27 batch 11500 [115000/160000] training loss: 564.455627322197\n",
            "epoch 27 batch 11600 [116000/160000] training loss: 576.9003437757492\n",
            "epoch 27 batch 11700 [117000/160000] training loss: 560.3665920495987\n",
            "epoch 27 batch 11800 [118000/160000] training loss: 572.7670520544052\n",
            "epoch 27 batch 11900 [119000/160000] training loss: 554.3104680776596\n",
            "epoch 27 batch 12000 [120000/160000] training loss: 571.8020296096802\n",
            "epoch 27 batch 12100 [121000/160000] training loss: 556.8026096820831\n",
            "epoch 27 batch 12200 [122000/160000] training loss: 546.8638050556183\n",
            "epoch 27 batch 12300 [123000/160000] training loss: 580.3763726949692\n",
            "epoch 27 batch 12400 [124000/160000] training loss: 579.5178434848785\n",
            "epoch 27 batch 12500 [125000/160000] training loss: 556.6125155687332\n",
            "epoch 27 batch 12600 [126000/160000] training loss: 560.1269401311874\n",
            "epoch 27 batch 12700 [127000/160000] training loss: 595.5542237758636\n",
            "epoch 27 batch 12800 [128000/160000] training loss: 600.7644526958466\n",
            "epoch 27 batch 12900 [129000/160000] training loss: 583.2164307832718\n",
            "epoch 27 batch 13000 [130000/160000] training loss: 554.6484024524689\n",
            "epoch 27 batch 13100 [131000/160000] training loss: 591.357691526413\n",
            "epoch 27 batch 13200 [132000/160000] training loss: 615.4814176559448\n",
            "epoch 27 batch 13300 [133000/160000] training loss: 604.0139155387878\n",
            "epoch 27 batch 13400 [134000/160000] training loss: 555.2894861698151\n",
            "epoch 27 batch 13500 [135000/160000] training loss: 578.4625170230865\n",
            "epoch 27 batch 13600 [136000/160000] training loss: 537.2212278842926\n",
            "epoch 27 batch 13700 [137000/160000] training loss: 615.8144748210907\n",
            "epoch 27 batch 13800 [138000/160000] training loss: 572.8946197032928\n",
            "epoch 27 batch 13900 [139000/160000] training loss: 615.9976902008057\n",
            "epoch 27 batch 14000 [140000/160000] training loss: 581.7886099815369\n",
            "epoch 27 batch 14100 [141000/160000] training loss: 596.6912953853607\n",
            "epoch 27 batch 14200 [142000/160000] training loss: 557.52672290802\n",
            "epoch 27 batch 14300 [143000/160000] training loss: 578.9487855434418\n",
            "epoch 27 batch 14400 [144000/160000] training loss: 568.1530313491821\n",
            "epoch 27 batch 14500 [145000/160000] training loss: 570.6498527526855\n",
            "epoch 27 batch 14600 [146000/160000] training loss: 578.087385892868\n",
            "epoch 27 batch 14700 [147000/160000] training loss: 596.3315279483795\n",
            "epoch 27 batch 14800 [148000/160000] training loss: 591.9497382640839\n",
            "epoch 27 batch 14900 [149000/160000] training loss: 601.1986410617828\n",
            "epoch 27 batch 15000 [150000/160000] training loss: 599.6102077960968\n",
            "epoch 27 batch 15100 [151000/160000] training loss: 567.570262670517\n",
            "epoch 27 batch 15200 [152000/160000] training loss: 592.7245533466339\n",
            "epoch 27 batch 15300 [153000/160000] training loss: 601.827775478363\n",
            "epoch 27 batch 15400 [154000/160000] training loss: 571.4624555110931\n",
            "epoch 27 batch 15500 [155000/160000] training loss: 590.8721120357513\n",
            "epoch 27 batch 15600 [156000/160000] training loss: 567.1302840709686\n",
            "epoch 27 batch 15700 [157000/160000] training loss: 573.4169219732285\n",
            "epoch 27 batch 15800 [158000/160000] training loss: 544.869464635849\n",
            "epoch 27 batch 15900 [159000/160000] training loss: 547.9934637546539\n",
            "epoch 28 batch 0 [0/160000] training loss: 4.915731430053711\n",
            "epoch 28 batch 100 [1000/160000] training loss: 564.2063148021698\n",
            "epoch 28 batch 200 [2000/160000] training loss: 577.2713642120361\n",
            "epoch 28 batch 300 [3000/160000] training loss: 594.2354633808136\n",
            "epoch 28 batch 400 [4000/160000] training loss: 555.5337889194489\n",
            "epoch 28 batch 500 [5000/160000] training loss: 611.8021514415741\n",
            "epoch 28 batch 600 [6000/160000] training loss: 589.0764243602753\n",
            "epoch 28 batch 700 [7000/160000] training loss: 564.1574323177338\n",
            "epoch 28 batch 800 [8000/160000] training loss: 603.7131872177124\n",
            "epoch 28 batch 900 [9000/160000] training loss: 577.3538219928741\n",
            "epoch 28 batch 1000 [10000/160000] training loss: 584.7603192329407\n",
            "epoch 28 batch 1100 [11000/160000] training loss: 593.7094368934631\n",
            "epoch 28 batch 1200 [12000/160000] training loss: 575.9645211696625\n",
            "epoch 28 batch 1300 [13000/160000] training loss: 568.066164970398\n",
            "epoch 28 batch 1400 [14000/160000] training loss: 570.4484190940857\n",
            "epoch 28 batch 1500 [15000/160000] training loss: 533.8034410476685\n",
            "epoch 28 batch 1600 [16000/160000] training loss: 570.4409835338593\n",
            "epoch 28 batch 1700 [17000/160000] training loss: 560.2171466350555\n",
            "epoch 28 batch 1800 [18000/160000] training loss: 551.5280442237854\n",
            "epoch 28 batch 1900 [19000/160000] training loss: 585.6276288032532\n",
            "epoch 28 batch 2000 [20000/160000] training loss: 550.0845696926117\n",
            "epoch 28 batch 2100 [21000/160000] training loss: 562.645831823349\n",
            "epoch 28 batch 2200 [22000/160000] training loss: 595.0891726016998\n",
            "epoch 28 batch 2300 [23000/160000] training loss: 581.7027597427368\n",
            "epoch 28 batch 2400 [24000/160000] training loss: 584.9181959629059\n",
            "epoch 28 batch 2500 [25000/160000] training loss: 578.0748097896576\n",
            "epoch 28 batch 2600 [26000/160000] training loss: 549.8230721950531\n",
            "epoch 28 batch 2700 [27000/160000] training loss: 584.2499914169312\n",
            "epoch 28 batch 2800 [28000/160000] training loss: 581.47165620327\n",
            "epoch 28 batch 2900 [29000/160000] training loss: 629.4939515590668\n",
            "epoch 28 batch 3000 [30000/160000] training loss: 554.6966791152954\n",
            "epoch 28 batch 3100 [31000/160000] training loss: 593.4280133247375\n",
            "epoch 28 batch 3200 [32000/160000] training loss: 562.8115663528442\n",
            "epoch 28 batch 3300 [33000/160000] training loss: 560.9117457866669\n",
            "epoch 28 batch 3400 [34000/160000] training loss: 561.4040868282318\n",
            "epoch 28 batch 3500 [35000/160000] training loss: 566.0610989332199\n",
            "epoch 28 batch 3600 [36000/160000] training loss: 602.1528227329254\n",
            "epoch 28 batch 3700 [37000/160000] training loss: 552.6430218219757\n",
            "epoch 28 batch 3800 [38000/160000] training loss: 566.8182668685913\n",
            "epoch 28 batch 3900 [39000/160000] training loss: 602.7010734081268\n",
            "epoch 28 batch 4000 [40000/160000] training loss: 564.7376787662506\n",
            "epoch 28 batch 4100 [41000/160000] training loss: 578.3262333869934\n",
            "epoch 28 batch 4200 [42000/160000] training loss: 618.2158260345459\n",
            "epoch 28 batch 4300 [43000/160000] training loss: 576.4184694290161\n",
            "epoch 28 batch 4400 [44000/160000] training loss: 552.641499042511\n",
            "epoch 28 batch 4500 [45000/160000] training loss: 589.2619934082031\n",
            "epoch 28 batch 4600 [46000/160000] training loss: 565.0701310634613\n",
            "epoch 28 batch 4700 [47000/160000] training loss: 536.8732392787933\n",
            "epoch 28 batch 4800 [48000/160000] training loss: 579.7207975387573\n",
            "epoch 28 batch 4900 [49000/160000] training loss: 576.1589748859406\n",
            "epoch 28 batch 5000 [50000/160000] training loss: 549.3184745311737\n",
            "epoch 28 batch 5100 [51000/160000] training loss: 558.723007440567\n",
            "epoch 28 batch 5200 [52000/160000] training loss: 575.7792670726776\n",
            "epoch 28 batch 5300 [53000/160000] training loss: 600.5552966594696\n",
            "epoch 28 batch 5400 [54000/160000] training loss: 591.3372292518616\n",
            "epoch 28 batch 5500 [55000/160000] training loss: 560.634734749794\n",
            "epoch 28 batch 5600 [56000/160000] training loss: 561.5748555660248\n",
            "epoch 28 batch 5700 [57000/160000] training loss: 577.1251363754272\n",
            "epoch 28 batch 5800 [58000/160000] training loss: 565.1072731018066\n",
            "epoch 28 batch 5900 [59000/160000] training loss: 556.4343914985657\n",
            "epoch 28 batch 6000 [60000/160000] training loss: 567.2861869335175\n",
            "epoch 28 batch 6100 [61000/160000] training loss: 605.4282591342926\n",
            "epoch 28 batch 6200 [62000/160000] training loss: 606.1056261062622\n",
            "epoch 28 batch 6300 [63000/160000] training loss: 600.3100106716156\n",
            "epoch 28 batch 6400 [64000/160000] training loss: 607.3131210803986\n",
            "epoch 28 batch 6500 [65000/160000] training loss: 568.2174637317657\n",
            "epoch 28 batch 6600 [66000/160000] training loss: 566.3662219047546\n",
            "epoch 28 batch 6700 [67000/160000] training loss: 584.5225310325623\n",
            "epoch 28 batch 6800 [68000/160000] training loss: 572.4988641738892\n",
            "epoch 28 batch 6900 [69000/160000] training loss: 598.1408190727234\n",
            "epoch 28 batch 7000 [70000/160000] training loss: 586.0531969070435\n",
            "epoch 28 batch 7100 [71000/160000] training loss: 569.301390171051\n",
            "epoch 28 batch 7200 [72000/160000] training loss: 575.4416453838348\n",
            "epoch 28 batch 7300 [73000/160000] training loss: 550.3905584812164\n",
            "epoch 28 batch 7400 [74000/160000] training loss: 563.6687760353088\n",
            "epoch 28 batch 7500 [75000/160000] training loss: 549.3143496513367\n",
            "epoch 28 batch 7600 [76000/160000] training loss: 567.0709645748138\n",
            "epoch 28 batch 7700 [77000/160000] training loss: 591.9074647426605\n",
            "epoch 28 batch 7800 [78000/160000] training loss: 614.2385945320129\n",
            "epoch 28 batch 7900 [79000/160000] training loss: 570.5346283912659\n",
            "epoch 28 batch 8000 [80000/160000] training loss: 567.7502701282501\n",
            "epoch 28 batch 8100 [81000/160000] training loss: 561.2741137742996\n",
            "epoch 28 batch 8200 [82000/160000] training loss: 597.2490310668945\n",
            "epoch 28 batch 8300 [83000/160000] training loss: 600.7537834644318\n",
            "epoch 28 batch 8400 [84000/160000] training loss: 553.1895562410355\n",
            "epoch 28 batch 8500 [85000/160000] training loss: 610.6975704431534\n",
            "epoch 28 batch 8600 [86000/160000] training loss: 573.9263813495636\n",
            "epoch 28 batch 8700 [87000/160000] training loss: 578.9541983604431\n",
            "epoch 28 batch 8800 [88000/160000] training loss: 600.8100502490997\n",
            "epoch 28 batch 8900 [89000/160000] training loss: 606.2822538614273\n",
            "epoch 28 batch 9000 [90000/160000] training loss: 584.3874473571777\n",
            "epoch 28 batch 9100 [91000/160000] training loss: 571.9374310970306\n",
            "epoch 28 batch 9200 [92000/160000] training loss: 574.3576445579529\n",
            "epoch 28 batch 9300 [93000/160000] training loss: 572.658940076828\n",
            "epoch 28 batch 9400 [94000/160000] training loss: 558.3970196247101\n",
            "epoch 28 batch 9500 [95000/160000] training loss: 576.5662114620209\n",
            "epoch 28 batch 9600 [96000/160000] training loss: 586.9116644859314\n",
            "epoch 28 batch 9700 [97000/160000] training loss: 572.9393932819366\n",
            "epoch 28 batch 9800 [98000/160000] training loss: 538.5015952587128\n",
            "epoch 28 batch 9900 [99000/160000] training loss: 578.5215454101562\n",
            "epoch 28 batch 10000 [100000/160000] training loss: 603.9559903144836\n",
            "epoch 28 batch 10100 [101000/160000] training loss: 577.5355813503265\n",
            "epoch 28 batch 10200 [102000/160000] training loss: 532.108503818512\n",
            "epoch 28 batch 10300 [103000/160000] training loss: 569.3324875831604\n",
            "epoch 28 batch 10400 [104000/160000] training loss: 552.2428848743439\n",
            "epoch 28 batch 10500 [105000/160000] training loss: 637.5395228862762\n",
            "epoch 28 batch 10600 [106000/160000] training loss: 610.6781768798828\n",
            "epoch 28 batch 10700 [107000/160000] training loss: 567.9441950321198\n",
            "epoch 28 batch 10800 [108000/160000] training loss: 568.2555334568024\n",
            "epoch 28 batch 10900 [109000/160000] training loss: 541.443122625351\n",
            "epoch 28 batch 11000 [110000/160000] training loss: 586.7016453742981\n",
            "epoch 28 batch 11100 [111000/160000] training loss: 582.60582447052\n",
            "epoch 28 batch 11200 [112000/160000] training loss: 567.920872092247\n",
            "epoch 28 batch 11300 [113000/160000] training loss: 574.5897583961487\n",
            "epoch 28 batch 11400 [114000/160000] training loss: 598.9708961248398\n",
            "epoch 28 batch 11500 [115000/160000] training loss: 563.9862070083618\n",
            "epoch 28 batch 11600 [116000/160000] training loss: 577.1921719312668\n",
            "epoch 28 batch 11700 [117000/160000] training loss: 559.7812420129776\n",
            "epoch 28 batch 11800 [118000/160000] training loss: 572.5279198884964\n",
            "epoch 28 batch 11900 [119000/160000] training loss: 553.8323106765747\n",
            "epoch 28 batch 12000 [120000/160000] training loss: 571.4924042224884\n",
            "epoch 28 batch 12100 [121000/160000] training loss: 556.9745030403137\n",
            "epoch 28 batch 12200 [122000/160000] training loss: 547.0121396780014\n",
            "epoch 28 batch 12300 [123000/160000] training loss: 580.475994348526\n",
            "epoch 28 batch 12400 [124000/160000] training loss: 579.1524138450623\n",
            "epoch 28 batch 12500 [125000/160000] training loss: 556.0293018817902\n",
            "epoch 28 batch 12600 [126000/160000] training loss: 559.7951927185059\n",
            "epoch 28 batch 12700 [127000/160000] training loss: 595.4152356386185\n",
            "epoch 28 batch 12800 [128000/160000] training loss: 600.1842058897018\n",
            "epoch 28 batch 12900 [129000/160000] training loss: 582.8420901298523\n",
            "epoch 28 batch 13000 [130000/160000] training loss: 554.4907627105713\n",
            "epoch 28 batch 13100 [131000/160000] training loss: 590.8301086425781\n",
            "epoch 28 batch 13200 [132000/160000] training loss: 612.1250953674316\n",
            "epoch 28 batch 13300 [133000/160000] training loss: 602.1912000179291\n",
            "epoch 28 batch 13400 [134000/160000] training loss: 555.073458313942\n",
            "epoch 28 batch 13500 [135000/160000] training loss: 578.2271316051483\n",
            "epoch 28 batch 13600 [136000/160000] training loss: 537.1337914466858\n",
            "epoch 28 batch 13700 [137000/160000] training loss: 615.587021112442\n",
            "epoch 28 batch 13800 [138000/160000] training loss: 572.825377702713\n",
            "epoch 28 batch 13900 [139000/160000] training loss: 617.0638301372528\n",
            "epoch 28 batch 14000 [140000/160000] training loss: 581.0883514881134\n",
            "epoch 28 batch 14100 [141000/160000] training loss: 596.8673284053802\n",
            "epoch 28 batch 14200 [142000/160000] training loss: 556.7737195491791\n",
            "epoch 28 batch 14300 [143000/160000] training loss: 578.9785263538361\n",
            "epoch 28 batch 14400 [144000/160000] training loss: 568.3114273548126\n",
            "epoch 28 batch 14500 [145000/160000] training loss: 570.8030388355255\n",
            "epoch 28 batch 14600 [146000/160000] training loss: 577.9763118028641\n",
            "epoch 28 batch 14700 [147000/160000] training loss: 596.4584596157074\n",
            "epoch 28 batch 14800 [148000/160000] training loss: 592.1932520866394\n",
            "epoch 28 batch 14900 [149000/160000] training loss: 600.7207541465759\n",
            "epoch 28 batch 15000 [150000/160000] training loss: 599.519469499588\n",
            "epoch 28 batch 15100 [151000/160000] training loss: 567.4018890857697\n",
            "epoch 28 batch 15200 [152000/160000] training loss: 592.8166770935059\n",
            "epoch 28 batch 15300 [153000/160000] training loss: 602.0390601158142\n",
            "epoch 28 batch 15400 [154000/160000] training loss: 571.219235420227\n",
            "epoch 28 batch 15500 [155000/160000] training loss: 590.1240170001984\n",
            "epoch 28 batch 15600 [156000/160000] training loss: 567.2370533943176\n",
            "epoch 28 batch 15700 [157000/160000] training loss: 573.3407448530197\n",
            "epoch 28 batch 15800 [158000/160000] training loss: 545.0418281555176\n",
            "epoch 28 batch 15900 [159000/160000] training loss: 547.8727185726166\n",
            "epoch 29 batch 0 [0/160000] training loss: 4.953210353851318\n",
            "epoch 29 batch 100 [1000/160000] training loss: 564.4022097587585\n",
            "epoch 29 batch 200 [2000/160000] training loss: 577.0591032505035\n",
            "epoch 29 batch 300 [3000/160000] training loss: 594.3242065906525\n",
            "epoch 29 batch 400 [4000/160000] training loss: 555.8135107755661\n",
            "epoch 29 batch 500 [5000/160000] training loss: 611.8406891822815\n",
            "epoch 29 batch 600 [6000/160000] training loss: 589.7310400009155\n",
            "epoch 29 batch 700 [7000/160000] training loss: 564.4161026477814\n",
            "epoch 29 batch 800 [8000/160000] training loss: 603.8242499828339\n",
            "epoch 29 batch 900 [9000/160000] training loss: 578.0104329586029\n",
            "epoch 29 batch 1000 [10000/160000] training loss: 585.2019637823105\n",
            "epoch 29 batch 1100 [11000/160000] training loss: 594.096599817276\n",
            "epoch 29 batch 1200 [12000/160000] training loss: 575.8861050605774\n",
            "epoch 29 batch 1300 [13000/160000] training loss: 568.6099509000778\n",
            "epoch 29 batch 1400 [14000/160000] training loss: 570.9439194202423\n",
            "epoch 29 batch 1500 [15000/160000] training loss: 534.2795023918152\n",
            "epoch 29 batch 1600 [16000/160000] training loss: 570.6644749641418\n",
            "epoch 29 batch 1700 [17000/160000] training loss: 560.4114530086517\n",
            "epoch 29 batch 1800 [18000/160000] training loss: 551.5588908195496\n",
            "epoch 29 batch 1900 [19000/160000] training loss: 585.5008754730225\n",
            "epoch 29 batch 2000 [20000/160000] training loss: 549.958132982254\n",
            "epoch 29 batch 2100 [21000/160000] training loss: 562.7226314544678\n",
            "epoch 29 batch 2200 [22000/160000] training loss: 595.1121889352798\n",
            "epoch 29 batch 2300 [23000/160000] training loss: 581.6329423189163\n",
            "epoch 29 batch 2400 [24000/160000] training loss: 585.1326403617859\n",
            "epoch 29 batch 2500 [25000/160000] training loss: 578.3459236621857\n",
            "epoch 29 batch 2600 [26000/160000] training loss: 549.7453932762146\n",
            "epoch 29 batch 2700 [27000/160000] training loss: 584.6391544342041\n",
            "epoch 29 batch 2800 [28000/160000] training loss: 581.3894335031509\n",
            "epoch 29 batch 2900 [29000/160000] training loss: 629.0982992649078\n",
            "epoch 29 batch 3000 [30000/160000] training loss: 554.6524238586426\n",
            "epoch 29 batch 3100 [31000/160000] training loss: 593.700722694397\n",
            "epoch 29 batch 3200 [32000/160000] training loss: 562.4751484394073\n",
            "epoch 29 batch 3300 [33000/160000] training loss: 561.2786836624146\n",
            "epoch 29 batch 3400 [34000/160000] training loss: 561.396110534668\n",
            "epoch 29 batch 3500 [35000/160000] training loss: 565.9949278831482\n",
            "epoch 29 batch 3600 [36000/160000] training loss: 601.9911172389984\n",
            "epoch 29 batch 3700 [37000/160000] training loss: 553.1644730567932\n",
            "epoch 29 batch 3800 [38000/160000] training loss: 566.9098737239838\n",
            "epoch 29 batch 3900 [39000/160000] training loss: 602.6008670330048\n",
            "epoch 29 batch 4000 [40000/160000] training loss: 564.6916747093201\n",
            "epoch 29 batch 4100 [41000/160000] training loss: 578.9522068500519\n",
            "epoch 29 batch 4200 [42000/160000] training loss: 619.2871341705322\n",
            "epoch 29 batch 4300 [43000/160000] training loss: 576.8887329101562\n",
            "epoch 29 batch 4400 [44000/160000] training loss: 552.5763807296753\n",
            "epoch 29 batch 4500 [45000/160000] training loss: 589.3353805541992\n",
            "epoch 29 batch 4600 [46000/160000] training loss: 565.004798412323\n",
            "epoch 29 batch 4700 [47000/160000] training loss: 537.2338917255402\n",
            "epoch 29 batch 4800 [48000/160000] training loss: 579.885823726654\n",
            "epoch 29 batch 4900 [49000/160000] training loss: 576.4241237640381\n",
            "epoch 29 batch 5000 [50000/160000] training loss: 549.879399895668\n",
            "epoch 29 batch 5100 [51000/160000] training loss: 558.3951003551483\n",
            "epoch 29 batch 5200 [52000/160000] training loss: 575.3325431346893\n",
            "epoch 29 batch 5300 [53000/160000] training loss: 600.0464172363281\n",
            "epoch 29 batch 5400 [54000/160000] training loss: 591.9712810516357\n",
            "epoch 29 batch 5500 [55000/160000] training loss: 560.6677718162537\n",
            "epoch 29 batch 5600 [56000/160000] training loss: 561.9121835231781\n",
            "epoch 29 batch 5700 [57000/160000] training loss: 577.1656582355499\n",
            "epoch 29 batch 5800 [58000/160000] training loss: 564.7363359928131\n",
            "epoch 29 batch 5900 [59000/160000] training loss: 556.4457684755325\n",
            "epoch 29 batch 6000 [60000/160000] training loss: 567.3442051410675\n",
            "epoch 29 batch 6100 [61000/160000] training loss: 605.4568867683411\n",
            "epoch 29 batch 6200 [62000/160000] training loss: 605.0871769189835\n",
            "epoch 29 batch 6300 [63000/160000] training loss: 600.7482180595398\n",
            "epoch 29 batch 6400 [64000/160000] training loss: 607.0796492099762\n",
            "epoch 29 batch 6500 [65000/160000] training loss: 568.3224291801453\n",
            "epoch 29 batch 6600 [66000/160000] training loss: 566.0712776184082\n",
            "epoch 29 batch 6700 [67000/160000] training loss: 584.5281207561493\n",
            "epoch 29 batch 6800 [68000/160000] training loss: 572.7474465370178\n",
            "epoch 29 batch 6900 [69000/160000] training loss: 597.6974320411682\n",
            "epoch 29 batch 7000 [70000/160000] training loss: 586.526547908783\n",
            "epoch 29 batch 7100 [71000/160000] training loss: 568.5642130374908\n",
            "epoch 29 batch 7200 [72000/160000] training loss: 575.2944024801254\n",
            "epoch 29 batch 7300 [73000/160000] training loss: 550.3558895587921\n",
            "epoch 29 batch 7400 [74000/160000] training loss: 563.6237263679504\n",
            "epoch 29 batch 7500 [75000/160000] training loss: 551.019186258316\n",
            "epoch 29 batch 7600 [76000/160000] training loss: 567.0092833042145\n",
            "epoch 29 batch 7700 [77000/160000] training loss: 592.3997752666473\n",
            "epoch 29 batch 7800 [78000/160000] training loss: 614.4085726737976\n",
            "epoch 29 batch 7900 [79000/160000] training loss: 570.5191252231598\n",
            "epoch 29 batch 8000 [80000/160000] training loss: 567.8824172019958\n",
            "epoch 29 batch 8100 [81000/160000] training loss: 561.3933559656143\n",
            "epoch 29 batch 8200 [82000/160000] training loss: 597.3066520690918\n",
            "epoch 29 batch 8300 [83000/160000] training loss: 600.8155462741852\n",
            "epoch 29 batch 8400 [84000/160000] training loss: 553.3702157735825\n",
            "epoch 29 batch 8500 [85000/160000] training loss: 610.4751292467117\n",
            "epoch 29 batch 8600 [86000/160000] training loss: 574.3761525154114\n",
            "epoch 29 batch 8700 [87000/160000] training loss: 578.8449025154114\n",
            "epoch 29 batch 8800 [88000/160000] training loss: 600.9798240661621\n",
            "epoch 29 batch 8900 [89000/160000] training loss: 606.0849821567535\n",
            "epoch 29 batch 9000 [90000/160000] training loss: 583.9585475921631\n",
            "epoch 29 batch 9100 [91000/160000] training loss: 572.0077644586563\n",
            "epoch 29 batch 9200 [92000/160000] training loss: 574.4416173696518\n",
            "epoch 29 batch 9300 [93000/160000] training loss: 572.4888532161713\n",
            "epoch 29 batch 9400 [94000/160000] training loss: 558.0952415466309\n",
            "epoch 29 batch 9500 [95000/160000] training loss: 576.3975465297699\n",
            "epoch 29 batch 9600 [96000/160000] training loss: 586.6960701942444\n",
            "epoch 29 batch 9700 [97000/160000] training loss: 572.7577687501907\n",
            "epoch 29 batch 9800 [98000/160000] training loss: 538.4662709236145\n",
            "epoch 29 batch 9900 [99000/160000] training loss: 578.5391552448273\n",
            "epoch 29 batch 10000 [100000/160000] training loss: 603.8334827423096\n",
            "epoch 29 batch 10100 [101000/160000] training loss: 577.5695812702179\n",
            "epoch 29 batch 10200 [102000/160000] training loss: 531.9726831912994\n",
            "epoch 29 batch 10300 [103000/160000] training loss: 569.4516322612762\n",
            "epoch 29 batch 10400 [104000/160000] training loss: 552.1583375930786\n",
            "epoch 29 batch 10500 [105000/160000] training loss: 637.1291103363037\n",
            "epoch 29 batch 10600 [106000/160000] training loss: 611.0000123977661\n",
            "epoch 29 batch 10700 [107000/160000] training loss: 567.7338392734528\n",
            "epoch 29 batch 10800 [108000/160000] training loss: 568.2415007352829\n",
            "epoch 29 batch 10900 [109000/160000] training loss: 541.5927780866623\n",
            "epoch 29 batch 11000 [110000/160000] training loss: 587.1489253044128\n",
            "epoch 29 batch 11100 [111000/160000] training loss: 582.4182026386261\n",
            "epoch 29 batch 11200 [112000/160000] training loss: 567.5989508628845\n",
            "epoch 29 batch 11300 [113000/160000] training loss: 574.3911998271942\n",
            "epoch 29 batch 11400 [114000/160000] training loss: 599.127702832222\n",
            "epoch 29 batch 11500 [115000/160000] training loss: 563.565140247345\n",
            "epoch 29 batch 11600 [116000/160000] training loss: 577.4363800287247\n",
            "epoch 29 batch 11700 [117000/160000] training loss: 559.2686285972595\n",
            "epoch 29 batch 11800 [118000/160000] training loss: 572.1163506507874\n",
            "epoch 29 batch 11900 [119000/160000] training loss: 553.4141101837158\n",
            "epoch 29 batch 12000 [120000/160000] training loss: 571.2525901794434\n",
            "epoch 29 batch 12100 [121000/160000] training loss: 557.1510595083237\n",
            "epoch 29 batch 12200 [122000/160000] training loss: 547.1477798223495\n",
            "epoch 29 batch 12300 [123000/160000] training loss: 580.4795382022858\n",
            "epoch 29 batch 12400 [124000/160000] training loss: 578.8743886947632\n",
            "epoch 29 batch 12500 [125000/160000] training loss: 555.4738042354584\n",
            "epoch 29 batch 12600 [126000/160000] training loss: 559.4614298343658\n",
            "epoch 29 batch 12700 [127000/160000] training loss: 595.3211114406586\n",
            "epoch 29 batch 12800 [128000/160000] training loss: 599.686569571495\n",
            "epoch 29 batch 12900 [129000/160000] training loss: 582.5679496526718\n",
            "epoch 29 batch 13000 [130000/160000] training loss: 554.3919878005981\n",
            "epoch 29 batch 13100 [131000/160000] training loss: 590.3444256782532\n",
            "epoch 29 batch 13200 [132000/160000] training loss: 609.2324728965759\n",
            "epoch 29 batch 13300 [133000/160000] training loss: 599.2359220981598\n",
            "epoch 29 batch 13400 [134000/160000] training loss: 554.8029876947403\n",
            "epoch 29 batch 13500 [135000/160000] training loss: 578.1193625926971\n",
            "epoch 29 batch 13600 [136000/160000] training loss: 537.0228886604309\n",
            "epoch 29 batch 13700 [137000/160000] training loss: 615.3137319087982\n",
            "epoch 29 batch 13800 [138000/160000] training loss: 572.7937004566193\n",
            "epoch 29 batch 13900 [139000/160000] training loss: 617.6253980398178\n",
            "epoch 29 batch 14000 [140000/160000] training loss: 580.4451971054077\n",
            "epoch 29 batch 14100 [141000/160000] training loss: 597.298149228096\n",
            "epoch 29 batch 14200 [142000/160000] training loss: 556.1058576107025\n",
            "epoch 29 batch 14300 [143000/160000] training loss: 579.0206456184387\n",
            "epoch 29 batch 14400 [144000/160000] training loss: 568.4419965744019\n",
            "epoch 29 batch 14500 [145000/160000] training loss: 570.9349734783173\n",
            "epoch 29 batch 14600 [146000/160000] training loss: 577.9077658653259\n",
            "epoch 29 batch 14700 [147000/160000] training loss: 596.5277314186096\n",
            "epoch 29 batch 14800 [148000/160000] training loss: 592.3485867977142\n",
            "epoch 29 batch 14900 [149000/160000] training loss: 600.3350760936737\n",
            "epoch 29 batch 15000 [150000/160000] training loss: 599.3653817176819\n",
            "epoch 29 batch 15100 [151000/160000] training loss: 567.2495260238647\n",
            "epoch 29 batch 15200 [152000/160000] training loss: 592.9410247802734\n",
            "epoch 29 batch 15300 [153000/160000] training loss: 602.2151606082916\n",
            "epoch 29 batch 15400 [154000/160000] training loss: 570.9863967895508\n",
            "epoch 29 batch 15500 [155000/160000] training loss: 589.2944505214691\n",
            "epoch 29 batch 15600 [156000/160000] training loss: 567.3156750202179\n",
            "epoch 29 batch 15700 [157000/160000] training loss: 573.3160769939423\n",
            "epoch 29 batch 15800 [158000/160000] training loss: 545.1830492019653\n",
            "epoch 29 batch 15900 [159000/160000] training loss: 547.8484053611755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/final_training_epoch_losses', 'rb') as f:\n",
        "  training_losses = pickle.load(f)\n",
        "\n",
        "plt.scatter(x = [k for k in range(len(training_losses))], y = training_losses)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross entropy loss for classif.')\n",
        "plt.title('Evolution of training loss through epochs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "6_SiLupRSDK8",
        "outputId": "a3681aaf-b877-4ac2-c2fa-006d89ce04f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Evolution of training loss through epochs')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 5 / EVALUATION OF THE MODEL ON MOVIES REVIEWS CLASSIFICATION TASK"
      ],
      "metadata": {
        "id": "pbbiYKUPP-IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing by bootstrapping 1000 batches in test dataset (= 10 000 reviews in total) :\n",
        "final_model.eval()\n",
        "\n",
        "bootstrap_number = 50\n",
        "bootstrap_size = 20\n",
        "bootstrapped_accuracies = []\n",
        "\n",
        "for boot in range(bootstrap_number):\n",
        "\n",
        "  test_loader = DataLoader(dataset = cinema_reviews['test'], batch_size = 10,\n",
        "                            num_workers=2, shuffle=True, drop_last=True)\n",
        "  right_answers = 0\n",
        "  true_test_size = 0 # few KeyError so we discount them\n",
        "\n",
        "  for i in range(bootstrap_size):\n",
        "    try:\n",
        "      batch = next(iter(test_loader))\n",
        "      true_label = batch['label'][0]\n",
        "      probas = F.softmax(final_model(preprocessing(batch, max_dialog_size, max_size_of_utterance)[0]), dim = 1)\n",
        "      votes = torch.sum(probas, axis=0)\n",
        "\n",
        "      if votes[0] < votes[1]:\n",
        "        pred = 1 \n",
        "      else:\n",
        "        pred = 0\n",
        "\n",
        "      if pred == true_label:\n",
        "        right_answers += 1\n",
        "      \n",
        "      true_test_size += 1\n",
        "    \n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "  bootstrapped_accuracies.append(right_answers/true_test_size)"
      ],
      "metadata": {
        "id": "015qNAeEXV1y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n, bins, patches = plt.hist(x = bootstrapped_accuracies, bins = 'auto', color = '#0504aa',\n",
        "                            alpha = 0.7, rwidth = 0.85)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Bootstrapped accuracy values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of bootstrapped accuracies')\n",
        "maxfreq = n.max()\n",
        "# Set a clean upper y-axis limit.\n",
        "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
      ],
      "metadata": {
        "id": "GVV5AUenGi6u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "c1f50a20-4304-4283-8907-9805352d177d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 20.0)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# average accuracy over bootstrapped sets of reviews :\n",
        "print('Average accuracy over bootstrapped sets of samples = ', int(np.mean(bootstrapped_accuracies)*100), '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zq78P0-g6SV",
        "outputId": "7479e140-8441-4a54-83b4-348e66393a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy over bootstrapped sets of samples =  64 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 6 / ASSESSMENT OF THE IMPACT OF PRETRAINING ON CLASSIFICATION "
      ],
      "metadata": {
        "id": "enZfEO5NPg2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we want to know how the classification model would have behaved if no pretraining (hierarchical model with MLM loss) had been performed before training the classification layer weights:\n",
        "* In the following cells we instanciate the pretraining model without any MLM loss training\n",
        "* After that we train the classification layer weights for 30 epochs, as we did for our pretrained model, and then we evaluate the accuracy of this not pretrained classification model"
      ],
      "metadata": {
        "id": "pen19vG6PxbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "final_model = FinalModel(model_not_pretrained)\n",
        "final_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSq_tAaaPwZB",
        "outputId": "fc5ea083-d9f2-4907-9f72-1083eb1ff822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalModel(\n",
              "  (first_layer_encoders): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (second_layer_encoder): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (classif_layer): Linear(in_features=3000, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimization of parameters of classif layer \n",
        "\n",
        "def classif_trainer_2(final_model, num_epoch = 10, max_norm = 1, epoch_b4_stopping = 0, epoch_losses = [], classif_layer_only = True, clipping = True):\n",
        "\n",
        "  if epoch_b4_stopping != 0:\n",
        "    epoch_losses = epoch_losses\n",
        "  else:\n",
        "    epoch_losses = [0 for i in range(num_epoch)]\n",
        "\n",
        "  if classif_layer_only == True:\n",
        "    for param in final_model.parameters():\n",
        "      param.requires_grad = False\n",
        "    for param in final_model.classif_layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "  optimizer = opt.SGD(final_model.parameters(), lr=0.001, momentum=0.9)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  # training mode \"train\" only changes the \"dropout\" or \"batchnorm\" layers\n",
        "  final_model.train()\n",
        "\n",
        "  # main loop (train+test)\n",
        "  for epoch in tqdm(range(epoch_b4_stopping, num_epoch)):\n",
        "\n",
        "      if epoch == epoch_b4_stopping:\n",
        "        epoch_losses[epoch] = 0 \n",
        "\n",
        "      loss_100_batches = 0\n",
        "      \n",
        "      for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "          # Clear previous gradients in the graph\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # compute batch loss\n",
        "          loss = batch_loss_classif(batch)\n",
        "\n",
        "          # Compute clipped gradients\n",
        "          loss.backward()\n",
        "          if clipping == True:\n",
        "            utils.clip_grad_norm(final_model.parameters(), max_norm)\n",
        "\n",
        "          epoch_losses[epoch] += loss.item()\n",
        "          loss_100_batches += loss.item()\n",
        "\n",
        "          # Apply Gradient descent\n",
        "          optimizer.step()\n",
        "          \n",
        "          if batch_idx %100 ==0:\n",
        "              print('epoch {} batch {} [{}/{}] training loss: {}'.format(epoch,batch_idx,batch_idx*10,\n",
        "                      len(train_loader.dataset),loss_100_batches))\n",
        "              loss_100_batches = 0\n",
        "              \n",
        "          if batch_idx %1000 == 0:\n",
        "            with open(\"./drive/MyDrive/final_training_params_saving_no_pretraining\", \"wb\") as f:\n",
        "              pickle.dump({'epoch_losses': epoch_losses, 'epoch_b4_stopping' : epoch}, f)\n",
        "            torch.save(final_model, './drive/MyDrive/final_model_no_pretraining')\n",
        "\n",
        "  with open(\"./drive/MyDrive/final_training_epoch_losses_no_pretraining\", \"wb\") as f:\n",
        "              pickle.dump(epoch_losses, f)\n",
        "  torch.save(final_model, './drive/MyDrive/final_model_no_pretraining')"
      ],
      "metadata": {
        "id": "4k7YfUfHRwfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classif_trainer_2(final_model, num_epoch = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "247c629cd0d645bfa0c617fa40b690b6",
            "747f06ec34944bddbdab51974193c36e",
            "346b6bf1dbbf4ff78515416cb794e783",
            "0b7ae10eceac4769b28be91d4293af61",
            "7f409689d6cd42d482cd5b321dc28df5",
            "66adb3120d4a455d86b62318bc3e9843",
            "bc48cd5f291a4720acceea7eae28f9b4",
            "19c1d7dccd174698af59751954df1354",
            "895e418a88da4f8d91d9cc774119e9c5",
            "6f4ff8af86ad4ea190d8e28b09921ce8",
            "cc50bff4f6d3438a8ade9daf0598725a"
          ]
        },
        "id": "Mri2ITF2STRt",
        "outputId": "e9581ebf-f57b-41ec-c071-93b4d7f07176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "247c629cd0d645bfa0c617fa40b690b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 batch 0 [0/160000] training loss: 9.418587684631348\n",
            "epoch 0 batch 100 [1000/160000] training loss: 771.1224870681763\n",
            "epoch 0 batch 200 [2000/160000] training loss: 726.0660371780396\n",
            "epoch 0 batch 300 [3000/160000] training loss: 709.7422337532043\n",
            "epoch 0 batch 400 [4000/160000] training loss: 716.8814163208008\n",
            "epoch 0 batch 500 [5000/160000] training loss: 713.0321044921875\n",
            "epoch 0 batch 600 [6000/160000] training loss: 681.4173812866211\n",
            "epoch 0 batch 700 [7000/160000] training loss: 694.5596301555634\n",
            "epoch 0 batch 800 [8000/160000] training loss: 679.0434510707855\n",
            "epoch 0 batch 900 [9000/160000] training loss: 702.944591999054\n",
            "epoch 0 batch 1000 [10000/160000] training loss: 671.0274631977081\n",
            "epoch 0 batch 1100 [11000/160000] training loss: 688.3555669784546\n",
            "epoch 0 batch 1200 [12000/160000] training loss: 668.4535999298096\n",
            "epoch 0 batch 1300 [13000/160000] training loss: 702.5404033660889\n",
            "epoch 0 batch 1400 [14000/160000] training loss: 703.2277233600616\n",
            "epoch 0 batch 1500 [15000/160000] training loss: 622.8303842544556\n",
            "epoch 0 batch 1600 [16000/160000] training loss: 665.51238322258\n",
            "epoch 0 batch 1700 [17000/160000] training loss: 630.4377801418304\n",
            "epoch 0 batch 1800 [18000/160000] training loss: 674.8723654747009\n",
            "epoch 0 batch 1900 [19000/160000] training loss: 669.7724134922028\n",
            "epoch 0 batch 2000 [20000/160000] training loss: 680.3646941184998\n",
            "epoch 0 batch 2100 [21000/160000] training loss: 657.4536952972412\n",
            "epoch 0 batch 2200 [22000/160000] training loss: 708.4205121994019\n",
            "epoch 0 batch 2300 [23000/160000] training loss: 612.1739130020142\n",
            "epoch 0 batch 2400 [24000/160000] training loss: 636.3541965484619\n",
            "epoch 0 batch 2500 [25000/160000] training loss: 650.2351188659668\n",
            "epoch 0 batch 2600 [26000/160000] training loss: 638.0775501728058\n",
            "epoch 0 batch 2700 [27000/160000] training loss: 632.2683608531952\n",
            "epoch 0 batch 2800 [28000/160000] training loss: 643.6465308666229\n",
            "epoch 0 batch 2900 [29000/160000] training loss: 634.7571790218353\n",
            "epoch 0 batch 3000 [30000/160000] training loss: 658.628947019577\n",
            "epoch 0 batch 3100 [31000/160000] training loss: 628.5318734645844\n",
            "epoch 0 batch 3200 [32000/160000] training loss: 641.5688223838806\n",
            "epoch 0 batch 3300 [33000/160000] training loss: 634.2646415233612\n",
            "epoch 0 batch 3400 [34000/160000] training loss: 630.2913191318512\n",
            "epoch 0 batch 3500 [35000/160000] training loss: 616.8804309368134\n",
            "epoch 0 batch 3600 [36000/160000] training loss: 622.4541532993317\n",
            "epoch 0 batch 3700 [37000/160000] training loss: 625.9136934280396\n",
            "epoch 0 batch 3800 [38000/160000] training loss: 619.1751978397369\n",
            "epoch 0 batch 3900 [39000/160000] training loss: 648.0665168762207\n",
            "epoch 0 batch 4000 [40000/160000] training loss: 637.4266760349274\n",
            "epoch 0 batch 4100 [41000/160000] training loss: 619.4655702114105\n",
            "epoch 0 batch 4200 [42000/160000] training loss: 640.0168559551239\n",
            "epoch 0 batch 4300 [43000/160000] training loss: 671.5576050281525\n",
            "epoch 0 batch 4400 [44000/160000] training loss: 632.8446822166443\n",
            "epoch 0 batch 4500 [45000/160000] training loss: 640.8515050411224\n",
            "epoch 0 batch 4600 [46000/160000] training loss: 639.9277613162994\n",
            "epoch 0 batch 4700 [47000/160000] training loss: 620.1577832698822\n",
            "epoch 0 batch 4800 [48000/160000] training loss: 618.3422083854675\n",
            "epoch 0 batch 4900 [49000/160000] training loss: 635.0657660961151\n",
            "epoch 0 batch 5000 [50000/160000] training loss: 639.5658469200134\n",
            "epoch 0 batch 5100 [51000/160000] training loss: 616.9697442054749\n",
            "epoch 0 batch 5200 [52000/160000] training loss: 636.066085100174\n",
            "epoch 0 batch 5300 [53000/160000] training loss: 610.0667035579681\n",
            "epoch 0 batch 5400 [54000/160000] training loss: 623.1490676403046\n",
            "epoch 0 batch 5500 [55000/160000] training loss: 631.9236578941345\n",
            "epoch 0 batch 5600 [56000/160000] training loss: 653.0454683303833\n",
            "epoch 0 batch 5700 [57000/160000] training loss: 631.4337079524994\n",
            "epoch 0 batch 5800 [58000/160000] training loss: 623.1967256069183\n",
            "epoch 0 batch 5900 [59000/160000] training loss: 610.8986556529999\n",
            "epoch 0 batch 6000 [60000/160000] training loss: 636.1843404769897\n",
            "epoch 0 batch 6100 [61000/160000] training loss: 619.5637485980988\n",
            "epoch 0 batch 6200 [62000/160000] training loss: 620.4596869945526\n",
            "epoch 0 batch 6300 [63000/160000] training loss: 609.6627480983734\n",
            "epoch 0 batch 6400 [64000/160000] training loss: 629.2997722625732\n",
            "epoch 0 batch 6500 [65000/160000] training loss: 667.3731870651245\n",
            "epoch 0 batch 6600 [66000/160000] training loss: 647.2238593101501\n",
            "epoch 0 batch 6700 [67000/160000] training loss: 622.6473577022552\n",
            "epoch 0 batch 6800 [68000/160000] training loss: 602.9651296138763\n",
            "epoch 0 batch 6900 [69000/160000] training loss: 638.5053412914276\n",
            "epoch 0 batch 7000 [70000/160000] training loss: 598.4978129863739\n",
            "epoch 0 batch 7100 [71000/160000] training loss: 612.6804122924805\n",
            "epoch 0 batch 7200 [72000/160000] training loss: 646.3110761642456\n",
            "epoch 0 batch 7300 [73000/160000] training loss: 610.5318286418915\n",
            "epoch 0 batch 7400 [74000/160000] training loss: 619.5485520362854\n",
            "epoch 0 batch 7500 [75000/160000] training loss: 632.5769488811493\n",
            "epoch 0 batch 7600 [76000/160000] training loss: 647.4926657676697\n",
            "epoch 0 batch 7700 [77000/160000] training loss: 626.9008162021637\n",
            "epoch 0 batch 7800 [78000/160000] training loss: 598.0137116909027\n",
            "epoch 0 batch 7900 [79000/160000] training loss: 620.5393912792206\n",
            "epoch 0 batch 8000 [80000/160000] training loss: 584.9651684761047\n",
            "epoch 0 batch 8100 [81000/160000] training loss: 616.2548370361328\n",
            "epoch 0 batch 8200 [82000/160000] training loss: 595.0962855815887\n",
            "epoch 0 batch 8300 [83000/160000] training loss: 572.1032180786133\n",
            "epoch 0 batch 8400 [84000/160000] training loss: 600.6398520469666\n",
            "epoch 0 batch 8500 [85000/160000] training loss: 641.2179429531097\n",
            "epoch 0 batch 8600 [86000/160000] training loss: 587.105304479599\n",
            "epoch 0 batch 8700 [87000/160000] training loss: 649.754842042923\n",
            "epoch 0 batch 8800 [88000/160000] training loss: 605.1587204933167\n",
            "epoch 0 batch 8900 [89000/160000] training loss: 623.0690903663635\n",
            "epoch 0 batch 9000 [90000/160000] training loss: 598.503782749176\n",
            "epoch 0 batch 9100 [91000/160000] training loss: 647.1451833248138\n",
            "epoch 0 batch 9200 [92000/160000] training loss: 602.9339187145233\n",
            "epoch 0 batch 9300 [93000/160000] training loss: 587.5066270828247\n",
            "epoch 0 batch 9400 [94000/160000] training loss: 607.0420827865601\n",
            "epoch 0 batch 9500 [95000/160000] training loss: 594.0917932987213\n",
            "epoch 0 batch 9600 [96000/160000] training loss: 584.780565738678\n",
            "epoch 0 batch 9700 [97000/160000] training loss: 644.791253566742\n",
            "epoch 0 batch 9800 [98000/160000] training loss: 605.5462603569031\n",
            "epoch 0 batch 9900 [99000/160000] training loss: 576.0142579078674\n",
            "epoch 0 batch 10000 [100000/160000] training loss: 582.8273684978485\n",
            "epoch 0 batch 10100 [101000/160000] training loss: 632.5045366287231\n",
            "epoch 0 batch 10200 [102000/160000] training loss: 611.0210614204407\n",
            "epoch 0 batch 10300 [103000/160000] training loss: 602.3402726650238\n",
            "epoch 0 batch 10400 [104000/160000] training loss: 576.5251910686493\n",
            "epoch 0 batch 10500 [105000/160000] training loss: 598.5300858020782\n",
            "epoch 0 batch 10600 [106000/160000] training loss: 619.8269321918488\n",
            "epoch 0 batch 10700 [107000/160000] training loss: 587.0774304866791\n",
            "epoch 0 batch 10800 [108000/160000] training loss: 618.2520251274109\n",
            "epoch 0 batch 10900 [109000/160000] training loss: 598.0316140651703\n",
            "epoch 0 batch 11000 [110000/160000] training loss: 594.8778183460236\n",
            "epoch 0 batch 11100 [111000/160000] training loss: 618.5777325630188\n",
            "epoch 0 batch 11200 [112000/160000] training loss: 589.9082925319672\n",
            "epoch 0 batch 11300 [113000/160000] training loss: 614.6648056507111\n",
            "epoch 0 batch 11400 [114000/160000] training loss: 593.0013561248779\n",
            "epoch 0 batch 11500 [115000/160000] training loss: 604.0091080665588\n",
            "epoch 0 batch 11600 [116000/160000] training loss: 618.6993064880371\n",
            "epoch 0 batch 11700 [117000/160000] training loss: 586.950008392334\n",
            "epoch 0 batch 11800 [118000/160000] training loss: 605.3861067295074\n",
            "epoch 0 batch 11900 [119000/160000] training loss: 589.1803495883942\n",
            "epoch 0 batch 12000 [120000/160000] training loss: 611.561285495758\n",
            "epoch 0 batch 12100 [121000/160000] training loss: 591.4170882701874\n",
            "epoch 0 batch 12200 [122000/160000] training loss: 611.5231502056122\n",
            "epoch 0 batch 12300 [123000/160000] training loss: 579.9413197040558\n",
            "epoch 0 batch 12400 [124000/160000] training loss: 583.9216060638428\n",
            "epoch 0 batch 12500 [125000/160000] training loss: 593.7613742351532\n",
            "epoch 0 batch 12600 [126000/160000] training loss: 615.9660568237305\n",
            "epoch 0 batch 12700 [127000/160000] training loss: 605.6263420581818\n",
            "epoch 0 batch 12800 [128000/160000] training loss: 611.5368580818176\n",
            "epoch 0 batch 12900 [129000/160000] training loss: 606.5534815788269\n",
            "epoch 0 batch 13000 [130000/160000] training loss: 628.020393371582\n",
            "epoch 0 batch 13100 [131000/160000] training loss: 616.6984162330627\n",
            "epoch 0 batch 13200 [132000/160000] training loss: 585.1369502544403\n",
            "epoch 0 batch 13300 [133000/160000] training loss: 619.9485092163086\n",
            "epoch 0 batch 13400 [134000/160000] training loss: 590.6132724285126\n",
            "epoch 0 batch 13500 [135000/160000] training loss: 597.9160153865814\n",
            "epoch 0 batch 13600 [136000/160000] training loss: 605.3041603565216\n",
            "epoch 0 batch 13700 [137000/160000] training loss: 582.766921043396\n",
            "epoch 0 batch 13800 [138000/160000] training loss: 583.0023701190948\n",
            "epoch 0 batch 13900 [139000/160000] training loss: 613.84512591362\n",
            "epoch 0 batch 14000 [140000/160000] training loss: 604.92799949646\n",
            "epoch 0 batch 14100 [141000/160000] training loss: 621.7818794250488\n",
            "epoch 0 batch 14200 [142000/160000] training loss: 618.0497324466705\n",
            "epoch 0 batch 14300 [143000/160000] training loss: 581.967010974884\n",
            "epoch 0 batch 14400 [144000/160000] training loss: 609.3245856761932\n",
            "epoch 0 batch 14500 [145000/160000] training loss: 599.2417786121368\n",
            "epoch 0 batch 14600 [146000/160000] training loss: 613.4985327720642\n",
            "epoch 0 batch 14700 [147000/160000] training loss: 615.4922592639923\n",
            "epoch 0 batch 14800 [148000/160000] training loss: 611.997273683548\n",
            "epoch 0 batch 14900 [149000/160000] training loss: 650.2163004875183\n",
            "epoch 0 batch 15000 [150000/160000] training loss: 599.4141783714294\n",
            "epoch 0 batch 15100 [151000/160000] training loss: 577.3754246234894\n",
            "epoch 0 batch 15200 [152000/160000] training loss: 596.7260255813599\n",
            "epoch 0 batch 15300 [153000/160000] training loss: 610.3459694385529\n",
            "epoch 0 batch 15400 [154000/160000] training loss: 614.4533410072327\n",
            "epoch 0 batch 15500 [155000/160000] training loss: 606.3658185005188\n",
            "epoch 0 batch 15600 [156000/160000] training loss: 632.2737963199615\n",
            "epoch 0 batch 15700 [157000/160000] training loss: 611.6703321933746\n",
            "epoch 0 batch 15800 [158000/160000] training loss: 608.2703692913055\n",
            "epoch 0 batch 15900 [159000/160000] training loss: 623.1974810361862\n",
            "epoch 1 batch 0 [0/160000] training loss: 6.7270121574401855\n",
            "epoch 1 batch 100 [1000/160000] training loss: 583.8459907770157\n",
            "epoch 1 batch 200 [2000/160000] training loss: 589.393764257431\n",
            "epoch 1 batch 300 [3000/160000] training loss: 588.1976652145386\n",
            "epoch 1 batch 400 [4000/160000] training loss: 590.8852248191833\n",
            "epoch 1 batch 500 [5000/160000] training loss: 582.37047123909\n",
            "epoch 1 batch 600 [6000/160000] training loss: 590.0031142234802\n",
            "epoch 1 batch 700 [7000/160000] training loss: 578.5480742454529\n",
            "epoch 1 batch 800 [8000/160000] training loss: 605.1006317138672\n",
            "epoch 1 batch 900 [9000/160000] training loss: 587.0362706184387\n",
            "epoch 1 batch 1000 [10000/160000] training loss: 594.5493202209473\n",
            "epoch 1 batch 1100 [11000/160000] training loss: 592.9676151275635\n",
            "epoch 1 batch 1200 [12000/160000] training loss: 596.7735259532928\n",
            "epoch 1 batch 1300 [13000/160000] training loss: 610.2883026599884\n",
            "epoch 1 batch 1400 [14000/160000] training loss: 583.2627127170563\n",
            "epoch 1 batch 1500 [15000/160000] training loss: 562.1323511600494\n",
            "epoch 1 batch 1600 [16000/160000] training loss: 609.8983602523804\n",
            "epoch 1 batch 1700 [17000/160000] training loss: 597.935064792633\n",
            "epoch 1 batch 1800 [18000/160000] training loss: 595.1102182865143\n",
            "epoch 1 batch 1900 [19000/160000] training loss: 590.2285556793213\n",
            "epoch 1 batch 2000 [20000/160000] training loss: 597.9317491054535\n",
            "epoch 1 batch 2100 [21000/160000] training loss: 552.837583065033\n",
            "epoch 1 batch 2200 [22000/160000] training loss: 606.4099814891815\n",
            "epoch 1 batch 2300 [23000/160000] training loss: 588.6151163578033\n",
            "epoch 1 batch 2400 [24000/160000] training loss: 584.5876138210297\n",
            "epoch 1 batch 2500 [25000/160000] training loss: 623.1077556610107\n",
            "epoch 1 batch 2600 [26000/160000] training loss: 578.7807672023773\n",
            "epoch 1 batch 2700 [27000/160000] training loss: 598.9674415588379\n",
            "epoch 1 batch 2800 [28000/160000] training loss: 572.1212320327759\n",
            "epoch 1 batch 2900 [29000/160000] training loss: 624.9362001419067\n",
            "epoch 1 batch 3000 [30000/160000] training loss: 582.0780243873596\n",
            "epoch 1 batch 3100 [31000/160000] training loss: 603.9309358596802\n",
            "epoch 1 batch 3200 [32000/160000] training loss: 593.6700468063354\n",
            "epoch 1 batch 3300 [33000/160000] training loss: 601.1443209648132\n",
            "epoch 1 batch 3400 [34000/160000] training loss: 577.2238426208496\n",
            "epoch 1 batch 3500 [35000/160000] training loss: 615.5735545158386\n",
            "epoch 1 batch 3600 [36000/160000] training loss: 592.7249546051025\n",
            "epoch 1 batch 3700 [37000/160000] training loss: 584.8099012374878\n",
            "epoch 1 batch 3800 [38000/160000] training loss: 596.1965343952179\n",
            "epoch 1 batch 3900 [39000/160000] training loss: 592.6504089832306\n",
            "epoch 1 batch 4000 [40000/160000] training loss: 573.5102500915527\n",
            "epoch 1 batch 4100 [41000/160000] training loss: 586.4018676280975\n",
            "epoch 1 batch 4200 [42000/160000] training loss: 615.2153413295746\n",
            "epoch 1 batch 4300 [43000/160000] training loss: 588.7344744205475\n",
            "epoch 1 batch 4400 [44000/160000] training loss: 579.6261029243469\n",
            "epoch 1 batch 4500 [45000/160000] training loss: 590.1397850513458\n",
            "epoch 1 batch 4600 [46000/160000] training loss: 569.4239118099213\n",
            "epoch 1 batch 4700 [47000/160000] training loss: 562.8174669742584\n",
            "epoch 1 batch 4800 [48000/160000] training loss: 580.0996344089508\n",
            "epoch 1 batch 4900 [49000/160000] training loss: 596.7476444244385\n",
            "epoch 1 batch 5000 [50000/160000] training loss: 581.4213297367096\n",
            "epoch 1 batch 5100 [51000/160000] training loss: 576.8603191375732\n",
            "epoch 1 batch 5200 [52000/160000] training loss: 591.6417021751404\n",
            "epoch 1 batch 5300 [53000/160000] training loss: 614.7159898281097\n",
            "epoch 1 batch 5400 [54000/160000] training loss: 576.1180009841919\n",
            "epoch 1 batch 5500 [55000/160000] training loss: 576.4633305072784\n",
            "epoch 1 batch 5600 [56000/160000] training loss: 598.2146801948547\n",
            "epoch 1 batch 5700 [57000/160000] training loss: 591.2291448116302\n",
            "epoch 1 batch 5800 [58000/160000] training loss: 582.675226688385\n",
            "epoch 1 batch 5900 [59000/160000] training loss: 567.9734809398651\n",
            "epoch 1 batch 6000 [60000/160000] training loss: 598.909065246582\n",
            "epoch 1 batch 6100 [61000/160000] training loss: 599.9985837936401\n",
            "epoch 1 batch 6200 [62000/160000] training loss: 607.4014089107513\n",
            "epoch 1 batch 6300 [63000/160000] training loss: 566.8899531364441\n",
            "epoch 1 batch 6400 [64000/160000] training loss: 575.6741323471069\n",
            "epoch 1 batch 6500 [65000/160000] training loss: 600.1691830158234\n",
            "epoch 1 batch 6600 [66000/160000] training loss: 579.730010509491\n",
            "epoch 1 batch 6700 [67000/160000] training loss: 600.4057347774506\n",
            "epoch 1 batch 6800 [68000/160000] training loss: 560.9188351631165\n",
            "epoch 1 batch 6900 [69000/160000] training loss: 588.5448346138\n",
            "epoch 1 batch 7000 [70000/160000] training loss: 578.5179755687714\n",
            "epoch 1 batch 7100 [71000/160000] training loss: 571.0344018936157\n",
            "epoch 1 batch 7200 [72000/160000] training loss: 580.5747299194336\n",
            "epoch 1 batch 7300 [73000/160000] training loss: 576.4779005050659\n",
            "epoch 1 batch 7400 [74000/160000] training loss: 573.8853437900543\n",
            "epoch 1 batch 7500 [75000/160000] training loss: 573.9267425537109\n",
            "epoch 1 batch 7600 [76000/160000] training loss: 596.7997472286224\n",
            "epoch 1 batch 7700 [77000/160000] training loss: 606.9655940532684\n",
            "epoch 1 batch 7800 [78000/160000] training loss: 598.6059572696686\n",
            "epoch 1 batch 7900 [79000/160000] training loss: 575.6117870807648\n",
            "epoch 1 batch 8000 [80000/160000] training loss: 578.6558711528778\n",
            "epoch 1 batch 8100 [81000/160000] training loss: 574.1702506542206\n",
            "epoch 1 batch 8200 [82000/160000] training loss: 572.3343276977539\n",
            "epoch 1 batch 8300 [83000/160000] training loss: 597.7884621620178\n",
            "epoch 1 batch 8400 [84000/160000] training loss: 603.6433432102203\n",
            "epoch 1 batch 8500 [85000/160000] training loss: 604.572256565094\n",
            "epoch 1 batch 8600 [86000/160000] training loss: 571.1248886585236\n",
            "epoch 1 batch 8700 [87000/160000] training loss: 582.5603868961334\n",
            "epoch 1 batch 8800 [88000/160000] training loss: 598.8807969093323\n",
            "epoch 1 batch 8900 [89000/160000] training loss: 611.4145896434784\n",
            "epoch 1 batch 9000 [90000/160000] training loss: 606.2681839466095\n",
            "epoch 1 batch 9100 [91000/160000] training loss: 592.6633808612823\n",
            "epoch 1 batch 9200 [92000/160000] training loss: 570.1955649852753\n",
            "epoch 1 batch 9300 [93000/160000] training loss: 585.3878848552704\n",
            "epoch 1 batch 9400 [94000/160000] training loss: 559.5035858154297\n",
            "epoch 1 batch 9500 [95000/160000] training loss: 590.131644487381\n",
            "epoch 1 batch 9600 [96000/160000] training loss: 582.8430504798889\n",
            "epoch 1 batch 9700 [97000/160000] training loss: 581.9277107715607\n",
            "epoch 1 batch 9800 [98000/160000] training loss: 577.2384712696075\n",
            "epoch 1 batch 9900 [99000/160000] training loss: 581.9845595359802\n",
            "epoch 1 batch 10000 [100000/160000] training loss: 609.0707739591599\n",
            "epoch 1 batch 10100 [101000/160000] training loss: 585.3321566581726\n",
            "epoch 1 batch 10200 [102000/160000] training loss: 558.3130161762238\n",
            "epoch 1 batch 10300 [103000/160000] training loss: 591.1753587722778\n",
            "epoch 1 batch 10400 [104000/160000] training loss: 586.8515267372131\n",
            "epoch 1 batch 10500 [105000/160000] training loss: 621.3145685195923\n",
            "epoch 1 batch 10600 [106000/160000] training loss: 616.9969108104706\n",
            "epoch 1 batch 10700 [107000/160000] training loss: 592.1024768352509\n",
            "epoch 1 batch 10800 [108000/160000] training loss: 577.9872577190399\n",
            "epoch 1 batch 10900 [109000/160000] training loss: 577.5334689617157\n",
            "epoch 1 batch 11000 [110000/160000] training loss: 564.9053092002869\n",
            "epoch 1 batch 11100 [111000/160000] training loss: 587.9554874897003\n",
            "epoch 1 batch 11200 [112000/160000] training loss: 582.4520874023438\n",
            "epoch 1 batch 11300 [113000/160000] training loss: 574.4422705173492\n",
            "epoch 1 batch 11400 [114000/160000] training loss: 600.5832723379135\n",
            "epoch 1 batch 11500 [115000/160000] training loss: 584.0890152454376\n",
            "epoch 1 batch 11600 [116000/160000] training loss: 596.3060433864594\n",
            "epoch 1 batch 11700 [117000/160000] training loss: 564.3553252220154\n",
            "epoch 1 batch 11800 [118000/160000] training loss: 578.3349468708038\n",
            "epoch 1 batch 11900 [119000/160000] training loss: 569.617258310318\n",
            "epoch 1 batch 12000 [120000/160000] training loss: 576.4586553573608\n",
            "epoch 1 batch 12100 [121000/160000] training loss: 593.3643867969513\n",
            "epoch 1 batch 12200 [122000/160000] training loss: 566.7117969989777\n",
            "epoch 1 batch 12300 [123000/160000] training loss: 586.0663270950317\n",
            "epoch 1 batch 12400 [124000/160000] training loss: 594.5429677963257\n",
            "epoch 1 batch 12500 [125000/160000] training loss: 578.5587458610535\n",
            "epoch 1 batch 12600 [126000/160000] training loss: 578.7736923694611\n",
            "epoch 1 batch 12700 [127000/160000] training loss: 573.9475405216217\n",
            "epoch 1 batch 12800 [128000/160000] training loss: 602.2300715446472\n",
            "epoch 1 batch 12900 [129000/160000] training loss: 593.1823935508728\n",
            "epoch 1 batch 13000 [130000/160000] training loss: 601.2511715888977\n",
            "epoch 1 batch 13100 [131000/160000] training loss: 579.7810816764832\n",
            "epoch 1 batch 13200 [132000/160000] training loss: 618.830502986908\n",
            "epoch 1 batch 13300 [133000/160000] training loss: 604.4089300632477\n",
            "epoch 1 batch 13400 [134000/160000] training loss: 562.6409494876862\n",
            "epoch 1 batch 13500 [135000/160000] training loss: 578.7299451828003\n",
            "epoch 1 batch 13600 [136000/160000] training loss: 564.5669348239899\n",
            "epoch 1 batch 13700 [137000/160000] training loss: 611.4700810909271\n",
            "epoch 1 batch 13800 [138000/160000] training loss: 574.0653936862946\n",
            "epoch 1 batch 13900 [139000/160000] training loss: 610.2857880592346\n",
            "epoch 1 batch 14000 [140000/160000] training loss: 596.9076845645905\n",
            "epoch 1 batch 14100 [141000/160000] training loss: 581.113178730011\n",
            "epoch 1 batch 14200 [142000/160000] training loss: 588.3736624717712\n",
            "epoch 1 batch 14300 [143000/160000] training loss: 597.0247874259949\n",
            "epoch 1 batch 14400 [144000/160000] training loss: 604.2220454216003\n",
            "epoch 1 batch 14500 [145000/160000] training loss: 593.3348250389099\n",
            "epoch 1 batch 14600 [146000/160000] training loss: 595.7883715629578\n",
            "epoch 1 batch 14700 [147000/160000] training loss: 584.6942067146301\n",
            "epoch 1 batch 14800 [148000/160000] training loss: 591.2921578884125\n",
            "epoch 1 batch 14900 [149000/160000] training loss: 591.7760717868805\n",
            "epoch 1 batch 15000 [150000/160000] training loss: 621.9581837654114\n",
            "epoch 1 batch 15100 [151000/160000] training loss: 596.3497107028961\n",
            "epoch 1 batch 15200 [152000/160000] training loss: 622.5364623069763\n",
            "epoch 1 batch 15300 [153000/160000] training loss: 578.4660408496857\n",
            "epoch 1 batch 15400 [154000/160000] training loss: 612.6893529891968\n",
            "epoch 1 batch 15500 [155000/160000] training loss: 586.0309314727783\n",
            "epoch 1 batch 15600 [156000/160000] training loss: 553.6461141109467\n",
            "epoch 1 batch 15700 [157000/160000] training loss: 575.0646314620972\n",
            "epoch 1 batch 15800 [158000/160000] training loss: 575.7268164157867\n",
            "epoch 1 batch 15900 [159000/160000] training loss: 565.322508096695\n",
            "epoch 2 batch 0 [0/160000] training loss: 6.052274227142334\n",
            "epoch 2 batch 100 [1000/160000] training loss: 582.7512813806534\n",
            "epoch 2 batch 200 [2000/160000] training loss: 580.8347895145416\n",
            "epoch 2 batch 300 [3000/160000] training loss: 581.6341009140015\n",
            "epoch 2 batch 400 [4000/160000] training loss: 588.1497068405151\n",
            "epoch 2 batch 500 [5000/160000] training loss: 577.8336398601532\n",
            "epoch 2 batch 600 [6000/160000] training loss: 576.2306439876556\n",
            "epoch 2 batch 700 [7000/160000] training loss: 565.7285070419312\n",
            "epoch 2 batch 800 [8000/160000] training loss: 596.564014673233\n",
            "epoch 2 batch 900 [9000/160000] training loss: 583.8920629024506\n",
            "epoch 2 batch 1000 [10000/160000] training loss: 588.8985712528229\n",
            "epoch 2 batch 1100 [11000/160000] training loss: 577.6980690956116\n",
            "epoch 2 batch 1200 [12000/160000] training loss: 591.6932139396667\n",
            "epoch 2 batch 1300 [13000/160000] training loss: 606.5005035400391\n",
            "epoch 2 batch 1400 [14000/160000] training loss: 574.6541743278503\n",
            "epoch 2 batch 1500 [15000/160000] training loss: 552.2477366924286\n",
            "epoch 2 batch 1600 [16000/160000] training loss: 595.1985375881195\n",
            "epoch 2 batch 1700 [17000/160000] training loss: 593.6625726222992\n",
            "epoch 2 batch 1800 [18000/160000] training loss: 582.981707572937\n",
            "epoch 2 batch 1900 [19000/160000] training loss: 582.1801798343658\n",
            "epoch 2 batch 2000 [20000/160000] training loss: 580.6622431278229\n",
            "epoch 2 batch 2100 [21000/160000] training loss: 545.9862189292908\n",
            "epoch 2 batch 2200 [22000/160000] training loss: 595.4029381275177\n",
            "epoch 2 batch 2300 [23000/160000] training loss: 581.0819997787476\n",
            "epoch 2 batch 2400 [24000/160000] training loss: 573.7688398361206\n",
            "epoch 2 batch 2500 [25000/160000] training loss: 613.0131034851074\n",
            "epoch 2 batch 2600 [26000/160000] training loss: 566.2633554935455\n",
            "epoch 2 batch 2700 [27000/160000] training loss: 591.1616344451904\n",
            "epoch 2 batch 2800 [28000/160000] training loss: 556.2458097934723\n",
            "epoch 2 batch 2900 [29000/160000] training loss: 621.2774815559387\n",
            "epoch 2 batch 3000 [30000/160000] training loss: 578.6293120384216\n",
            "epoch 2 batch 3100 [31000/160000] training loss: 596.0688962936401\n",
            "epoch 2 batch 3200 [32000/160000] training loss: 582.8308067321777\n",
            "epoch 2 batch 3300 [33000/160000] training loss: 589.1989731788635\n",
            "epoch 2 batch 3400 [34000/160000] training loss: 570.8912386894226\n",
            "epoch 2 batch 3500 [35000/160000] training loss: 602.8514430522919\n",
            "epoch 2 batch 3600 [36000/160000] training loss: 579.9122550487518\n",
            "epoch 2 batch 3700 [37000/160000] training loss: 569.4589021205902\n",
            "epoch 2 batch 3800 [38000/160000] training loss: 585.7960727214813\n",
            "epoch 2 batch 3900 [39000/160000] training loss: 582.4113807678223\n",
            "epoch 2 batch 4000 [40000/160000] training loss: 569.8894140720367\n",
            "epoch 2 batch 4100 [41000/160000] training loss: 575.0344204902649\n",
            "epoch 2 batch 4200 [42000/160000] training loss: 608.2408571243286\n",
            "epoch 2 batch 4300 [43000/160000] training loss: 579.669305562973\n",
            "epoch 2 batch 4400 [44000/160000] training loss: 568.2892850637436\n",
            "epoch 2 batch 4500 [45000/160000] training loss: 582.9310476779938\n",
            "epoch 2 batch 4600 [46000/160000] training loss: 553.5754306316376\n",
            "epoch 2 batch 4700 [47000/160000] training loss: 554.6212952136993\n",
            "epoch 2 batch 4800 [48000/160000] training loss: 575.050873041153\n",
            "epoch 2 batch 4900 [49000/160000] training loss: 584.3503737449646\n",
            "epoch 2 batch 5000 [50000/160000] training loss: 570.7296726703644\n",
            "epoch 2 batch 5100 [51000/160000] training loss: 567.1521832942963\n",
            "epoch 2 batch 5200 [52000/160000] training loss: 579.3062171936035\n",
            "epoch 2 batch 5300 [53000/160000] training loss: 600.2181870937347\n",
            "epoch 2 batch 5400 [54000/160000] training loss: 567.0073845386505\n",
            "epoch 2 batch 5500 [55000/160000] training loss: 565.2371726036072\n",
            "epoch 2 batch 5600 [56000/160000] training loss: 589.5579750537872\n",
            "epoch 2 batch 5700 [57000/160000] training loss: 585.4095778465271\n",
            "epoch 2 batch 5800 [58000/160000] training loss: 577.784012556076\n",
            "epoch 2 batch 5900 [59000/160000] training loss: 556.6812198162079\n",
            "epoch 2 batch 6000 [60000/160000] training loss: 594.8846786022186\n",
            "epoch 2 batch 6100 [61000/160000] training loss: 592.0755128860474\n",
            "epoch 2 batch 6200 [62000/160000] training loss: 600.515873670578\n",
            "epoch 2 batch 6300 [63000/160000] training loss: 561.1556332111359\n",
            "epoch 2 batch 6400 [64000/160000] training loss: 563.4013969898224\n",
            "epoch 2 batch 6500 [65000/160000] training loss: 587.118070602417\n",
            "epoch 2 batch 6600 [66000/160000] training loss: 566.2193796634674\n",
            "epoch 2 batch 6700 [67000/160000] training loss: 591.968005657196\n",
            "epoch 2 batch 6800 [68000/160000] training loss: 553.1533567905426\n",
            "epoch 2 batch 6900 [69000/160000] training loss: 573.2247135639191\n",
            "epoch 2 batch 7000 [70000/160000] training loss: 568.8821029663086\n",
            "epoch 2 batch 7100 [71000/160000] training loss: 556.3857967853546\n",
            "epoch 2 batch 7200 [72000/160000] training loss: 572.3828003406525\n",
            "epoch 2 batch 7300 [73000/160000] training loss: 567.4137830734253\n",
            "epoch 2 batch 7400 [74000/160000] training loss: 566.939285993576\n",
            "epoch 2 batch 7500 [75000/160000] training loss: 569.4581564664841\n",
            "epoch 2 batch 7600 [76000/160000] training loss: 587.9919587373734\n",
            "epoch 2 batch 7700 [77000/160000] training loss: 600.4508981704712\n",
            "epoch 2 batch 7800 [78000/160000] training loss: 595.2723200321198\n",
            "epoch 2 batch 7900 [79000/160000] training loss: 566.9370541572571\n",
            "epoch 2 batch 8000 [80000/160000] training loss: 570.9187703132629\n",
            "epoch 2 batch 8100 [81000/160000] training loss: 566.0933663845062\n",
            "epoch 2 batch 8200 [82000/160000] training loss: 558.1385610103607\n",
            "epoch 2 batch 8300 [83000/160000] training loss: 587.2213940620422\n",
            "epoch 2 batch 8400 [84000/160000] training loss: 591.0405476093292\n",
            "epoch 2 batch 8500 [85000/160000] training loss: 594.7581642866135\n",
            "epoch 2 batch 8600 [86000/160000] training loss: 562.2967643737793\n",
            "epoch 2 batch 8700 [87000/160000] training loss: 574.2430393695831\n",
            "epoch 2 batch 8800 [88000/160000] training loss: 590.8842959403992\n",
            "epoch 2 batch 8900 [89000/160000] training loss: 600.2872329950333\n",
            "epoch 2 batch 9000 [90000/160000] training loss: 596.6274673938751\n",
            "epoch 2 batch 9100 [91000/160000] training loss: 589.849116563797\n",
            "epoch 2 batch 9200 [92000/160000] training loss: 552.3545353412628\n",
            "epoch 2 batch 9300 [93000/160000] training loss: 583.1438601016998\n",
            "epoch 2 batch 9400 [94000/160000] training loss: 552.0290780067444\n",
            "epoch 2 batch 9500 [95000/160000] training loss: 579.6469804048538\n",
            "epoch 2 batch 9600 [96000/160000] training loss: 574.2248377799988\n",
            "epoch 2 batch 9700 [97000/160000] training loss: 574.1463599205017\n",
            "epoch 2 batch 9800 [98000/160000] training loss: 572.7113130092621\n",
            "epoch 2 batch 9900 [99000/160000] training loss: 574.4230999946594\n",
            "epoch 2 batch 10000 [100000/160000] training loss: 603.1968631744385\n",
            "epoch 2 batch 10100 [101000/160000] training loss: 576.3079397678375\n",
            "epoch 2 batch 10200 [102000/160000] training loss: 548.5721211433411\n",
            "epoch 2 batch 10300 [103000/160000] training loss: 583.6172777414322\n",
            "epoch 2 batch 10400 [104000/160000] training loss: 577.5874404907227\n",
            "epoch 2 batch 10500 [105000/160000] training loss: 613.3087475299835\n",
            "epoch 2 batch 10600 [106000/160000] training loss: 606.6635272502899\n",
            "epoch 2 batch 10700 [107000/160000] training loss: 584.1510519981384\n",
            "epoch 2 batch 10800 [108000/160000] training loss: 565.6387958526611\n",
            "epoch 2 batch 10900 [109000/160000] training loss: 568.9286744594574\n",
            "epoch 2 batch 11000 [110000/160000] training loss: 561.5696678161621\n",
            "epoch 2 batch 11100 [111000/160000] training loss: 575.9188268184662\n",
            "epoch 2 batch 11200 [112000/160000] training loss: 579.2996082305908\n",
            "epoch 2 batch 11300 [113000/160000] training loss: 565.9325697422028\n",
            "epoch 2 batch 11400 [114000/160000] training loss: 593.75142121315\n",
            "epoch 2 batch 11500 [115000/160000] training loss: 578.9820852279663\n",
            "epoch 2 batch 11600 [116000/160000] training loss: 591.8035042285919\n",
            "epoch 2 batch 11700 [117000/160000] training loss: 555.3623999357224\n",
            "epoch 2 batch 11800 [118000/160000] training loss: 571.184427022934\n",
            "epoch 2 batch 11900 [119000/160000] training loss: 559.1353754997253\n",
            "epoch 2 batch 12000 [120000/160000] training loss: 572.7028965950012\n",
            "epoch 2 batch 12100 [121000/160000] training loss: 591.4823231697083\n",
            "epoch 2 batch 12200 [122000/160000] training loss: 560.1064667701721\n",
            "epoch 2 batch 12300 [123000/160000] training loss: 577.6920425891876\n",
            "epoch 2 batch 12400 [124000/160000] training loss: 586.6578345298767\n",
            "epoch 2 batch 12500 [125000/160000] training loss: 569.9005272388458\n",
            "epoch 2 batch 12600 [126000/160000] training loss: 573.561261177063\n",
            "epoch 2 batch 12700 [127000/160000] training loss: 566.7999503612518\n",
            "epoch 2 batch 12800 [128000/160000] training loss: 591.5136106014252\n",
            "epoch 2 batch 12900 [129000/160000] training loss: 585.388011932373\n",
            "epoch 2 batch 13000 [130000/160000] training loss: 596.3906910419464\n",
            "epoch 2 batch 13100 [131000/160000] training loss: 569.1158275604248\n",
            "epoch 2 batch 13200 [132000/160000] training loss: 612.0719496011734\n",
            "epoch 2 batch 13300 [133000/160000] training loss: 593.8129739761353\n",
            "epoch 2 batch 13400 [134000/160000] training loss: 555.6535601615906\n",
            "epoch 2 batch 13500 [135000/160000] training loss: 570.870551109314\n",
            "epoch 2 batch 13600 [136000/160000] training loss: 556.6053249835968\n",
            "epoch 2 batch 13700 [137000/160000] training loss: 603.8173053264618\n",
            "epoch 2 batch 13800 [138000/160000] training loss: 572.0357623100281\n",
            "epoch 2 batch 13900 [139000/160000] training loss: 602.5010986328125\n",
            "epoch 2 batch 14000 [140000/160000] training loss: 589.5970144271851\n",
            "epoch 2 batch 14100 [141000/160000] training loss: 573.9484798908234\n",
            "epoch 2 batch 14200 [142000/160000] training loss: 581.2842316627502\n",
            "epoch 2 batch 14300 [143000/160000] training loss: 587.4627203941345\n",
            "epoch 2 batch 14400 [144000/160000] training loss: 596.1618225574493\n",
            "epoch 2 batch 14500 [145000/160000] training loss: 583.169896364212\n",
            "epoch 2 batch 14600 [146000/160000] training loss: 587.0548920631409\n",
            "epoch 2 batch 14700 [147000/160000] training loss: 575.7894477844238\n",
            "epoch 2 batch 14800 [148000/160000] training loss: 582.2030572891235\n",
            "epoch 2 batch 14900 [149000/160000] training loss: 581.311418056488\n",
            "epoch 2 batch 15000 [150000/160000] training loss: 616.2199450731277\n",
            "epoch 2 batch 15100 [151000/160000] training loss: 587.4540457725525\n",
            "epoch 2 batch 15200 [152000/160000] training loss: 616.8625860214233\n",
            "epoch 2 batch 15300 [153000/160000] training loss: 574.7074160575867\n",
            "epoch 2 batch 15400 [154000/160000] training loss: 603.5633084774017\n",
            "epoch 2 batch 15500 [155000/160000] training loss: 576.765664100647\n",
            "epoch 2 batch 15600 [156000/160000] training loss: 547.9480035305023\n",
            "epoch 2 batch 15700 [157000/160000] training loss: 567.6420199871063\n",
            "epoch 2 batch 15800 [158000/160000] training loss: 569.6638021469116\n",
            "epoch 2 batch 15900 [159000/160000] training loss: 557.6512506008148\n",
            "epoch 3 batch 0 [0/160000] training loss: 5.992264747619629\n",
            "epoch 3 batch 100 [1000/160000] training loss: 578.3361296653748\n",
            "epoch 3 batch 200 [2000/160000] training loss: 576.6520764827728\n",
            "epoch 3 batch 300 [3000/160000] training loss: 576.1083877086639\n",
            "epoch 3 batch 400 [4000/160000] training loss: 584.9672417640686\n",
            "epoch 3 batch 500 [5000/160000] training loss: 575.7737367153168\n",
            "epoch 3 batch 600 [6000/160000] training loss: 571.7094776630402\n",
            "epoch 3 batch 700 [7000/160000] training loss: 561.6132004261017\n",
            "epoch 3 batch 800 [8000/160000] training loss: 590.7131400108337\n",
            "epoch 3 batch 900 [9000/160000] training loss: 583.260810136795\n",
            "epoch 3 batch 1000 [10000/160000] training loss: 585.1160492897034\n",
            "epoch 3 batch 1100 [11000/160000] training loss: 570.9663965702057\n",
            "epoch 3 batch 1200 [12000/160000] training loss: 585.6513750553131\n",
            "epoch 3 batch 1300 [13000/160000] training loss: 603.1390662193298\n",
            "epoch 3 batch 1400 [14000/160000] training loss: 569.3679690361023\n",
            "epoch 3 batch 1500 [15000/160000] training loss: 549.7599701881409\n",
            "epoch 3 batch 1600 [16000/160000] training loss: 589.2237496376038\n",
            "epoch 3 batch 1700 [17000/160000] training loss: 589.7212495803833\n",
            "epoch 3 batch 1800 [18000/160000] training loss: 578.2261755466461\n",
            "epoch 3 batch 1900 [19000/160000] training loss: 577.0525238513947\n",
            "epoch 3 batch 2000 [20000/160000] training loss: 571.9141798019409\n",
            "epoch 3 batch 2100 [21000/160000] training loss: 543.7522504329681\n",
            "epoch 3 batch 2200 [22000/160000] training loss: 592.5257935523987\n",
            "epoch 3 batch 2300 [23000/160000] training loss: 577.7711718082428\n",
            "epoch 3 batch 2400 [24000/160000] training loss: 568.5592150688171\n",
            "epoch 3 batch 2500 [25000/160000] training loss: 607.179450750351\n",
            "epoch 3 batch 2600 [26000/160000] training loss: 561.5443940162659\n",
            "epoch 3 batch 2700 [27000/160000] training loss: 589.4354453086853\n",
            "epoch 3 batch 2800 [28000/160000] training loss: 548.5179297924042\n",
            "epoch 3 batch 2900 [29000/160000] training loss: 618.6169939041138\n",
            "epoch 3 batch 3000 [30000/160000] training loss: 576.860723733902\n",
            "epoch 3 batch 3100 [31000/160000] training loss: 592.7792611122131\n",
            "epoch 3 batch 3200 [32000/160000] training loss: 577.2658517360687\n",
            "epoch 3 batch 3300 [33000/160000] training loss: 584.2543704509735\n",
            "epoch 3 batch 3400 [34000/160000] training loss: 569.1056041717529\n",
            "epoch 3 batch 3500 [35000/160000] training loss: 594.6574010848999\n",
            "epoch 3 batch 3600 [36000/160000] training loss: 574.3740825653076\n",
            "epoch 3 batch 3700 [37000/160000] training loss: 562.1801495552063\n",
            "epoch 3 batch 3800 [38000/160000] training loss: 579.5461575984955\n",
            "epoch 3 batch 3900 [39000/160000] training loss: 577.3049304485321\n",
            "epoch 3 batch 4000 [40000/160000] training loss: 567.8186333179474\n",
            "epoch 3 batch 4100 [41000/160000] training loss: 571.7353365421295\n",
            "epoch 3 batch 4200 [42000/160000] training loss: 604.1611368656158\n",
            "epoch 3 batch 4300 [43000/160000] training loss: 577.307067155838\n",
            "epoch 3 batch 4400 [44000/160000] training loss: 562.9989159107208\n",
            "epoch 3 batch 4500 [45000/160000] training loss: 579.9704599380493\n",
            "epoch 3 batch 4600 [46000/160000] training loss: 545.9659976959229\n",
            "epoch 3 batch 4700 [47000/160000] training loss: 551.6516082286835\n",
            "epoch 3 batch 4800 [48000/160000] training loss: 572.17040848732\n",
            "epoch 3 batch 4900 [49000/160000] training loss: 578.0229213237762\n",
            "epoch 3 batch 5000 [50000/160000] training loss: 566.9332580566406\n",
            "epoch 3 batch 5100 [51000/160000] training loss: 562.9749894142151\n",
            "epoch 3 batch 5200 [52000/160000] training loss: 573.5691945552826\n",
            "epoch 3 batch 5300 [53000/160000] training loss: 591.4514428377151\n",
            "epoch 3 batch 5400 [54000/160000] training loss: 563.4170119762421\n",
            "epoch 3 batch 5500 [55000/160000] training loss: 560.2070243358612\n",
            "epoch 3 batch 5600 [56000/160000] training loss: 584.4968321323395\n",
            "epoch 3 batch 5700 [57000/160000] training loss: 582.1179428100586\n",
            "epoch 3 batch 5800 [58000/160000] training loss: 574.8635280132294\n",
            "epoch 3 batch 5900 [59000/160000] training loss: 552.5883293151855\n",
            "epoch 3 batch 6000 [60000/160000] training loss: 593.3912703990936\n",
            "epoch 3 batch 6100 [61000/160000] training loss: 587.535392999649\n",
            "epoch 3 batch 6200 [62000/160000] training loss: 596.7691628932953\n",
            "epoch 3 batch 6300 [63000/160000] training loss: 558.76011967659\n",
            "epoch 3 batch 6400 [64000/160000] training loss: 558.2344102859497\n",
            "epoch 3 batch 6500 [65000/160000] training loss: 580.4965801239014\n",
            "epoch 3 batch 6600 [66000/160000] training loss: 558.8599207401276\n",
            "epoch 3 batch 6700 [67000/160000] training loss: 587.8476667404175\n",
            "epoch 3 batch 6800 [68000/160000] training loss: 548.7959334850311\n",
            "epoch 3 batch 6900 [69000/160000] training loss: 565.6484417915344\n",
            "epoch 3 batch 7000 [70000/160000] training loss: 562.4821171760559\n",
            "epoch 3 batch 7100 [71000/160000] training loss: 548.224326133728\n",
            "epoch 3 batch 7200 [72000/160000] training loss: 568.4781847000122\n",
            "epoch 3 batch 7300 [73000/160000] training loss: 562.8556206226349\n",
            "epoch 3 batch 7400 [74000/160000] training loss: 562.578647851944\n",
            "epoch 3 batch 7500 [75000/160000] training loss: 567.5679644346237\n",
            "epoch 3 batch 7600 [76000/160000] training loss: 583.0893663167953\n",
            "epoch 3 batch 7700 [77000/160000] training loss: 598.3069231510162\n",
            "epoch 3 batch 7800 [78000/160000] training loss: 593.9663298130035\n",
            "epoch 3 batch 7900 [79000/160000] training loss: 562.5971255302429\n",
            "epoch 3 batch 8000 [80000/160000] training loss: 566.7086429595947\n",
            "epoch 3 batch 8100 [81000/160000] training loss: 561.7135138511658\n",
            "epoch 3 batch 8200 [82000/160000] training loss: 550.6413912773132\n",
            "epoch 3 batch 8300 [83000/160000] training loss: 582.5425357818604\n",
            "epoch 3 batch 8400 [84000/160000] training loss: 584.6694934368134\n",
            "epoch 3 batch 8500 [85000/160000] training loss: 590.2551398277283\n",
            "epoch 3 batch 8600 [86000/160000] training loss: 557.6043348312378\n",
            "epoch 3 batch 8700 [87000/160000] training loss: 569.7078170776367\n",
            "epoch 3 batch 8800 [88000/160000] training loss: 587.1993143558502\n",
            "epoch 3 batch 8900 [89000/160000] training loss: 594.2669539451599\n",
            "epoch 3 batch 9000 [90000/160000] training loss: 592.3931798934937\n",
            "epoch 3 batch 9100 [91000/160000] training loss: 588.299868106842\n",
            "epoch 3 batch 9200 [92000/160000] training loss: 542.8302855491638\n",
            "epoch 3 batch 9300 [93000/160000] training loss: 582.5743095874786\n",
            "epoch 3 batch 9400 [94000/160000] training loss: 547.9285981655121\n",
            "epoch 3 batch 9500 [95000/160000] training loss: 574.9039314985275\n",
            "epoch 3 batch 9600 [96000/160000] training loss: 569.3471252918243\n",
            "epoch 3 batch 9700 [97000/160000] training loss: 570.4591009616852\n",
            "epoch 3 batch 9800 [98000/160000] training loss: 570.1762702465057\n",
            "epoch 3 batch 9900 [99000/160000] training loss: 570.5297441482544\n",
            "epoch 3 batch 10000 [100000/160000] training loss: 600.8098456859589\n",
            "epoch 3 batch 10100 [101000/160000] training loss: 572.7047090530396\n",
            "epoch 3 batch 10200 [102000/160000] training loss: 543.6900918483734\n",
            "epoch 3 batch 10300 [103000/160000] training loss: 580.3447703123093\n",
            "epoch 3 batch 10400 [104000/160000] training loss: 573.4228920936584\n",
            "epoch 3 batch 10500 [105000/160000] training loss: 610.0753042697906\n",
            "epoch 3 batch 10600 [106000/160000] training loss: 601.0693302154541\n",
            "epoch 3 batch 10700 [107000/160000] training loss: 579.2188075780869\n",
            "epoch 3 batch 10800 [108000/160000] training loss: 559.1640455722809\n",
            "epoch 3 batch 10900 [109000/160000] training loss: 563.827380657196\n",
            "epoch 3 batch 11000 [110000/160000] training loss: 561.5716276168823\n",
            "epoch 3 batch 11100 [111000/160000] training loss: 569.1394755840302\n",
            "epoch 3 batch 11200 [112000/160000] training loss: 577.1842665672302\n",
            "epoch 3 batch 11300 [113000/160000] training loss: 561.7677750587463\n",
            "epoch 3 batch 11400 [114000/160000] training loss: 590.0126671791077\n",
            "epoch 3 batch 11500 [115000/160000] training loss: 576.5830626487732\n",
            "epoch 3 batch 11600 [116000/160000] training loss: 590.5470204353333\n",
            "epoch 3 batch 11700 [117000/160000] training loss: 551.4315857887268\n",
            "epoch 3 batch 11800 [118000/160000] training loss: 566.8795466423035\n",
            "epoch 3 batch 11900 [119000/160000] training loss: 553.7837604284286\n",
            "epoch 3 batch 12000 [120000/160000] training loss: 570.7297782897949\n",
            "epoch 3 batch 12100 [121000/160000] training loss: 591.6977696418762\n",
            "epoch 3 batch 12200 [122000/160000] training loss: 557.2942974567413\n",
            "epoch 3 batch 12300 [123000/160000] training loss: 573.6239104270935\n",
            "epoch 3 batch 12400 [124000/160000] training loss: 582.0945687294006\n",
            "epoch 3 batch 12500 [125000/160000] training loss: 565.1016192436218\n",
            "epoch 3 batch 12600 [126000/160000] training loss: 571.4143220186234\n",
            "epoch 3 batch 12700 [127000/160000] training loss: 563.0072727203369\n",
            "epoch 3 batch 12800 [128000/160000] training loss: 586.1910231113434\n",
            "epoch 3 batch 12900 [129000/160000] training loss: 582.1732473373413\n",
            "epoch 3 batch 13000 [130000/160000] training loss: 592.7884538173676\n",
            "epoch 3 batch 13100 [131000/160000] training loss: 563.8889362812042\n",
            "epoch 3 batch 13200 [132000/160000] training loss: 607.2204231023788\n",
            "epoch 3 batch 13300 [133000/160000] training loss: 588.0383675098419\n",
            "epoch 3 batch 13400 [134000/160000] training loss: 552.3145184516907\n",
            "epoch 3 batch 13500 [135000/160000] training loss: 567.1871576309204\n",
            "epoch 3 batch 13600 [136000/160000] training loss: 552.3394477367401\n",
            "epoch 3 batch 13700 [137000/160000] training loss: 600.445431470871\n",
            "epoch 3 batch 13800 [138000/160000] training loss: 572.3228933811188\n",
            "epoch 3 batch 13900 [139000/160000] training loss: 599.4118094444275\n",
            "epoch 3 batch 14000 [140000/160000] training loss: 584.9758496284485\n",
            "epoch 3 batch 14100 [141000/160000] training loss: 568.6801354885101\n",
            "epoch 3 batch 14200 [142000/160000] training loss: 577.3979649543762\n",
            "epoch 3 batch 14300 [143000/160000] training loss: 582.2280166149139\n",
            "epoch 3 batch 14400 [144000/160000] training loss: 591.732652425766\n",
            "epoch 3 batch 14500 [145000/160000] training loss: 577.9804127216339\n",
            "epoch 3 batch 14600 [146000/160000] training loss: 581.2914663553238\n",
            "epoch 3 batch 14700 [147000/160000] training loss: 570.659265756607\n",
            "epoch 3 batch 14800 [148000/160000] training loss: 577.5523793697357\n",
            "epoch 3 batch 14900 [149000/160000] training loss: 575.3978946208954\n",
            "epoch 3 batch 15000 [150000/160000] training loss: 612.9055334329605\n",
            "epoch 3 batch 15100 [151000/160000] training loss: 582.0028638839722\n",
            "epoch 3 batch 15200 [152000/160000] training loss: 613.1517388820648\n",
            "epoch 3 batch 15300 [153000/160000] training loss: 572.5920445919037\n",
            "epoch 3 batch 15400 [154000/160000] training loss: 598.9573719501495\n",
            "epoch 3 batch 15500 [155000/160000] training loss: 571.353363275528\n",
            "epoch 3 batch 15600 [156000/160000] training loss: 545.6152453422546\n",
            "epoch 3 batch 15700 [157000/160000] training loss: 562.670224905014\n",
            "epoch 3 batch 15800 [158000/160000] training loss: 566.5728847980499\n",
            "epoch 3 batch 15900 [159000/160000] training loss: 553.1097056865692\n",
            "epoch 4 batch 0 [0/160000] training loss: 5.896219730377197\n",
            "epoch 4 batch 100 [1000/160000] training loss: 575.6379241943359\n",
            "epoch 4 batch 200 [2000/160000] training loss: 574.4572942256927\n",
            "epoch 4 batch 300 [3000/160000] training loss: 573.4816470146179\n",
            "epoch 4 batch 400 [4000/160000] training loss: 582.6968936920166\n",
            "epoch 4 batch 500 [5000/160000] training loss: 575.4528484344482\n",
            "epoch 4 batch 600 [6000/160000] training loss: 569.7293779850006\n",
            "epoch 4 batch 700 [7000/160000] training loss: 559.5985993146896\n",
            "epoch 4 batch 800 [8000/160000] training loss: 587.2307696342468\n",
            "epoch 4 batch 900 [9000/160000] training loss: 582.1883671283722\n",
            "epoch 4 batch 1000 [10000/160000] training loss: 582.5081691741943\n",
            "epoch 4 batch 1100 [11000/160000] training loss: 567.426317691803\n",
            "epoch 4 batch 1200 [12000/160000] training loss: 580.353306055069\n",
            "epoch 4 batch 1300 [13000/160000] training loss: 600.7811243534088\n",
            "epoch 4 batch 1400 [14000/160000] training loss: 566.4677674770355\n",
            "epoch 4 batch 1500 [15000/160000] training loss: 548.5963101387024\n",
            "epoch 4 batch 1600 [16000/160000] training loss: 585.9810161590576\n",
            "epoch 4 batch 1700 [17000/160000] training loss: 587.4111757278442\n",
            "epoch 4 batch 1800 [18000/160000] training loss: 575.414228439331\n",
            "epoch 4 batch 1900 [19000/160000] training loss: 573.31689286232\n",
            "epoch 4 batch 2000 [20000/160000] training loss: 566.271064043045\n",
            "epoch 4 batch 2100 [21000/160000] training loss: 543.1803846359253\n",
            "epoch 4 batch 2200 [22000/160000] training loss: 591.1595768928528\n",
            "epoch 4 batch 2300 [23000/160000] training loss: 575.462916135788\n",
            "epoch 4 batch 2400 [24000/160000] training loss: 565.1996095180511\n",
            "epoch 4 batch 2500 [25000/160000] training loss: 603.4726140499115\n",
            "epoch 4 batch 2600 [26000/160000] training loss: 559.0582356452942\n",
            "epoch 4 batch 2700 [27000/160000] training loss: 588.1397788524628\n",
            "epoch 4 batch 2800 [28000/160000] training loss: 543.9873492717743\n",
            "epoch 4 batch 2900 [29000/160000] training loss: 617.0935864448547\n",
            "epoch 4 batch 3000 [30000/160000] training loss: 576.127691745758\n",
            "epoch 4 batch 3100 [31000/160000] training loss: 590.781996011734\n",
            "epoch 4 batch 3200 [32000/160000] training loss: 573.6003565788269\n",
            "epoch 4 batch 3300 [33000/160000] training loss: 581.4585647583008\n",
            "epoch 4 batch 3400 [34000/160000] training loss: 568.5674774646759\n",
            "epoch 4 batch 3500 [35000/160000] training loss: 590.2333359718323\n",
            "epoch 4 batch 3600 [36000/160000] training loss: 571.3026728630066\n",
            "epoch 4 batch 3700 [37000/160000] training loss: 557.6608437299728\n",
            "epoch 4 batch 3800 [38000/160000] training loss: 574.8360748291016\n",
            "epoch 4 batch 3900 [39000/160000] training loss: 574.6006586551666\n",
            "epoch 4 batch 4000 [40000/160000] training loss: 566.5015397071838\n",
            "epoch 4 batch 4100 [41000/160000] training loss: 571.1084439754486\n",
            "epoch 4 batch 4200 [42000/160000] training loss: 601.2725064754486\n",
            "epoch 4 batch 4300 [43000/160000] training loss: 576.502002120018\n",
            "epoch 4 batch 4400 [44000/160000] training loss: 559.9726730585098\n",
            "epoch 4 batch 4500 [45000/160000] training loss: 578.2981340885162\n",
            "epoch 4 batch 4600 [46000/160000] training loss: 541.6884424686432\n",
            "epoch 4 batch 4700 [47000/160000] training loss: 549.9822664260864\n",
            "epoch 4 batch 4800 [48000/160000] training loss: 570.3135657310486\n",
            "epoch 4 batch 4900 [49000/160000] training loss: 574.1839544773102\n",
            "epoch 4 batch 5000 [50000/160000] training loss: 565.0106041431427\n",
            "epoch 4 batch 5100 [51000/160000] training loss: 560.8332867622375\n",
            "epoch 4 batch 5200 [52000/160000] training loss: 570.4632968902588\n",
            "epoch 4 batch 5300 [53000/160000] training loss: 585.4322472810745\n",
            "epoch 4 batch 5400 [54000/160000] training loss: 562.3162536621094\n",
            "epoch 4 batch 5500 [55000/160000] training loss: 557.5735442638397\n",
            "epoch 4 batch 5600 [56000/160000] training loss: 581.614955663681\n",
            "epoch 4 batch 5700 [57000/160000] training loss: 579.7943346500397\n",
            "epoch 4 batch 5800 [58000/160000] training loss: 572.8849101066589\n",
            "epoch 4 batch 5900 [59000/160000] training loss: 550.303849697113\n",
            "epoch 4 batch 6000 [60000/160000] training loss: 592.1642551422119\n",
            "epoch 4 batch 6100 [61000/160000] training loss: 584.1943438053131\n",
            "epoch 4 batch 6200 [62000/160000] training loss: 594.4144675731659\n",
            "epoch 4 batch 6300 [63000/160000] training loss: 557.5343668460846\n",
            "epoch 4 batch 6400 [64000/160000] training loss: 555.5726749897003\n",
            "epoch 4 batch 6500 [65000/160000] training loss: 576.2138087749481\n",
            "epoch 4 batch 6600 [66000/160000] training loss: 553.7446715831757\n",
            "epoch 4 batch 6700 [67000/160000] training loss: 585.642706155777\n",
            "epoch 4 batch 6800 [68000/160000] training loss: 546.2342884540558\n",
            "epoch 4 batch 6900 [69000/160000] training loss: 561.0683124065399\n",
            "epoch 4 batch 7000 [70000/160000] training loss: 557.8877804279327\n",
            "epoch 4 batch 7100 [71000/160000] training loss: 542.9664258956909\n",
            "epoch 4 batch 7200 [72000/160000] training loss: 565.9075036048889\n",
            "epoch 4 batch 7300 [73000/160000] training loss: 559.8074586391449\n",
            "epoch 4 batch 7400 [74000/160000] training loss: 559.4246273040771\n",
            "epoch 4 batch 7500 [75000/160000] training loss: 566.2871260643005\n",
            "epoch 4 batch 7600 [76000/160000] training loss: 579.7281295061111\n",
            "epoch 4 batch 7700 [77000/160000] training loss: 596.7078964710236\n",
            "epoch 4 batch 7800 [78000/160000] training loss: 593.0266239643097\n",
            "epoch 4 batch 7900 [79000/160000] training loss: 560.2648322582245\n",
            "epoch 4 batch 8000 [80000/160000] training loss: 563.9371616840363\n",
            "epoch 4 batch 8100 [81000/160000] training loss: 558.4992690086365\n",
            "epoch 4 batch 8200 [82000/160000] training loss: 546.4109108448029\n",
            "epoch 4 batch 8300 [83000/160000] training loss: 579.987875699997\n",
            "epoch 4 batch 8400 [84000/160000] training loss: 580.9782018661499\n",
            "epoch 4 batch 8500 [85000/160000] training loss: 587.7011594772339\n",
            "epoch 4 batch 8600 [86000/160000] training loss: 554.4482588768005\n",
            "epoch 4 batch 8700 [87000/160000] training loss: 566.5254395008087\n",
            "epoch 4 batch 8800 [88000/160000] training loss: 584.9708228111267\n",
            "epoch 4 batch 8900 [89000/160000] training loss: 590.1247117519379\n",
            "epoch 4 batch 9000 [90000/160000] training loss: 589.9292154312134\n",
            "epoch 4 batch 9100 [91000/160000] training loss: 587.0827479362488\n",
            "epoch 4 batch 9200 [92000/160000] training loss: 536.9577496051788\n",
            "epoch 4 batch 9300 [93000/160000] training loss: 582.0698885917664\n",
            "epoch 4 batch 9400 [94000/160000] training loss: 544.6607799530029\n",
            "epoch 4 batch 9500 [95000/160000] training loss: 572.4308594465256\n",
            "epoch 4 batch 9600 [96000/160000] training loss: 566.0601754188538\n",
            "epoch 4 batch 9700 [97000/160000] training loss: 567.733195900917\n",
            "epoch 4 batch 9800 [98000/160000] training loss: 568.3095126152039\n",
            "epoch 4 batch 9900 [99000/160000] training loss: 568.2913143634796\n",
            "epoch 4 batch 10000 [100000/160000] training loss: 599.4701042175293\n",
            "epoch 4 batch 10100 [101000/160000] training loss: 570.7727348804474\n",
            "epoch 4 batch 10200 [102000/160000] training loss: 540.9771280288696\n",
            "epoch 4 batch 10300 [103000/160000] training loss: 578.3935710191727\n",
            "epoch 4 batch 10400 [104000/160000] training loss: 571.2146492004395\n",
            "epoch 4 batch 10500 [105000/160000] training loss: 608.1941096782684\n",
            "epoch 4 batch 10600 [106000/160000] training loss: 597.6122252941132\n",
            "epoch 4 batch 10700 [107000/160000] training loss: 575.6811596155167\n",
            "epoch 4 batch 10800 [108000/160000] training loss: 555.0834918022156\n",
            "epoch 4 batch 10900 [109000/160000] training loss: 560.2054730653763\n",
            "epoch 4 batch 11000 [110000/160000] training loss: 562.2470586299896\n",
            "epoch 4 batch 11100 [111000/160000] training loss: 564.5953989028931\n",
            "epoch 4 batch 11200 [112000/160000] training loss: 575.6055760383606\n",
            "epoch 4 batch 11300 [113000/160000] training loss: 559.3030986785889\n",
            "epoch 4 batch 11400 [114000/160000] training loss: 586.7226836681366\n",
            "epoch 4 batch 11500 [115000/160000] training loss: 575.2086329460144\n",
            "epoch 4 batch 11600 [116000/160000] training loss: 590.4611577987671\n",
            "epoch 4 batch 11700 [117000/160000] training loss: 549.3773756027222\n",
            "epoch 4 batch 11800 [118000/160000] training loss: 563.861222743988\n",
            "epoch 4 batch 11900 [119000/160000] training loss: 550.861212015152\n",
            "epoch 4 batch 12000 [120000/160000] training loss: 569.5735377073288\n",
            "epoch 4 batch 12100 [121000/160000] training loss: 592.1373062133789\n",
            "epoch 4 batch 12200 [122000/160000] training loss: 555.7886984348297\n",
            "epoch 4 batch 12300 [123000/160000] training loss: 571.3720102310181\n",
            "epoch 4 batch 12400 [124000/160000] training loss: 578.9298095703125\n",
            "epoch 4 batch 12500 [125000/160000] training loss: 562.1340584754944\n",
            "epoch 4 batch 12600 [126000/160000] training loss: 570.3046305179596\n",
            "epoch 4 batch 12700 [127000/160000] training loss: 560.5386209487915\n",
            "epoch 4 batch 12800 [128000/160000] training loss: 582.9915571212769\n",
            "epoch 4 batch 12900 [129000/160000] training loss: 580.5157563686371\n",
            "epoch 4 batch 13000 [130000/160000] training loss: 590.2546098232269\n",
            "epoch 4 batch 13100 [131000/160000] training loss: 561.1882703304291\n",
            "epoch 4 batch 13200 [132000/160000] training loss: 603.7748711109161\n",
            "epoch 4 batch 13300 [133000/160000] training loss: 584.3588635921478\n",
            "epoch 4 batch 13400 [134000/160000] training loss: 550.5150272846222\n",
            "epoch 4 batch 13500 [135000/160000] training loss: 565.3484420776367\n",
            "epoch 4 batch 13600 [136000/160000] training loss: 549.6154091358185\n",
            "epoch 4 batch 13700 [137000/160000] training loss: 598.760356426239\n",
            "epoch 4 batch 13800 [138000/160000] training loss: 573.0859735012054\n",
            "epoch 4 batch 13900 [139000/160000] training loss: 598.1676535606384\n",
            "epoch 4 batch 14000 [140000/160000] training loss: 581.4974160194397\n",
            "epoch 4 batch 14100 [141000/160000] training loss: 564.7082896232605\n",
            "epoch 4 batch 14200 [142000/160000] training loss: 574.7694194316864\n",
            "epoch 4 batch 14300 [143000/160000] training loss: 578.8575971126556\n",
            "epoch 4 batch 14400 [144000/160000] training loss: 588.9630641937256\n",
            "epoch 4 batch 14500 [145000/160000] training loss: 574.9220035076141\n",
            "epoch 4 batch 14600 [146000/160000] training loss: 577.0674040317535\n",
            "epoch 4 batch 14700 [147000/160000] training loss: 567.0885405540466\n",
            "epoch 4 batch 14800 [148000/160000] training loss: 574.888436794281\n",
            "epoch 4 batch 14900 [149000/160000] training loss: 571.4175748825073\n",
            "epoch 4 batch 15000 [150000/160000] training loss: 610.3454627990723\n",
            "epoch 4 batch 15100 [151000/160000] training loss: 578.4667456150055\n",
            "epoch 4 batch 15200 [152000/160000] training loss: 610.4536445140839\n",
            "epoch 4 batch 15300 [153000/160000] training loss: 571.1406519412994\n",
            "epoch 4 batch 15400 [154000/160000] training loss: 596.2447276115417\n",
            "epoch 4 batch 15500 [155000/160000] training loss: 567.6235435009003\n",
            "epoch 4 batch 15600 [156000/160000] training loss: 544.0050523281097\n",
            "epoch 4 batch 15700 [157000/160000] training loss: 559.2110152244568\n",
            "epoch 4 batch 15800 [158000/160000] training loss: 564.7712016105652\n",
            "epoch 4 batch 15900 [159000/160000] training loss: 550.1455907821655\n",
            "epoch 5 batch 0 [0/160000] training loss: 5.7998199462890625\n",
            "epoch 5 batch 100 [1000/160000] training loss: 573.9956555366516\n",
            "epoch 5 batch 200 [2000/160000] training loss: 573.3583402633667\n",
            "epoch 5 batch 300 [3000/160000] training loss: 572.1994822025299\n",
            "epoch 5 batch 400 [4000/160000] training loss: 580.942803144455\n",
            "epoch 5 batch 500 [5000/160000] training loss: 575.7600328922272\n",
            "epoch 5 batch 600 [6000/160000] training loss: 568.6136775016785\n",
            "epoch 5 batch 700 [7000/160000] training loss: 558.4251394271851\n",
            "epoch 5 batch 800 [8000/160000] training loss: 585.1723515987396\n",
            "epoch 5 batch 900 [9000/160000] training loss: 580.8924624919891\n",
            "epoch 5 batch 1000 [10000/160000] training loss: 580.8473777770996\n",
            "epoch 5 batch 1100 [11000/160000] training loss: 565.1010286808014\n",
            "epoch 5 batch 1200 [12000/160000] training loss: 575.8212428092957\n",
            "epoch 5 batch 1300 [13000/160000] training loss: 599.053985118866\n",
            "epoch 5 batch 1400 [14000/160000] training loss: 564.8197050094604\n",
            "epoch 5 batch 1500 [15000/160000] training loss: 547.4848122596741\n",
            "epoch 5 batch 1600 [16000/160000] training loss: 583.8573429584503\n",
            "epoch 5 batch 1700 [17000/160000] training loss: 585.9182894229889\n",
            "epoch 5 batch 1800 [18000/160000] training loss: 573.517413020134\n",
            "epoch 5 batch 1900 [19000/160000] training loss: 570.4577267169952\n",
            "epoch 5 batch 2000 [20000/160000] training loss: 562.1013822555542\n",
            "epoch 5 batch 2100 [21000/160000] training loss: 543.1996428966522\n",
            "epoch 5 batch 2200 [22000/160000] training loss: 590.435849905014\n",
            "epoch 5 batch 2300 [23000/160000] training loss: 573.9297063350677\n",
            "epoch 5 batch 2400 [24000/160000] training loss: 562.8472490310669\n",
            "epoch 5 batch 2500 [25000/160000] training loss: 600.8572814464569\n",
            "epoch 5 batch 2600 [26000/160000] training loss: 557.614988565445\n",
            "epoch 5 batch 2700 [27000/160000] training loss: 586.9255735874176\n",
            "epoch 5 batch 2800 [28000/160000] training loss: 541.0252895355225\n",
            "epoch 5 batch 2900 [29000/160000] training loss: 616.1787958145142\n",
            "epoch 5 batch 3000 [30000/160000] training loss: 575.8804230690002\n",
            "epoch 5 batch 3100 [31000/160000] training loss: 589.4473600387573\n",
            "epoch 5 batch 3200 [32000/160000] training loss: 570.824675321579\n",
            "epoch 5 batch 3300 [33000/160000] training loss: 579.5838544368744\n",
            "epoch 5 batch 3400 [34000/160000] training loss: 568.535763502121\n",
            "epoch 5 batch 3500 [35000/160000] training loss: 587.6267991065979\n",
            "epoch 5 batch 3600 [36000/160000] training loss: 569.4213585853577\n",
            "epoch 5 batch 3700 [37000/160000] training loss: 554.6020836830139\n",
            "epoch 5 batch 3800 [38000/160000] training loss: 571.3873875141144\n",
            "epoch 5 batch 3900 [39000/160000] training loss: 573.1791243553162\n",
            "epoch 5 batch 4000 [40000/160000] training loss: 565.5294270515442\n",
            "epoch 5 batch 4100 [41000/160000] training loss: 571.4833555221558\n",
            "epoch 5 batch 4200 [42000/160000] training loss: 599.1342799663544\n",
            "epoch 5 batch 4300 [43000/160000] training loss: 576.1691294908524\n",
            "epoch 5 batch 4400 [44000/160000] training loss: 558.0551761388779\n",
            "epoch 5 batch 4500 [45000/160000] training loss: 577.2487802505493\n",
            "epoch 5 batch 4600 [46000/160000] training loss: 539.1144826412201\n",
            "epoch 5 batch 4700 [47000/160000] training loss: 548.8676738739014\n",
            "epoch 5 batch 4800 [48000/160000] training loss: 568.9908540248871\n",
            "epoch 5 batch 4900 [49000/160000] training loss: 571.6774146556854\n",
            "epoch 5 batch 5000 [50000/160000] training loss: 563.7764587402344\n",
            "epoch 5 batch 5100 [51000/160000] training loss: 559.6054084300995\n",
            "epoch 5 batch 5200 [52000/160000] training loss: 568.6969935894012\n",
            "epoch 5 batch 5300 [53000/160000] training loss: 581.0491601228714\n",
            "epoch 5 batch 5400 [54000/160000] training loss: 562.2269825935364\n",
            "epoch 5 batch 5500 [55000/160000] training loss: 556.2203090190887\n",
            "epoch 5 batch 5600 [56000/160000] training loss: 579.9384245872498\n",
            "epoch 5 batch 5700 [57000/160000] training loss: 577.9221441745758\n",
            "epoch 5 batch 5800 [58000/160000] training loss: 571.5214456319809\n",
            "epoch 5 batch 5900 [59000/160000] training loss: 548.8124029636383\n",
            "epoch 5 batch 6000 [60000/160000] training loss: 590.9826369285583\n",
            "epoch 5 batch 6100 [61000/160000] training loss: 581.5699152946472\n",
            "epoch 5 batch 6200 [62000/160000] training loss: 592.8468596935272\n",
            "epoch 5 batch 6300 [63000/160000] training loss: 556.9663550853729\n",
            "epoch 5 batch 6400 [64000/160000] training loss: 553.9171450138092\n",
            "epoch 5 batch 6500 [65000/160000] training loss: 572.9400968551636\n",
            "epoch 5 batch 6600 [66000/160000] training loss: 549.8626420497894\n",
            "epoch 5 batch 6700 [67000/160000] training loss: 584.4051692485809\n",
            "epoch 5 batch 6800 [68000/160000] training loss: 544.7685346603394\n",
            "epoch 5 batch 6900 [69000/160000] training loss: 557.9644446372986\n",
            "epoch 5 batch 7000 [70000/160000] training loss: 554.4648694992065\n",
            "epoch 5 batch 7100 [71000/160000] training loss: 539.4059453010559\n",
            "epoch 5 batch 7200 [72000/160000] training loss: 564.1950302124023\n",
            "epoch 5 batch 7300 [73000/160000] training loss: 557.556369304657\n",
            "epoch 5 batch 7400 [74000/160000] training loss: 556.8334784507751\n",
            "epoch 5 batch 7500 [75000/160000] training loss: 565.3204092979431\n",
            "epoch 5 batch 7600 [76000/160000] training loss: 577.2768099308014\n",
            "epoch 5 batch 7700 [77000/160000] training loss: 595.1180636882782\n",
            "epoch 5 batch 7800 [78000/160000] training loss: 592.2535262107849\n",
            "epoch 5 batch 7900 [79000/160000] training loss: 558.8101186752319\n",
            "epoch 5 batch 8000 [80000/160000] training loss: 561.9393501281738\n",
            "epoch 5 batch 8100 [81000/160000] training loss: 555.9238840341568\n",
            "epoch 5 batch 8200 [82000/160000] training loss: 543.857503414154\n",
            "epoch 5 batch 8300 [83000/160000] training loss: 578.4901385307312\n",
            "epoch 5 batch 8400 [84000/160000] training loss: 578.7039947509766\n",
            "epoch 5 batch 8500 [85000/160000] training loss: 586.0271459817886\n",
            "epoch 5 batch 8600 [86000/160000] training loss: 552.2530310153961\n",
            "epoch 5 batch 8700 [87000/160000] training loss: 564.2312088012695\n",
            "epoch 5 batch 8800 [88000/160000] training loss: 583.3766028881073\n",
            "epoch 5 batch 8900 [89000/160000] training loss: 587.007309794426\n",
            "epoch 5 batch 9000 [90000/160000] training loss: 588.3936364650726\n",
            "epoch 5 batch 9100 [91000/160000] training loss: 586.113392829895\n",
            "epoch 5 batch 9200 [92000/160000] training loss: 533.0489081144333\n",
            "epoch 5 batch 9300 [93000/160000] training loss: 581.5120782852173\n",
            "epoch 5 batch 9400 [94000/160000] training loss: 541.9118587970734\n",
            "epoch 5 batch 9500 [95000/160000] training loss: 571.148074388504\n",
            "epoch 5 batch 9600 [96000/160000] training loss: 563.6012778282166\n",
            "epoch 5 batch 9700 [97000/160000] training loss: 565.426442027092\n",
            "epoch 5 batch 9800 [98000/160000] training loss: 566.7927238941193\n",
            "epoch 5 batch 9900 [99000/160000] training loss: 566.959958076477\n",
            "epoch 5 batch 10000 [100000/160000] training loss: 598.4379360675812\n",
            "epoch 5 batch 10100 [101000/160000] training loss: 569.6682130098343\n",
            "epoch 5 batch 10200 [102000/160000] training loss: 539.4256534576416\n",
            "epoch 5 batch 10300 [103000/160000] training loss: 576.9576344490051\n",
            "epoch 5 batch 10400 [104000/160000] training loss: 570.0379711389542\n",
            "epoch 5 batch 10500 [105000/160000] training loss: 606.8369944095612\n",
            "epoch 5 batch 10600 [106000/160000] training loss: 595.309685587883\n",
            "epoch 5 batch 10700 [107000/160000] training loss: 572.957461476326\n",
            "epoch 5 batch 10800 [108000/160000] training loss: 552.2861647605896\n",
            "epoch 5 batch 10900 [109000/160000] training loss: 557.4662463665009\n",
            "epoch 5 batch 11000 [110000/160000] training loss: 562.9787032604218\n",
            "epoch 5 batch 11100 [111000/160000] training loss: 561.3755669593811\n",
            "epoch 5 batch 11200 [112000/160000] training loss: 574.3869106769562\n",
            "epoch 5 batch 11300 [113000/160000] training loss: 557.7129137516022\n",
            "epoch 5 batch 11400 [114000/160000] training loss: 583.6642620563507\n",
            "epoch 5 batch 11500 [115000/160000] training loss: 574.2950274944305\n",
            "epoch 5 batch 11600 [116000/160000] training loss: 590.7684535980225\n",
            "epoch 5 batch 11700 [117000/160000] training loss: 548.072093963623\n",
            "epoch 5 batch 11800 [118000/160000] training loss: 561.5971908569336\n",
            "epoch 5 batch 11900 [119000/160000] training loss: 549.258805513382\n",
            "epoch 5 batch 12000 [120000/160000] training loss: 568.8634585142136\n",
            "epoch 5 batch 12100 [121000/160000] training loss: 592.403538942337\n",
            "epoch 5 batch 12200 [122000/160000] training loss: 554.9069912433624\n",
            "epoch 5 batch 12300 [123000/160000] training loss: 569.9428105354309\n",
            "epoch 5 batch 12400 [124000/160000] training loss: 576.6200459003448\n",
            "epoch 5 batch 12500 [125000/160000] training loss: 560.1575336456299\n",
            "epoch 5 batch 12600 [126000/160000] training loss: 569.6768863201141\n",
            "epoch 5 batch 12700 [127000/160000] training loss: 558.7766599655151\n",
            "epoch 5 batch 12800 [128000/160000] training loss: 580.951530456543\n",
            "epoch 5 batch 12900 [129000/160000] training loss: 579.575169801712\n",
            "epoch 5 batch 13000 [130000/160000] training loss: 588.5354442596436\n",
            "epoch 5 batch 13100 [131000/160000] training loss: 559.8765661716461\n",
            "epoch 5 batch 13200 [132000/160000] training loss: 601.1771893501282\n",
            "epoch 5 batch 13300 [133000/160000] training loss: 581.73725938797\n",
            "epoch 5 batch 13400 [134000/160000] training loss: 549.5719254016876\n",
            "epoch 5 batch 13500 [135000/160000] training loss: 564.4300246238708\n",
            "epoch 5 batch 13600 [136000/160000] training loss: 547.6506042480469\n",
            "epoch 5 batch 13700 [137000/160000] training loss: 597.9178838729858\n",
            "epoch 5 batch 13800 [138000/160000] training loss: 573.9122619628906\n",
            "epoch 5 batch 13900 [139000/160000] training loss: 597.700962305069\n",
            "epoch 5 batch 14000 [140000/160000] training loss: 578.7612991333008\n",
            "epoch 5 batch 14100 [141000/160000] training loss: 561.6704339981079\n",
            "epoch 5 batch 14200 [142000/160000] training loss: 572.8791217803955\n",
            "epoch 5 batch 14300 [143000/160000] training loss: 576.3761944770813\n",
            "epoch 5 batch 14400 [144000/160000] training loss: 587.0892963409424\n",
            "epoch 5 batch 14500 [145000/160000] training loss: 572.9376907348633\n",
            "epoch 5 batch 14600 [146000/160000] training loss: 573.8345122337341\n",
            "epoch 5 batch 14700 [147000/160000] training loss: 564.4018020629883\n",
            "epoch 5 batch 14800 [148000/160000] training loss: 573.2110285758972\n",
            "epoch 5 batch 14900 [149000/160000] training loss: 568.4485173225403\n",
            "epoch 5 batch 15000 [150000/160000] training loss: 608.2448233366013\n",
            "epoch 5 batch 15100 [151000/160000] training loss: 576.1328961849213\n",
            "epoch 5 batch 15200 [152000/160000] training loss: 608.438572883606\n",
            "epoch 5 batch 15300 [153000/160000] training loss: 569.9870276451111\n",
            "epoch 5 batch 15400 [154000/160000] training loss: 594.4315025806427\n",
            "epoch 5 batch 15500 [155000/160000] training loss: 564.8928532600403\n",
            "epoch 5 batch 15600 [156000/160000] training loss: 542.6090049743652\n",
            "epoch 5 batch 15700 [157000/160000] training loss: 556.747252702713\n",
            "epoch 5 batch 15800 [158000/160000] training loss: 563.6048288345337\n",
            "epoch 5 batch 15900 [159000/160000] training loss: 548.1535649299622\n",
            "epoch 6 batch 0 [0/160000] training loss: 5.716917991638184\n",
            "epoch 6 batch 100 [1000/160000] training loss: 572.9709686040878\n",
            "epoch 6 batch 200 [2000/160000] training loss: 572.864667892456\n",
            "epoch 6 batch 300 [3000/160000] training loss: 571.4980750083923\n",
            "epoch 6 batch 400 [4000/160000] training loss: 579.5288381576538\n",
            "epoch 6 batch 500 [5000/160000] training loss: 576.3176023960114\n",
            "epoch 6 batch 600 [6000/160000] training loss: 567.8796317577362\n",
            "epoch 6 batch 700 [7000/160000] training loss: 557.6538548469543\n",
            "epoch 6 batch 800 [8000/160000] training loss: 584.0065929889679\n",
            "epoch 6 batch 900 [9000/160000] training loss: 579.5826187133789\n",
            "epoch 6 batch 1000 [10000/160000] training loss: 579.8620893955231\n",
            "epoch 6 batch 1100 [11000/160000] training loss: 563.3976099491119\n",
            "epoch 6 batch 1200 [12000/160000] training loss: 572.0399918556213\n",
            "epoch 6 batch 1300 [13000/160000] training loss: 597.8097803592682\n",
            "epoch 6 batch 1400 [14000/160000] training loss: 563.8463449478149\n",
            "epoch 6 batch 1500 [15000/160000] training loss: 546.2909798622131\n",
            "epoch 6 batch 1600 [16000/160000] training loss: 582.2761211395264\n",
            "epoch 6 batch 1700 [17000/160000] training loss: 584.7859954833984\n",
            "epoch 6 batch 1800 [18000/160000] training loss: 572.1101003885269\n",
            "epoch 6 batch 1900 [19000/160000] training loss: 568.2401595115662\n",
            "epoch 6 batch 2000 [20000/160000] training loss: 558.8502511978149\n",
            "epoch 6 batch 2100 [21000/160000] training loss: 543.4016515016556\n",
            "epoch 6 batch 2200 [22000/160000] training loss: 590.0319607257843\n",
            "epoch 6 batch 2300 [23000/160000] training loss: 572.948468208313\n",
            "epoch 6 batch 2400 [24000/160000] training loss: 561.1416132450104\n",
            "epoch 6 batch 2500 [25000/160000] training loss: 598.842615365982\n",
            "epoch 6 batch 2600 [26000/160000] training loss: 556.6935415267944\n",
            "epoch 6 batch 2700 [27000/160000] training loss: 585.8895698785782\n",
            "epoch 6 batch 2800 [28000/160000] training loss: 538.916543006897\n",
            "epoch 6 batch 2900 [29000/160000] training loss: 615.5502679347992\n",
            "epoch 6 batch 3000 [30000/160000] training loss: 575.9244122505188\n",
            "epoch 6 batch 3100 [31000/160000] training loss: 588.4896211624146\n",
            "epoch 6 batch 3200 [32000/160000] training loss: 568.532167673111\n",
            "epoch 6 batch 3300 [33000/160000] training loss: 578.1784498691559\n",
            "epoch 6 batch 3400 [34000/160000] training loss: 568.7288980484009\n",
            "epoch 6 batch 3500 [35000/160000] training loss: 585.899031162262\n",
            "epoch 6 batch 3600 [36000/160000] training loss: 568.2006809711456\n",
            "epoch 6 batch 3700 [37000/160000] training loss: 552.4713343381882\n",
            "epoch 6 batch 3800 [38000/160000] training loss: 568.9213070869446\n",
            "epoch 6 batch 3900 [39000/160000] training loss: 572.4669795036316\n",
            "epoch 6 batch 4000 [40000/160000] training loss: 564.7795143127441\n",
            "epoch 6 batch 4100 [41000/160000] training loss: 572.1574468612671\n",
            "epoch 6 batch 4200 [42000/160000] training loss: 597.5045070648193\n",
            "epoch 6 batch 4300 [43000/160000] training loss: 575.9281634092331\n",
            "epoch 6 batch 4400 [44000/160000] training loss: 556.7692515850067\n",
            "epoch 6 batch 4500 [45000/160000] training loss: 576.5670907497406\n",
            "epoch 6 batch 4600 [46000/160000] training loss: 537.5393717288971\n",
            "epoch 6 batch 4700 [47000/160000] training loss: 548.0930848121643\n",
            "epoch 6 batch 4800 [48000/160000] training loss: 567.9790508747101\n",
            "epoch 6 batch 4900 [49000/160000] training loss: 570.0526549816132\n",
            "epoch 6 batch 5000 [50000/160000] training loss: 562.8672182559967\n",
            "epoch 6 batch 5100 [51000/160000] training loss: 558.733286857605\n",
            "epoch 6 batch 5200 [52000/160000] training loss: 567.6931302547455\n",
            "epoch 6 batch 5300 [53000/160000] training loss: 577.7254663705826\n",
            "epoch 6 batch 5400 [54000/160000] training loss: 562.5454623699188\n",
            "epoch 6 batch 5500 [55000/160000] training loss: 555.5542480945587\n",
            "epoch 6 batch 5600 [56000/160000] training loss: 578.8736186027527\n",
            "epoch 6 batch 5700 [57000/160000] training loss: 576.3059959411621\n",
            "epoch 6 batch 5800 [58000/160000] training loss: 570.5206344127655\n",
            "epoch 6 batch 5900 [59000/160000] training loss: 547.7404901981354\n",
            "epoch 6 batch 6000 [60000/160000] training loss: 589.8608317375183\n",
            "epoch 6 batch 6100 [61000/160000] training loss: 579.4507472515106\n",
            "epoch 6 batch 6200 [62000/160000] training loss: 591.7736821174622\n",
            "epoch 6 batch 6300 [63000/160000] training loss: 556.8043005466461\n",
            "epoch 6 batch 6400 [64000/160000] training loss: 552.6938600540161\n",
            "epoch 6 batch 6500 [65000/160000] training loss: 570.2210516929626\n",
            "epoch 6 batch 6600 [66000/160000] training loss: 546.8705747127533\n",
            "epoch 6 batch 6700 [67000/160000] training loss: 583.7026209831238\n",
            "epoch 6 batch 6800 [68000/160000] training loss: 543.8600401878357\n",
            "epoch 6 batch 6900 [69000/160000] training loss: 555.704339504242\n",
            "epoch 6 batch 7000 [70000/160000] training loss: 551.8442907333374\n",
            "epoch 6 batch 7100 [71000/160000] training loss: 536.9496591091156\n",
            "epoch 6 batch 7200 [72000/160000] training loss: 563.1000809669495\n",
            "epoch 6 batch 7300 [73000/160000] training loss: 555.8452141284943\n",
            "epoch 6 batch 7400 [74000/160000] training loss: 554.5195701122284\n",
            "epoch 6 batch 7500 [75000/160000] training loss: 564.5056408643723\n",
            "epoch 6 batch 7600 [76000/160000] training loss: 575.4261322021484\n",
            "epoch 6 batch 7700 [77000/160000] training loss: 593.5980067253113\n",
            "epoch 6 batch 7800 [78000/160000] training loss: 591.6326680183411\n",
            "epoch 6 batch 7900 [79000/160000] training loss: 557.7567412853241\n",
            "epoch 6 batch 8000 [80000/160000] training loss: 560.3907432556152\n",
            "epoch 6 batch 8100 [81000/160000] training loss: 553.7972927093506\n",
            "epoch 6 batch 8200 [82000/160000] training loss: 542.2331871986389\n",
            "epoch 6 batch 8300 [83000/160000] training loss: 577.6222114562988\n",
            "epoch 6 batch 8400 [84000/160000] training loss: 577.2413618564606\n",
            "epoch 6 batch 8500 [85000/160000] training loss: 584.8337162733078\n",
            "epoch 6 batch 8600 [86000/160000] training loss: 550.7589688301086\n",
            "epoch 6 batch 8700 [87000/160000] training loss: 562.5794057846069\n",
            "epoch 6 batch 8800 [88000/160000] training loss: 582.0806570053101\n",
            "epoch 6 batch 8900 [89000/160000] training loss: 584.5190024375916\n",
            "epoch 6 batch 9000 [90000/160000] training loss: 587.4514915943146\n",
            "epoch 6 batch 9100 [91000/160000] training loss: 585.3753046989441\n",
            "epoch 6 batch 9200 [92000/160000] training loss: 530.3272100687027\n",
            "epoch 6 batch 9300 [93000/160000] training loss: 580.9401133060455\n",
            "epoch 6 batch 9400 [94000/160000] training loss: 539.6423013210297\n",
            "epoch 6 batch 9500 [95000/160000] training loss: 570.5487018823624\n",
            "epoch 6 batch 9600 [96000/160000] training loss: 561.675199508667\n",
            "epoch 6 batch 9700 [97000/160000] training loss: 563.4419065713882\n",
            "epoch 6 batch 9800 [98000/160000] training loss: 565.4810409545898\n",
            "epoch 6 batch 9900 [99000/160000] training loss: 566.149968624115\n",
            "epoch 6 batch 10000 [100000/160000] training loss: 597.5076758861542\n",
            "epoch 6 batch 10100 [101000/160000] training loss: 569.0644700527191\n",
            "epoch 6 batch 10200 [102000/160000] training loss: 538.5458469390869\n",
            "epoch 6 batch 10300 [103000/160000] training loss: 575.797900557518\n",
            "epoch 6 batch 10400 [104000/160000] training loss: 569.4388837814331\n",
            "epoch 6 batch 10500 [105000/160000] training loss: 605.8162388801575\n",
            "epoch 6 batch 10600 [106000/160000] training loss: 593.6978377103806\n",
            "epoch 6 batch 10700 [107000/160000] training loss: 570.8000582456589\n",
            "epoch 6 batch 10800 [108000/160000] training loss: 550.3201551437378\n",
            "epoch 6 batch 10900 [109000/160000] training loss: 555.2966457605362\n",
            "epoch 6 batch 11000 [110000/160000] training loss: 563.6257967948914\n",
            "epoch 6 batch 11100 [111000/160000] training loss: 559.0719583034515\n",
            "epoch 6 batch 11200 [112000/160000] training loss: 573.4098885059357\n",
            "epoch 6 batch 11300 [113000/160000] training loss: 556.6296782493591\n",
            "epoch 6 batch 11400 [114000/160000] training loss: 580.9111862182617\n",
            "epoch 6 batch 11500 [115000/160000] training loss: 573.6070716381073\n",
            "epoch 6 batch 11600 [116000/160000] training loss: 591.1341681480408\n",
            "epoch 6 batch 11700 [117000/160000] training loss: 547.1280379295349\n",
            "epoch 6 batch 11800 [118000/160000] training loss: 559.833619594574\n",
            "epoch 6 batch 11900 [119000/160000] training loss: 548.3878313302994\n",
            "epoch 6 batch 12000 [120000/160000] training loss: 568.3940181732178\n",
            "epoch 6 batch 12100 [121000/160000] training loss: 592.4525947570801\n",
            "epoch 6 batch 12200 [122000/160000] training loss: 554.392715215683\n",
            "epoch 6 batch 12300 [123000/160000] training loss: 568.9092273712158\n",
            "epoch 6 batch 12400 [124000/160000] training loss: 574.9267716407776\n",
            "epoch 6 batch 12500 [125000/160000] training loss: 558.7509903907776\n",
            "epoch 6 batch 12600 [126000/160000] training loss: 569.2869410514832\n",
            "epoch 6 batch 12700 [127000/160000] training loss: 557.4510078430176\n",
            "epoch 6 batch 12800 [128000/160000] training loss: 579.6422772407532\n",
            "epoch 6 batch 12900 [129000/160000] training loss: 579.0090577602386\n",
            "epoch 6 batch 13000 [130000/160000] training loss: 587.4052655696869\n",
            "epoch 6 batch 13100 [131000/160000] training loss: 559.3784708976746\n",
            "epoch 6 batch 13200 [132000/160000] training loss: 599.1456651687622\n",
            "epoch 6 batch 13300 [133000/160000] training loss: 579.7584366798401\n",
            "epoch 6 batch 13400 [134000/160000] training loss: 549.0897648334503\n",
            "epoch 6 batch 13500 [135000/160000] training loss: 563.9821310043335\n",
            "epoch 6 batch 13600 [136000/160000] training loss: 546.1403696537018\n",
            "epoch 6 batch 13700 [137000/160000] training loss: 597.5208973884583\n",
            "epoch 6 batch 13800 [138000/160000] training loss: 574.7263460159302\n",
            "epoch 6 batch 13900 [139000/160000] training loss: 597.576507806778\n",
            "epoch 6 batch 14000 [140000/160000] training loss: 576.612964630127\n",
            "epoch 6 batch 14100 [141000/160000] training loss: 559.2655503749847\n",
            "epoch 6 batch 14200 [142000/160000] training loss: 571.4943928718567\n",
            "epoch 6 batch 14300 [143000/160000] training loss: 574.3929486274719\n",
            "epoch 6 batch 14400 [144000/160000] training loss: 585.7486696243286\n",
            "epoch 6 batch 14500 [145000/160000] training loss: 571.5330018997192\n",
            "epoch 6 batch 14600 [146000/160000] training loss: 571.2847580909729\n",
            "epoch 6 batch 14700 [147000/160000] training loss: 562.3032393455505\n",
            "epoch 6 batch 14800 [148000/160000] training loss: 572.0524024963379\n",
            "epoch 6 batch 14900 [149000/160000] training loss: 566.0869579315186\n",
            "epoch 6 batch 15000 [150000/160000] training loss: 606.4873056411743\n",
            "epoch 6 batch 15100 [151000/160000] training loss: 574.564909696579\n",
            "epoch 6 batch 15200 [152000/160000] training loss: 606.8820180892944\n",
            "epoch 6 batch 15300 [153000/160000] training loss: 568.989274263382\n",
            "epoch 6 batch 15400 [154000/160000] training loss: 593.1216068267822\n",
            "epoch 6 batch 15500 [155000/160000] training loss: 562.8257355690002\n",
            "epoch 6 batch 15600 [156000/160000] training loss: 541.3785035610199\n",
            "epoch 6 batch 15700 [157000/160000] training loss: 554.9067335128784\n",
            "epoch 6 batch 15800 [158000/160000] training loss: 562.7866914272308\n",
            "epoch 6 batch 15900 [159000/160000] training loss: 546.8041017055511\n",
            "epoch 7 batch 0 [0/160000] training loss: 5.650873184204102\n",
            "epoch 7 batch 100 [1000/160000] training loss: 572.3140662908554\n",
            "epoch 7 batch 200 [2000/160000] training loss: 572.6997406482697\n",
            "epoch 7 batch 300 [3000/160000] training loss: 571.0413706302643\n",
            "epoch 7 batch 400 [4000/160000] training loss: 578.3909831047058\n",
            "epoch 7 batch 500 [5000/160000] training loss: 576.9314074516296\n",
            "epoch 7 batch 600 [6000/160000] training loss: 567.3944847583771\n",
            "epoch 7 batch 700 [7000/160000] training loss: 557.0750180482864\n",
            "epoch 7 batch 800 [8000/160000] training loss: 583.4226306676865\n",
            "epoch 7 batch 900 [9000/160000] training loss: 578.3675425052643\n",
            "epoch 7 batch 1000 [10000/160000] training loss: 579.3240275382996\n",
            "epoch 7 batch 1100 [11000/160000] training loss: 562.0823230743408\n",
            "epoch 7 batch 1200 [12000/160000] training loss: 568.9117319583893\n",
            "epoch 7 batch 1300 [13000/160000] training loss: 596.9419810771942\n",
            "epoch 7 batch 1400 [14000/160000] training loss: 563.2613244056702\n",
            "epoch 7 batch 1500 [15000/160000] training loss: 545.1443517208099\n",
            "epoch 7 batch 1600 [16000/160000] training loss: 581.0098969936371\n",
            "epoch 7 batch 1700 [17000/160000] training loss: 583.8221390247345\n",
            "epoch 7 batch 1800 [18000/160000] training loss: 571.0072504281998\n",
            "epoch 7 batch 1900 [19000/160000] training loss: 566.5069959163666\n",
            "epoch 7 batch 2000 [20000/160000] training loss: 556.270519733429\n",
            "epoch 7 batch 2100 [21000/160000] training loss: 543.6435986757278\n",
            "epoch 7 batch 2200 [22000/160000] training loss: 589.7856948375702\n",
            "epoch 7 batch 2300 [23000/160000] training loss: 572.3214366436005\n",
            "epoch 7 batch 2400 [24000/160000] training loss: 559.8598866462708\n",
            "epoch 7 batch 2500 [25000/160000] training loss: 597.2292859554291\n",
            "epoch 7 batch 2600 [26000/160000] training loss: 556.0646693706512\n",
            "epoch 7 batch 2700 [27000/160000] training loss: 585.0532186031342\n",
            "epoch 7 batch 2800 [28000/160000] training loss: 537.3259449005127\n",
            "epoch 7 batch 2900 [29000/160000] training loss: 615.0264723300934\n",
            "epoch 7 batch 3000 [30000/160000] training loss: 576.1250636577606\n",
            "epoch 7 batch 3100 [31000/160000] training loss: 587.7491579055786\n",
            "epoch 7 batch 3200 [32000/160000] training loss: 566.5505747795105\n",
            "epoch 7 batch 3300 [33000/160000] training loss: 577.0525166988373\n",
            "epoch 7 batch 3400 [34000/160000] training loss: 569.0145807266235\n",
            "epoch 7 batch 3500 [35000/160000] training loss: 584.6144137382507\n",
            "epoch 7 batch 3600 [36000/160000] training loss: 567.4026277065277\n",
            "epoch 7 batch 3700 [37000/160000] training loss: 550.9629416465759\n",
            "epoch 7 batch 3800 [38000/160000] training loss: 567.1577353477478\n",
            "epoch 7 batch 3900 [39000/160000] training loss: 572.1576120853424\n",
            "epoch 7 batch 4000 [40000/160000] training loss: 564.2040901184082\n",
            "epoch 7 batch 4100 [41000/160000] training loss: 572.8395221233368\n",
            "epoch 7 batch 4200 [42000/160000] training loss: 596.2345592975616\n",
            "epoch 7 batch 4300 [43000/160000] training loss: 575.6724883317947\n",
            "epoch 7 batch 4400 [44000/160000] training loss: 555.8602168560028\n",
            "epoch 7 batch 4500 [45000/160000] training loss: 576.1292626857758\n",
            "epoch 7 batch 4600 [46000/160000] training loss: 536.5802609920502\n",
            "epoch 7 batch 4700 [47000/160000] training loss: 547.5376260280609\n",
            "epoch 7 batch 4800 [48000/160000] training loss: 567.1808793544769\n",
            "epoch 7 batch 4900 [49000/160000] training loss: 569.0264751911163\n",
            "epoch 7 batch 5000 [50000/160000] training loss: 562.1407442092896\n",
            "epoch 7 batch 5100 [51000/160000] training loss: 557.9757113456726\n",
            "epoch 7 batch 5200 [52000/160000] training loss: 567.1434164047241\n",
            "epoch 7 batch 5300 [53000/160000] training loss: 575.1439265012741\n",
            "epoch 7 batch 5400 [54000/160000] training loss: 563.0305466651917\n",
            "epoch 7 batch 5500 [55000/160000] training loss: 555.2609076499939\n",
            "epoch 7 batch 5600 [56000/160000] training loss: 578.1260113716125\n",
            "epoch 7 batch 5700 [57000/160000] training loss: 574.8742952346802\n",
            "epoch 7 batch 5800 [58000/160000] training loss: 569.7442989349365\n",
            "epoch 7 batch 5900 [59000/160000] training loss: 546.9180207252502\n",
            "epoch 7 batch 6000 [60000/160000] training loss: 588.8312623500824\n",
            "epoch 7 batch 6100 [61000/160000] training loss: 577.6735873222351\n",
            "epoch 7 batch 6200 [62000/160000] training loss: 591.0453991889954\n",
            "epoch 7 batch 6300 [63000/160000] training loss: 556.9008586406708\n",
            "epoch 7 batch 6400 [64000/160000] training loss: 551.6822633743286\n",
            "epoch 7 batch 6500 [65000/160000] training loss: 567.9062888622284\n",
            "epoch 7 batch 6600 [66000/160000] training loss: 544.5664858818054\n",
            "epoch 7 batch 6700 [67000/160000] training loss: 583.2988114356995\n",
            "epoch 7 batch 6800 [68000/160000] training loss: 543.1783435344696\n",
            "epoch 7 batch 6900 [69000/160000] training loss: 553.9842176437378\n",
            "epoch 7 batch 7000 [70000/160000] training loss: 549.7855322360992\n",
            "epoch 7 batch 7100 [71000/160000] training loss: 535.2284967899323\n",
            "epoch 7 batch 7200 [72000/160000] training loss: 562.4340636730194\n",
            "epoch 7 batch 7300 [73000/160000] training loss: 554.5226054191589\n",
            "epoch 7 batch 7400 [74000/160000] training loss: 552.4055767059326\n",
            "epoch 7 batch 7500 [75000/160000] training loss: 563.712299823761\n",
            "epoch 7 batch 7600 [76000/160000] training loss: 573.9811153411865\n",
            "epoch 7 batch 7700 [77000/160000] training loss: 592.2395122051239\n",
            "epoch 7 batch 7800 [78000/160000] training loss: 591.137687921524\n",
            "epoch 7 batch 7900 [79000/160000] training loss: 556.9254896640778\n",
            "epoch 7 batch 8000 [80000/160000] training loss: 559.1349594593048\n",
            "epoch 7 batch 8100 [81000/160000] training loss: 552.0150824785233\n",
            "epoch 7 batch 8200 [82000/160000] training loss: 541.1513946056366\n",
            "epoch 7 batch 8300 [83000/160000] training loss: 577.1478002071381\n",
            "epoch 7 batch 8400 [84000/160000] training loss: 576.2531926631927\n",
            "epoch 7 batch 8500 [85000/160000] training loss: 583.958017706871\n",
            "epoch 7 batch 8600 [86000/160000] training loss: 549.7759826183319\n",
            "epoch 7 batch 8700 [87000/160000] training loss: 561.3945128917694\n",
            "epoch 7 batch 8800 [88000/160000] training loss: 580.9528975486755\n",
            "epoch 7 batch 8900 [89000/160000] training loss: 582.4406158924103\n",
            "epoch 7 batch 9000 [90000/160000] training loss: 586.9048914909363\n",
            "epoch 7 batch 9100 [91000/160000] training loss: 584.84321641922\n",
            "epoch 7 batch 9200 [92000/160000] training loss: 528.3929592370987\n",
            "epoch 7 batch 9300 [93000/160000] training loss: 580.3986291885376\n",
            "epoch 7 batch 9400 [94000/160000] training loss: 537.8031520843506\n",
            "epoch 7 batch 9500 [95000/160000] training loss: 570.3435831069946\n",
            "epoch 7 batch 9600 [96000/160000] training loss: 560.1301954984665\n",
            "epoch 7 batch 9700 [97000/160000] training loss: 561.7286704778671\n",
            "epoch 7 batch 9800 [98000/160000] training loss: 564.2929072380066\n",
            "epoch 7 batch 9900 [99000/160000] training loss: 565.660679101944\n",
            "epoch 7 batch 10000 [100000/160000] training loss: 596.6320979595184\n",
            "epoch 7 batch 10100 [101000/160000] training loss: 568.7659803628922\n",
            "epoch 7 batch 10200 [102000/160000] training loss: 538.081237077713\n",
            "epoch 7 batch 10300 [103000/160000] training loss: 574.8192126750946\n",
            "epoch 7 batch 10400 [104000/160000] training loss: 569.16830098629\n",
            "epoch 7 batch 10500 [105000/160000] training loss: 605.0547649860382\n",
            "epoch 7 batch 10600 [106000/160000] training loss: 592.5172472000122\n",
            "epoch 7 batch 10700 [107000/160000] training loss: 569.059387087822\n",
            "epoch 7 batch 10800 [108000/160000] training loss: 548.9398137331009\n",
            "epoch 7 batch 10900 [109000/160000] training loss: 553.5164791345596\n",
            "epoch 7 batch 11000 [110000/160000] training loss: 564.147988319397\n",
            "epoch 7 batch 11100 [111000/160000] training loss: 557.4188702106476\n",
            "epoch 7 batch 11200 [112000/160000] training loss: 572.6075124740601\n",
            "epoch 7 batch 11300 [113000/160000] training loss: 555.8686665296555\n",
            "epoch 7 batch 11400 [114000/160000] training loss: 578.5078831911087\n",
            "epoch 7 batch 11500 [115000/160000] training loss: 573.0643272399902\n",
            "epoch 7 batch 11600 [116000/160000] training loss: 591.4349720478058\n",
            "epoch 7 batch 11700 [117000/160000] training loss: 546.3852813243866\n",
            "epoch 7 batch 11800 [118000/160000] training loss: 558.4186251163483\n",
            "epoch 7 batch 11900 [119000/160000] training loss: 547.9230269193649\n",
            "epoch 7 batch 12000 [120000/160000] training loss: 568.0513432025909\n",
            "epoch 7 batch 12100 [121000/160000] training loss: 592.318990945816\n",
            "epoch 7 batch 12200 [122000/160000] training loss: 554.1245474815369\n",
            "epoch 7 batch 12300 [123000/160000] training loss: 568.0908212661743\n",
            "epoch 7 batch 12400 [124000/160000] training loss: 573.689710855484\n",
            "epoch 7 batch 12500 [125000/160000] training loss: 557.6994349956512\n",
            "epoch 7 batch 12600 [126000/160000] training loss: 569.0237835645676\n",
            "epoch 7 batch 12700 [127000/160000] training loss: 556.4133629798889\n",
            "epoch 7 batch 12800 [128000/160000] training loss: 578.8179569244385\n",
            "epoch 7 batch 12900 [129000/160000] training loss: 578.6456065177917\n",
            "epoch 7 batch 13000 [130000/160000] training loss: 586.680103302002\n",
            "epoch 7 batch 13100 [131000/160000] training loss: 559.3598592281342\n",
            "epoch 7 batch 13200 [132000/160000] training loss: 597.546525478363\n",
            "epoch 7 batch 13300 [133000/160000] training loss: 578.2100002765656\n",
            "epoch 7 batch 13400 [134000/160000] training loss: 548.8290777206421\n",
            "epoch 7 batch 13500 [135000/160000] training loss: 563.7715899944305\n",
            "epoch 7 batch 13600 [136000/160000] training loss: 544.9478397369385\n",
            "epoch 7 batch 13700 [137000/160000] training loss: 597.371559381485\n",
            "epoch 7 batch 13800 [138000/160000] training loss: 575.5196008682251\n",
            "epoch 7 batch 13900 [139000/160000] training loss: 597.6156697273254\n",
            "epoch 7 batch 14000 [140000/160000] training loss: 574.9380226135254\n",
            "epoch 7 batch 14100 [141000/160000] training loss: 557.2771532535553\n",
            "epoch 7 batch 14200 [142000/160000] training loss: 570.4608550071716\n",
            "epoch 7 batch 14300 [143000/160000] training loss: 572.7471029758453\n",
            "epoch 7 batch 14400 [144000/160000] training loss: 584.7541427612305\n",
            "epoch 7 batch 14500 [145000/160000] training loss: 570.4683620929718\n",
            "epoch 7 batch 14600 [146000/160000] training loss: 569.2231558561325\n",
            "epoch 7 batch 14700 [147000/160000] training loss: 560.6245429515839\n",
            "epoch 7 batch 14800 [148000/160000] training loss: 571.176998257637\n",
            "epoch 7 batch 14900 [149000/160000] training loss: 564.1360332965851\n",
            "epoch 7 batch 15000 [150000/160000] training loss: 605.0050249099731\n",
            "epoch 7 batch 15100 [151000/160000] training loss: 573.4800410270691\n",
            "epoch 7 batch 15200 [152000/160000] training loss: 605.6306467056274\n",
            "epoch 7 batch 15300 [153000/160000] training loss: 568.116082906723\n",
            "epoch 7 batch 15400 [154000/160000] training loss: 592.1178011894226\n",
            "epoch 7 batch 15500 [155000/160000] training loss: 561.2136588096619\n",
            "epoch 7 batch 15600 [156000/160000] training loss: 540.332731962204\n",
            "epoch 7 batch 15700 [157000/160000] training loss: 553.4533319473267\n",
            "epoch 7 batch 15800 [158000/160000] training loss: 562.1778991222382\n",
            "epoch 7 batch 15900 [159000/160000] training loss: 545.9023127555847\n",
            "epoch 8 batch 0 [0/160000] training loss: 5.600207328796387\n",
            "epoch 8 batch 100 [1000/160000] training loss: 571.8842172622681\n",
            "epoch 8 batch 200 [2000/160000] training loss: 572.7153677940369\n",
            "epoch 8 batch 300 [3000/160000] training loss: 570.6979827880859\n",
            "epoch 8 batch 400 [4000/160000] training loss: 577.4874756336212\n",
            "epoch 8 batch 500 [5000/160000] training loss: 577.5054688453674\n",
            "epoch 8 batch 600 [6000/160000] training loss: 567.0967667102814\n",
            "epoch 8 batch 700 [7000/160000] training loss: 556.5531797409058\n",
            "epoch 8 batch 800 [8000/160000] training loss: 583.2315312623978\n",
            "epoch 8 batch 900 [9000/160000] training loss: 577.2812025547028\n",
            "epoch 8 batch 1000 [10000/160000] training loss: 579.074060678482\n",
            "epoch 8 batch 1100 [11000/160000] training loss: 561.028548002243\n",
            "epoch 8 batch 1200 [12000/160000] training loss: 566.3168632984161\n",
            "epoch 8 batch 1300 [13000/160000] training loss: 596.3445789813995\n",
            "epoch 8 batch 1400 [14000/160000] training loss: 562.9225966930389\n",
            "epoch 8 batch 1500 [15000/160000] training loss: 544.1145334243774\n",
            "epoch 8 batch 1600 [16000/160000] training loss: 579.948406457901\n",
            "epoch 8 batch 1700 [17000/160000] training loss: 582.9559080600739\n",
            "epoch 8 batch 1800 [18000/160000] training loss: 570.1218771934509\n",
            "epoch 8 batch 1900 [19000/160000] training loss: 565.1353950500488\n",
            "epoch 8 batch 2000 [20000/160000] training loss: 554.1980080604553\n",
            "epoch 8 batch 2100 [21000/160000] training loss: 543.881633400917\n",
            "epoch 8 batch 2200 [22000/160000] training loss: 589.6222152709961\n",
            "epoch 8 batch 2300 [23000/160000] training loss: 571.9047658443451\n",
            "epoch 8 batch 2400 [24000/160000] training loss: 558.8690943717957\n",
            "epoch 8 batch 2500 [25000/160000] training loss: 595.9178051948547\n",
            "epoch 8 batch 2600 [26000/160000] training loss: 555.6141984462738\n",
            "epoch 8 batch 2700 [27000/160000] training loss: 584.4055688381195\n",
            "epoch 8 batch 2800 [28000/160000] training loss: 536.0720394849777\n",
            "epoch 8 batch 2900 [29000/160000] training loss: 614.5178234577179\n",
            "epoch 8 batch 3000 [30000/160000] training loss: 576.3839976787567\n",
            "epoch 8 batch 3100 [31000/160000] training loss: 587.1439859867096\n",
            "epoch 8 batch 3200 [32000/160000] training loss: 564.8008637428284\n",
            "epoch 8 batch 3300 [33000/160000] training loss: 576.117377281189\n",
            "epoch 8 batch 3400 [34000/160000] training loss: 569.3308727741241\n",
            "epoch 8 batch 3500 [35000/160000] training loss: 583.5712401866913\n",
            "epoch 8 batch 3600 [36000/160000] training loss: 566.8932454586029\n",
            "epoch 8 batch 3700 [37000/160000] training loss: 549.8790150880814\n",
            "epoch 8 batch 3800 [38000/160000] training loss: 565.8853824138641\n",
            "epoch 8 batch 3900 [39000/160000] training loss: 572.0890188217163\n",
            "epoch 8 batch 4000 [40000/160000] training loss: 563.7717704772949\n",
            "epoch 8 batch 4100 [41000/160000] training loss: 573.4225511550903\n",
            "epoch 8 batch 4200 [42000/160000] training loss: 595.2345316410065\n",
            "epoch 8 batch 4300 [43000/160000] training loss: 575.3789579868317\n",
            "epoch 8 batch 4400 [44000/160000] training loss: 555.1909343004227\n",
            "epoch 8 batch 4500 [45000/160000] training loss: 575.8741536140442\n",
            "epoch 8 batch 4600 [46000/160000] training loss: 536.0007884502411\n",
            "epoch 8 batch 4700 [47000/160000] training loss: 547.1027021408081\n",
            "epoch 8 batch 4800 [48000/160000] training loss: 566.5460654497147\n",
            "epoch 8 batch 4900 [49000/160000] training loss: 568.3968846797943\n",
            "epoch 8 batch 5000 [50000/160000] training loss: 561.5322828292847\n",
            "epoch 8 batch 5100 [51000/160000] training loss: 557.2632429599762\n",
            "epoch 8 batch 5200 [52000/160000] training loss: 566.8672814369202\n",
            "epoch 8 batch 5300 [53000/160000] training loss: 573.1273195743561\n",
            "epoch 8 batch 5400 [54000/160000] training loss: 563.5832796096802\n",
            "epoch 8 batch 5500 [55000/160000] training loss: 555.1670145988464\n",
            "epoch 8 batch 5600 [56000/160000] training loss: 577.5558753013611\n",
            "epoch 8 batch 5700 [57000/160000] training loss: 573.6074488162994\n",
            "epoch 8 batch 5800 [58000/160000] training loss: 569.1284826993942\n",
            "epoch 8 batch 5900 [59000/160000] training loss: 546.2484498023987\n",
            "epoch 8 batch 6000 [60000/160000] training loss: 587.9174942970276\n",
            "epoch 8 batch 6100 [61000/160000] training loss: 576.1218929290771\n",
            "epoch 8 batch 6200 [62000/160000] training loss: 590.5690469741821\n",
            "epoch 8 batch 6300 [63000/160000] training loss: 557.1660048961639\n",
            "epoch 8 batch 6400 [64000/160000] training loss: 550.802197933197\n",
            "epoch 8 batch 6500 [65000/160000] training loss: 565.9368007183075\n",
            "epoch 8 batch 6600 [66000/160000] training loss: 542.7888653278351\n",
            "epoch 8 batch 6700 [67000/160000] training loss: 583.060839176178\n",
            "epoch 8 batch 6800 [68000/160000] training loss: 542.5632722377777\n",
            "epoch 8 batch 6900 [69000/160000] training loss: 552.6479082107544\n",
            "epoch 8 batch 7000 [70000/160000] training loss: 548.1150991916656\n",
            "epoch 8 batch 7100 [71000/160000] training loss: 534.0038449764252\n",
            "epoch 8 batch 7200 [72000/160000] training loss: 562.0535926818848\n",
            "epoch 8 batch 7300 [73000/160000] training loss: 553.4807868003845\n",
            "epoch 8 batch 7400 [74000/160000] training loss: 550.489471912384\n",
            "epoch 8 batch 7500 [75000/160000] training loss: 562.8919435739517\n",
            "epoch 8 batch 7600 [76000/160000] training loss: 572.812705874443\n",
            "epoch 8 batch 7700 [77000/160000] training loss: 591.0912384986877\n",
            "epoch 8 batch 7800 [78000/160000] training loss: 590.7270784378052\n",
            "epoch 8 batch 7900 [79000/160000] training loss: 556.246416091919\n",
            "epoch 8 batch 8000 [80000/160000] training loss: 558.0932002067566\n",
            "epoch 8 batch 8100 [81000/160000] training loss: 550.503106713295\n",
            "epoch 8 batch 8200 [82000/160000] training loss: 540.3995671272278\n",
            "epoch 8 batch 8300 [83000/160000] training loss: 576.9173393249512\n",
            "epoch 8 batch 8400 [84000/160000] training loss: 575.5427696704865\n",
            "epoch 8 batch 8500 [85000/160000] training loss: 583.3186784982681\n",
            "epoch 8 batch 8600 [86000/160000] training loss: 549.1514408588409\n",
            "epoch 8 batch 8700 [87000/160000] training loss: 560.5483226776123\n",
            "epoch 8 batch 8800 [88000/160000] training loss: 579.9469285011292\n",
            "epoch 8 batch 8900 [89000/160000] training loss: 580.6366943120956\n",
            "epoch 8 batch 9000 [90000/160000] training loss: 586.6188676357269\n",
            "epoch 8 batch 9100 [91000/160000] training loss: 584.4845688343048\n",
            "epoch 8 batch 9200 [92000/160000] training loss: 527.0074481964111\n",
            "epoch 8 batch 9300 [93000/160000] training loss: 579.9174542427063\n",
            "epoch 8 batch 9400 [94000/160000] training loss: 536.3294067382812\n",
            "epoch 8 batch 9500 [95000/160000] training loss: 570.3657469749451\n",
            "epoch 8 batch 9600 [96000/160000] training loss: 558.8731023073196\n",
            "epoch 8 batch 9700 [97000/160000] training loss: 560.240918636322\n",
            "epoch 8 batch 9800 [98000/160000] training loss: 563.1942811012268\n",
            "epoch 8 batch 9900 [99000/160000] training loss: 565.3792264461517\n",
            "epoch 8 batch 10000 [100000/160000] training loss: 595.7938067913055\n",
            "epoch 8 batch 10100 [101000/160000] training loss: 568.649523973465\n",
            "epoch 8 batch 10200 [102000/160000] training loss: 537.8867483139038\n",
            "epoch 8 batch 10300 [103000/160000] training loss: 573.9709920883179\n",
            "epoch 8 batch 10400 [104000/160000] training loss: 569.0878729820251\n",
            "epoch 8 batch 10500 [105000/160000] training loss: 604.4837629795074\n",
            "epoch 8 batch 10600 [106000/160000] training loss: 591.616678237915\n",
            "epoch 8 batch 10700 [107000/160000] training loss: 567.6246347427368\n",
            "epoch 8 batch 10800 [108000/160000] training loss: 547.9687440395355\n",
            "epoch 8 batch 10900 [109000/160000] training loss: 552.0140345096588\n",
            "epoch 8 batch 11000 [110000/160000] training loss: 564.5444519519806\n",
            "epoch 8 batch 11100 [111000/160000] training loss: 556.21906042099\n",
            "epoch 8 batch 11200 [112000/160000] training loss: 571.9370429515839\n",
            "epoch 8 batch 11300 [113000/160000] training loss: 555.3298245668411\n",
            "epoch 8 batch 11400 [114000/160000] training loss: 576.4593795537949\n",
            "epoch 8 batch 11500 [115000/160000] training loss: 572.6406943798065\n",
            "epoch 8 batch 11600 [116000/160000] training loss: 591.6497502326965\n",
            "epoch 8 batch 11700 [117000/160000] training loss: 545.7613167762756\n",
            "epoch 8 batch 11800 [118000/160000] training loss: 557.2577965259552\n",
            "epoch 8 batch 11900 [119000/160000] training loss: 547.6821633577347\n",
            "epoch 8 batch 12000 [120000/160000] training loss: 567.7753537893295\n",
            "epoch 8 batch 12100 [121000/160000] training loss: 592.0478591918945\n",
            "epoch 8 batch 12200 [122000/160000] training loss: 554.032607793808\n",
            "epoch 8 batch 12300 [123000/160000] training loss: 567.4076707363129\n",
            "epoch 8 batch 12400 [124000/160000] training loss: 572.7868659496307\n",
            "epoch 8 batch 12500 [125000/160000] training loss: 556.8793413639069\n",
            "epoch 8 batch 12600 [126000/160000] training loss: 568.8385598659515\n",
            "epoch 8 batch 12700 [127000/160000] training loss: 555.5680220127106\n",
            "epoch 8 batch 12800 [128000/160000] training loss: 578.3208754062653\n",
            "epoch 8 batch 12900 [129000/160000] training loss: 578.3883728981018\n",
            "epoch 8 batch 13000 [130000/160000] training loss: 586.2173516750336\n",
            "epoch 8 batch 13100 [131000/160000] training loss: 559.6203999519348\n",
            "epoch 8 batch 13200 [132000/160000] training loss: 596.2969877719879\n",
            "epoch 8 batch 13300 [133000/160000] training loss: 576.9581055641174\n",
            "epoch 8 batch 13400 [134000/160000] training loss: 548.6584386825562\n",
            "epoch 8 batch 13500 [135000/160000] training loss: 563.6798491477966\n",
            "epoch 8 batch 13600 [136000/160000] training loss: 543.9982030391693\n",
            "epoch 8 batch 13700 [137000/160000] training loss: 597.3717792034149\n",
            "epoch 8 batch 13800 [138000/160000] training loss: 576.2911796569824\n",
            "epoch 8 batch 13900 [139000/160000] training loss: 597.7387120723724\n",
            "epoch 8 batch 14000 [140000/160000] training loss: 573.6362428665161\n",
            "epoch 8 batch 14100 [141000/160000] training loss: 555.5657866001129\n",
            "epoch 8 batch 14200 [142000/160000] training loss: 569.6658806800842\n",
            "epoch 8 batch 14300 [143000/160000] training loss: 571.3566083908081\n",
            "epoch 8 batch 14400 [144000/160000] training loss: 583.9971153736115\n",
            "epoch 8 batch 14500 [145000/160000] training loss: 569.621851682663\n",
            "epoch 8 batch 14600 [146000/160000] training loss: 567.5272679328918\n",
            "epoch 8 batch 14700 [147000/160000] training loss: 559.258704662323\n",
            "epoch 8 batch 14800 [148000/160000] training loss: 570.464721083641\n",
            "epoch 8 batch 14900 [149000/160000] training loss: 562.4894473552704\n",
            "epoch 8 batch 15000 [150000/160000] training loss: 603.7518265247345\n",
            "epoch 8 batch 15100 [151000/160000] training loss: 572.6967122554779\n",
            "epoch 8 batch 15200 [152000/160000] training loss: 604.5927703380585\n",
            "epoch 8 batch 15300 [153000/160000] training loss: 567.3637192249298\n",
            "epoch 8 batch 15400 [154000/160000] training loss: 591.3028094768524\n",
            "epoch 8 batch 15500 [155000/160000] training loss: 559.9237072467804\n",
            "epoch 8 batch 15600 [156000/160000] training loss: 539.4855830669403\n",
            "epoch 8 batch 15700 [157000/160000] training loss: 552.2439804077148\n",
            "epoch 8 batch 15800 [158000/160000] training loss: 561.7048041820526\n",
            "epoch 8 batch 15900 [159000/160000] training loss: 545.3284156322479\n",
            "epoch 9 batch 0 [0/160000] training loss: 5.562337875366211\n",
            "epoch 9 batch 100 [1000/160000] training loss: 571.599884390831\n",
            "epoch 9 batch 200 [2000/160000] training loss: 572.8228390216827\n",
            "epoch 9 batch 300 [3000/160000] training loss: 570.4214537143707\n",
            "epoch 9 batch 400 [4000/160000] training loss: 576.7760558128357\n",
            "epoch 9 batch 500 [5000/160000] training loss: 577.9991807937622\n",
            "epoch 9 batch 600 [6000/160000] training loss: 566.9327709674835\n",
            "epoch 9 batch 700 [7000/160000] training loss: 556.0188460350037\n",
            "epoch 9 batch 800 [8000/160000] training loss: 583.3104456663132\n",
            "epoch 9 batch 900 [9000/160000] training loss: 576.3205137252808\n",
            "epoch 9 batch 1000 [10000/160000] training loss: 579.0075576305389\n",
            "epoch 9 batch 1100 [11000/160000] training loss: 560.1613138914108\n",
            "epoch 9 batch 1200 [12000/160000] training loss: 564.1503355503082\n",
            "epoch 9 batch 1300 [13000/160000] training loss: 595.9212644100189\n",
            "epoch 9 batch 1400 [14000/160000] training loss: 562.7527363300323\n",
            "epoch 9 batch 1500 [15000/160000] training loss: 543.2167375087738\n",
            "epoch 9 batch 1600 [16000/160000] training loss: 579.0297503471375\n",
            "epoch 9 batch 1700 [17000/160000] training loss: 582.1594305038452\n",
            "epoch 9 batch 1800 [18000/160000] training loss: 569.3997766971588\n",
            "epoch 9 batch 1900 [19000/160000] training loss: 564.0323570966721\n",
            "epoch 9 batch 2000 [20000/160000] training loss: 552.5100393295288\n",
            "epoch 9 batch 2100 [21000/160000] training loss: 544.0997755527496\n",
            "epoch 9 batch 2200 [22000/160000] training loss: 589.5103771686554\n",
            "epoch 9 batch 2300 [23000/160000] training loss: 571.6075870990753\n",
            "epoch 9 batch 2400 [24000/160000] training loss: 558.0933575630188\n",
            "epoch 9 batch 2500 [25000/160000] training loss: 594.8400723934174\n",
            "epoch 9 batch 2600 [26000/160000] training loss: 555.2768828868866\n",
            "epoch 9 batch 2700 [27000/160000] training loss: 583.9277918338776\n",
            "epoch 9 batch 2800 [28000/160000] training loss: 535.0465540885925\n",
            "epoch 9 batch 2900 [29000/160000] training loss: 613.987804889679\n",
            "epoch 9 batch 3000 [30000/160000] training loss: 576.6477406024933\n",
            "epoch 9 batch 3100 [31000/160000] training loss: 586.6342878341675\n",
            "epoch 9 batch 3200 [32000/160000] training loss: 563.2414224147797\n",
            "epoch 9 batch 3300 [33000/160000] training loss: 575.323644399643\n",
            "epoch 9 batch 3400 [34000/160000] training loss: 569.6460273265839\n",
            "epoch 9 batch 3500 [35000/160000] training loss: 582.6744420528412\n",
            "epoch 9 batch 3600 [36000/160000] training loss: 566.5828886032104\n",
            "epoch 9 batch 3700 [37000/160000] training loss: 549.0828415155411\n",
            "epoch 9 batch 3800 [38000/160000] training loss: 564.9565224647522\n",
            "epoch 9 batch 3900 [39000/160000] training loss: 572.1694197654724\n",
            "epoch 9 batch 4000 [40000/160000] training loss: 563.4565541744232\n",
            "epoch 9 batch 4100 [41000/160000] training loss: 573.8778924942017\n",
            "epoch 9 batch 4200 [42000/160000] training loss: 594.4439611434937\n",
            "epoch 9 batch 4300 [43000/160000] training loss: 575.0464636087418\n",
            "epoch 9 batch 4400 [44000/160000] training loss: 554.6846537590027\n",
            "epoch 9 batch 4500 [45000/160000] training loss: 575.7652866840363\n",
            "epoch 9 batch 4600 [46000/160000] training loss: 535.6489589214325\n",
            "epoch 9 batch 4700 [47000/160000] training loss: 546.7258925437927\n",
            "epoch 9 batch 4800 [48000/160000] training loss: 566.039421081543\n",
            "epoch 9 batch 4900 [49000/160000] training loss: 568.0254712104797\n",
            "epoch 9 batch 5000 [50000/160000] training loss: 561.0139784812927\n",
            "epoch 9 batch 5100 [51000/160000] training loss: 556.5953185558319\n",
            "epoch 9 batch 5200 [52000/160000] training loss: 566.7560007572174\n",
            "epoch 9 batch 5300 [53000/160000] training loss: 571.5575737953186\n",
            "epoch 9 batch 5400 [54000/160000] training loss: 564.1621997356415\n",
            "epoch 9 batch 5500 [55000/160000] training loss: 555.1730546951294\n",
            "epoch 9 batch 5600 [56000/160000] training loss: 577.0994560718536\n",
            "epoch 9 batch 5700 [57000/160000] training loss: 572.5022192001343\n",
            "epoch 9 batch 5800 [58000/160000] training loss: 568.6465257406235\n",
            "epoch 9 batch 5900 [59000/160000] training loss: 545.6727070808411\n",
            "epoch 9 batch 6000 [60000/160000] training loss: 587.1296129226685\n",
            "epoch 9 batch 6100 [61000/160000] training loss: 574.725555896759\n",
            "epoch 9 batch 6200 [62000/160000] training loss: 590.2777433395386\n",
            "epoch 9 batch 6300 [63000/160000] training loss: 557.5419261455536\n",
            "epoch 9 batch 6400 [64000/160000] training loss: 550.0279502868652\n",
            "epoch 9 batch 6500 [65000/160000] training loss: 564.2750661373138\n",
            "epoch 9 batch 6600 [66000/160000] training loss: 541.4053795337677\n",
            "epoch 9 batch 6700 [67000/160000] training loss: 582.9160640239716\n",
            "epoch 9 batch 6800 [68000/160000] training loss: 541.9510359764099\n",
            "epoch 9 batch 6900 [69000/160000] training loss: 551.6079859733582\n",
            "epoch 9 batch 7000 [70000/160000] training loss: 546.7102060317993\n",
            "epoch 9 batch 7100 [71000/160000] training loss: 533.1235542297363\n",
            "epoch 9 batch 7200 [72000/160000] training loss: 561.8536431789398\n",
            "epoch 9 batch 7300 [73000/160000] training loss: 552.6450672149658\n",
            "epoch 9 batch 7400 [74000/160000] training loss: 548.774664402008\n",
            "epoch 9 batch 7500 [75000/160000] training loss: 562.0535887479782\n",
            "epoch 9 batch 7600 [76000/160000] training loss: 571.8339992761612\n",
            "epoch 9 batch 7700 [77000/160000] training loss: 590.1631481647491\n",
            "epoch 9 batch 7800 [78000/160000] training loss: 590.3608644008636\n",
            "epoch 9 batch 7900 [79000/160000] training loss: 555.6851661205292\n",
            "epoch 9 batch 8000 [80000/160000] training loss: 557.220865726471\n",
            "epoch 9 batch 8100 [81000/160000] training loss: 549.2087932825089\n",
            "epoch 9 batch 8200 [82000/160000] training loss: 539.8557891845703\n",
            "epoch 9 batch 8300 [83000/160000] training loss: 576.8356046676636\n",
            "epoch 9 batch 8400 [84000/160000] training loss: 574.9967532157898\n",
            "epoch 9 batch 8500 [85000/160000] training loss: 582.8636854887009\n",
            "epoch 9 batch 8600 [86000/160000] training loss: 548.7716548442841\n",
            "epoch 9 batch 8700 [87000/160000] training loss: 559.9446458816528\n",
            "epoch 9 batch 8800 [88000/160000] training loss: 579.0453777313232\n",
            "epoch 9 batch 8900 [89000/160000] training loss: 579.0211726427078\n",
            "epoch 9 batch 9000 [90000/160000] training loss: 586.499413728714\n",
            "epoch 9 batch 9100 [91000/160000] training loss: 584.2666394710541\n",
            "epoch 9 batch 9200 [92000/160000] training loss: 526.0090252161026\n",
            "epoch 9 batch 9300 [93000/160000] training loss: 579.5105514526367\n",
            "epoch 9 batch 9400 [94000/160000] training loss: 535.1569223403931\n",
            "epoch 9 batch 9500 [95000/160000] training loss: 570.5229448080063\n",
            "epoch 9 batch 9600 [96000/160000] training loss: 557.8428760766983\n",
            "epoch 9 batch 9700 [97000/160000] training loss: 558.9400173425674\n",
            "epoch 9 batch 9800 [98000/160000] training loss: 562.1725280284882\n",
            "epoch 9 batch 9900 [99000/160000] training loss: 565.2330462932587\n",
            "epoch 9 batch 10000 [100000/160000] training loss: 594.9801598787308\n",
            "epoch 9 batch 10100 [101000/160000] training loss: 568.6411837339401\n",
            "epoch 9 batch 10200 [102000/160000] training loss: 537.871773481369\n",
            "epoch 9 batch 10300 [103000/160000] training loss: 573.2229993343353\n",
            "epoch 9 batch 10400 [104000/160000] training loss: 569.1167627573013\n",
            "epoch 9 batch 10500 [105000/160000] training loss: 604.0412268638611\n",
            "epoch 9 batch 10600 [106000/160000] training loss: 590.9060342311859\n",
            "epoch 9 batch 10700 [107000/160000] training loss: 566.412925362587\n",
            "epoch 9 batch 10800 [108000/160000] training loss: 547.2721673250198\n",
            "epoch 9 batch 10900 [109000/160000] training loss: 550.7167316675186\n",
            "epoch 9 batch 11000 [110000/160000] training loss: 564.8316345214844\n",
            "epoch 9 batch 11100 [111000/160000] training loss: 555.326397895813\n",
            "epoch 9 batch 11200 [112000/160000] training loss: 571.3667349815369\n",
            "epoch 9 batch 11300 [113000/160000] training loss: 554.9566646814346\n",
            "epoch 9 batch 11400 [114000/160000] training loss: 574.7436277866364\n",
            "epoch 9 batch 11500 [115000/160000] training loss: 572.3198320865631\n",
            "epoch 9 batch 11600 [116000/160000] training loss: 591.7941362857819\n",
            "epoch 9 batch 11700 [117000/160000] training loss: 545.2086381912231\n",
            "epoch 9 batch 11800 [118000/160000] training loss: 556.2904113531113\n",
            "epoch 9 batch 11900 [119000/160000] training loss: 547.5615749359131\n",
            "epoch 9 batch 12000 [120000/160000] training loss: 567.5361174345016\n",
            "epoch 9 batch 12100 [121000/160000] training loss: 591.6804163455963\n",
            "epoch 9 batch 12200 [122000/160000] training loss: 554.0667767524719\n",
            "epoch 9 batch 12300 [123000/160000] training loss: 566.820175409317\n",
            "epoch 9 batch 12400 [124000/160000] training loss: 572.1293039321899\n",
            "epoch 9 batch 12500 [125000/160000] training loss: 556.2126693725586\n",
            "epoch 9 batch 12600 [126000/160000] training loss: 568.7077807188034\n",
            "epoch 9 batch 12700 [127000/160000] training loss: 554.8484904766083\n",
            "epoch 9 batch 12800 [128000/160000] training loss: 578.046966791153\n",
            "epoch 9 batch 12900 [129000/160000] training loss: 578.1802039146423\n",
            "epoch 9 batch 13000 [130000/160000] training loss: 585.9164345264435\n",
            "epoch 9 batch 13100 [131000/160000] training loss: 560.0404784679413\n",
            "epoch 9 batch 13200 [132000/160000] training loss: 595.3291742801666\n",
            "epoch 9 batch 13300 [133000/160000] training loss: 575.9173927307129\n",
            "epoch 9 batch 13400 [134000/160000] training loss: 548.5150871276855\n",
            "epoch 9 batch 13500 [135000/160000] training loss: 563.6485276222229\n",
            "epoch 9 batch 13600 [136000/160000] training loss: 543.2431282997131\n",
            "epoch 9 batch 13700 [137000/160000] training loss: 597.4708893299103\n",
            "epoch 9 batch 13800 [138000/160000] training loss: 577.0401182174683\n",
            "epoch 9 batch 13900 [139000/160000] training loss: 597.9040114879608\n",
            "epoch 9 batch 14000 [140000/160000] training loss: 572.6246333122253\n",
            "epoch 9 batch 14100 [141000/160000] training loss: 554.0468969345093\n",
            "epoch 9 batch 14200 [142000/160000] training loss: 569.0310049057007\n",
            "epoch 9 batch 14300 [143000/160000] training loss: 570.1641616821289\n",
            "epoch 9 batch 14400 [144000/160000] training loss: 583.40638422966\n",
            "epoch 9 batch 14500 [145000/160000] training loss: 568.9241738319397\n",
            "epoch 9 batch 14600 [146000/160000] training loss: 566.117751121521\n",
            "epoch 9 batch 14700 [147000/160000] training loss: 558.1330904960632\n",
            "epoch 9 batch 14800 [148000/160000] training loss: 569.8564068078995\n",
            "epoch 9 batch 14900 [149000/160000] training loss: 561.0822401046753\n",
            "epoch 9 batch 15000 [150000/160000] training loss: 602.690690279007\n",
            "epoch 9 batch 15100 [151000/160000] training loss: 572.1001374721527\n",
            "epoch 9 batch 15200 [152000/160000] training loss: 603.7170369625092\n",
            "epoch 9 batch 15300 [153000/160000] training loss: 566.7264862060547\n",
            "epoch 9 batch 15400 [154000/160000] training loss: 590.6014988422394\n",
            "epoch 9 batch 15500 [155000/160000] training loss: 558.8698453903198\n",
            "epoch 9 batch 15600 [156000/160000] training loss: 538.8338532447815\n",
            "epoch 9 batch 15700 [157000/160000] training loss: 551.1952316761017\n",
            "epoch 9 batch 15800 [158000/160000] training loss: 561.3245441913605\n",
            "epoch 9 batch 15900 [159000/160000] training loss: 545.0049376487732\n",
            "epoch 10 batch 0 [0/160000] training loss: 5.534981727600098\n",
            "epoch 10 batch 100 [1000/160000] training loss: 571.4107638597488\n",
            "epoch 10 batch 200 [2000/160000] training loss: 572.9632334709167\n",
            "epoch 10 batch 300 [3000/160000] training loss: 570.1951546669006\n",
            "epoch 10 batch 400 [4000/160000] training loss: 576.2170538902283\n",
            "epoch 10 batch 500 [5000/160000] training loss: 578.4005599021912\n",
            "epoch 10 batch 600 [6000/160000] training loss: 566.8514813184738\n",
            "epoch 10 batch 700 [7000/160000] training loss: 555.4578491449356\n",
            "epoch 10 batch 800 [8000/160000] training loss: 583.5741641521454\n",
            "epoch 10 batch 900 [9000/160000] training loss: 575.4701972007751\n",
            "epoch 10 batch 1000 [10000/160000] training loss: 579.0547118186951\n",
            "epoch 10 batch 1100 [11000/160000] training loss: 559.4364522695541\n",
            "epoch 10 batch 1200 [12000/160000] training loss: 562.3294394016266\n",
            "epoch 10 batch 1300 [13000/160000] training loss: 595.5977245569229\n",
            "epoch 10 batch 1400 [14000/160000] training loss: 562.7009547948837\n",
            "epoch 10 batch 1500 [15000/160000] training loss: 542.4462821483612\n",
            "epoch 10 batch 1600 [16000/160000] training loss: 578.2187268733978\n",
            "epoch 10 batch 1700 [17000/160000] training loss: 581.4184997081757\n",
            "epoch 10 batch 1800 [18000/160000] training loss: 568.8013851642609\n",
            "epoch 10 batch 1900 [19000/160000] training loss: 563.1309065818787\n",
            "epoch 10 batch 2000 [20000/160000] training loss: 551.1156916618347\n",
            "epoch 10 batch 2100 [21000/160000] training loss: 544.2931432723999\n",
            "epoch 10 batch 2200 [22000/160000] training loss: 589.4364067316055\n",
            "epoch 10 batch 2300 [23000/160000] training loss: 571.3761630058289\n",
            "epoch 10 batch 2400 [24000/160000] training loss: 557.4877874851227\n",
            "epoch 10 batch 2500 [25000/160000] training loss: 593.9425659179688\n",
            "epoch 10 batch 2600 [26000/160000] training loss: 555.0119173526764\n",
            "epoch 10 batch 2700 [27000/160000] training loss: 583.5964121818542\n",
            "epoch 10 batch 2800 [28000/160000] training loss: 534.1826092004776\n",
            "epoch 10 batch 2900 [29000/160000] training loss: 613.4275076389313\n",
            "epoch 10 batch 3000 [30000/160000] training loss: 576.8972067832947\n",
            "epoch 10 batch 3100 [31000/160000] training loss: 586.198573589325\n",
            "epoch 10 batch 3200 [32000/160000] training loss: 561.8459861278534\n",
            "epoch 10 batch 3300 [33000/160000] training loss: 574.6405091285706\n",
            "epoch 10 batch 3400 [34000/160000] training loss: 569.9420688152313\n",
            "epoch 10 batch 3500 [35000/160000] training loss: 581.8769444227219\n",
            "epoch 10 batch 3600 [36000/160000] training loss: 566.4063696861267\n",
            "epoch 10 batch 3700 [37000/160000] training loss: 548.4773104190826\n",
            "epoch 10 batch 3800 [38000/160000] training loss: 564.2685253620148\n",
            "epoch 10 batch 3900 [39000/160000] training loss: 572.3417685031891\n",
            "epoch 10 batch 4000 [40000/160000] training loss: 563.2384023666382\n",
            "epoch 10 batch 4100 [41000/160000] training loss: 574.2102041244507\n",
            "epoch 10 batch 4200 [42000/160000] training loss: 593.819554567337\n",
            "epoch 10 batch 4300 [43000/160000] training loss: 574.6806080341339\n",
            "epoch 10 batch 4400 [44000/160000] training loss: 554.2924798727036\n",
            "epoch 10 batch 4500 [45000/160000] training loss: 575.773928642273\n",
            "epoch 10 batch 4600 [46000/160000] training loss: 535.4291079044342\n",
            "epoch 10 batch 4700 [47000/160000] training loss: 546.3849439620972\n",
            "epoch 10 batch 4800 [48000/160000] training loss: 565.6360763311386\n",
            "epoch 10 batch 4900 [49000/160000] training loss: 567.8206343650818\n",
            "epoch 10 batch 5000 [50000/160000] training loss: 560.5742828845978\n",
            "epoch 10 batch 5100 [51000/160000] training loss: 555.9862017631531\n",
            "epoch 10 batch 5200 [52000/160000] training loss: 566.7432963848114\n",
            "epoch 10 batch 5300 [53000/160000] training loss: 570.3382102251053\n",
            "epoch 10 batch 5400 [54000/160000] training loss: 564.7473242282867\n",
            "epoch 10 batch 5500 [55000/160000] training loss: 555.2218301296234\n",
            "epoch 10 batch 5600 [56000/160000] training loss: 576.7255380153656\n",
            "epoch 10 batch 5700 [57000/160000] training loss: 571.5529868602753\n",
            "epoch 10 batch 5800 [58000/160000] training loss: 568.2868245840073\n",
            "epoch 10 batch 5900 [59000/160000] training loss: 545.1571009159088\n",
            "epoch 10 batch 6000 [60000/160000] training loss: 586.4641411304474\n",
            "epoch 10 batch 6100 [61000/160000] training loss: 573.4466512203217\n",
            "epoch 10 batch 6200 [62000/160000] training loss: 590.1228523254395\n",
            "epoch 10 batch 6300 [63000/160000] training loss: 557.9898450374603\n",
            "epoch 10 batch 6400 [64000/160000] training loss: 549.3525953292847\n",
            "epoch 10 batch 6500 [65000/160000] training loss: 562.8837368488312\n",
            "epoch 10 batch 6600 [66000/160000] training loss: 540.311733007431\n",
            "epoch 10 batch 6700 [67000/160000] training loss: 582.8265619277954\n",
            "epoch 10 batch 6800 [68000/160000] training loss: 541.3227651119232\n",
            "epoch 10 batch 6900 [69000/160000] training loss: 550.807694196701\n",
            "epoch 10 batch 7000 [70000/160000] training loss: 545.4878149032593\n",
            "epoch 10 batch 7100 [71000/160000] training loss: 532.4897946119308\n",
            "epoch 10 batch 7200 [72000/160000] training loss: 561.7603809833527\n",
            "epoch 10 batch 7300 [73000/160000] training loss: 551.9642963409424\n",
            "epoch 10 batch 7400 [74000/160000] training loss: 547.2513954639435\n",
            "epoch 10 batch 7500 [75000/160000] training loss: 561.2227576971054\n",
            "epoch 10 batch 7600 [76000/160000] training loss: 570.9865292310715\n",
            "epoch 10 batch 7700 [77000/160000] training loss: 589.4400568008423\n",
            "epoch 10 batch 7800 [78000/160000] training loss: 590.0096659660339\n",
            "epoch 10 batch 7900 [79000/160000] training loss: 555.2191370725632\n",
            "epoch 10 batch 8000 [80000/160000] training loss: 556.4886865615845\n",
            "epoch 10 batch 8100 [81000/160000] training loss: 548.0949695110321\n",
            "epoch 10 batch 8200 [82000/160000] training loss: 539.4458594322205\n",
            "epoch 10 batch 8300 [83000/160000] training loss: 576.8436675071716\n",
            "epoch 10 batch 8400 [84000/160000] training loss: 574.5517318248749\n",
            "epoch 10 batch 8500 [85000/160000] training loss: 582.5534192323685\n",
            "epoch 10 batch 8600 [86000/160000] training loss: 548.560026884079\n",
            "epoch 10 batch 8700 [87000/160000] training loss: 559.5106928348541\n",
            "epoch 10 batch 8800 [88000/160000] training loss: 578.2397899627686\n",
            "epoch 10 batch 8900 [89000/160000] training loss: 577.5420151948929\n",
            "epoch 10 batch 9000 [90000/160000] training loss: 586.4809091091156\n",
            "epoch 10 batch 9100 [91000/160000] training loss: 584.1589584350586\n",
            "epoch 10 batch 9200 [92000/160000] training loss: 525.2826166152954\n",
            "epoch 10 batch 9300 [93000/160000] training loss: 579.1821825504303\n",
            "epoch 10 batch 9400 [94000/160000] training loss: 534.2293655872345\n",
            "epoch 10 batch 9500 [95000/160000] training loss: 570.7630517482758\n",
            "epoch 10 batch 9600 [96000/160000] training loss: 556.9984809160233\n",
            "epoch 10 batch 9700 [97000/160000] training loss: 557.7958760261536\n",
            "epoch 10 batch 9800 [98000/160000] training loss: 561.219732761383\n",
            "epoch 10 batch 9900 [99000/160000] training loss: 565.1716814041138\n",
            "epoch 10 batch 10000 [100000/160000] training loss: 594.1832685470581\n",
            "epoch 10 batch 10100 [101000/160000] training loss: 568.6973382234573\n",
            "epoch 10 batch 10200 [102000/160000] training loss: 537.9767549037933\n",
            "epoch 10 batch 10300 [103000/160000] training loss: 572.5557525157928\n",
            "epoch 10 batch 10400 [104000/160000] training loss: 569.2048487663269\n",
            "epoch 10 batch 10500 [105000/160000] training loss: 603.6792786121368\n",
            "epoch 10 batch 10600 [106000/160000] training loss: 590.3302187919617\n",
            "epoch 10 batch 10700 [107000/160000] training loss: 565.366157412529\n",
            "epoch 10 batch 10800 [108000/160000] training loss: 546.7524334192276\n",
            "epoch 10 batch 10900 [109000/160000] training loss: 549.5767585039139\n",
            "epoch 10 batch 11000 [110000/160000] training loss: 565.0298004150391\n",
            "epoch 10 batch 11100 [111000/160000] training loss: 554.635458946228\n",
            "epoch 10 batch 11200 [112000/160000] training loss: 570.8726353645325\n",
            "epoch 10 batch 11300 [113000/160000] training loss: 554.714120388031\n",
            "epoch 10 batch 11400 [114000/160000] training loss: 573.3228752613068\n",
            "epoch 10 batch 11500 [115000/160000] training loss: 572.085066318512\n",
            "epoch 10 batch 11600 [116000/160000] training loss: 591.8889403343201\n",
            "epoch 10 batch 11700 [117000/160000] training loss: 544.6994066238403\n",
            "epoch 10 batch 11800 [118000/160000] training loss: 555.4754730463028\n",
            "epoch 10 batch 11900 [119000/160000] training loss: 547.5022288560867\n",
            "epoch 10 batch 12000 [120000/160000] training loss: 567.3203023672104\n",
            "epoch 10 batch 12100 [121000/160000] training loss: 591.249837398529\n",
            "epoch 10 batch 12200 [122000/160000] training loss: 554.1870925426483\n",
            "epoch 10 batch 12300 [123000/160000] training loss: 566.3071584701538\n",
            "epoch 10 batch 12400 [124000/160000] training loss: 571.6548767089844\n",
            "epoch 10 batch 12500 [125000/160000] training loss: 555.6499602794647\n",
            "epoch 10 batch 12600 [126000/160000] training loss: 568.6181447505951\n",
            "epoch 10 batch 12700 [127000/160000] training loss: 554.2099932432175\n",
            "epoch 10 batch 12800 [128000/160000] training loss: 577.9286088943481\n",
            "epoch 10 batch 12900 [129000/160000] training loss: 577.9881784915924\n",
            "epoch 10 batch 13000 [130000/160000] training loss: 585.7106132507324\n",
            "epoch 10 batch 13100 [131000/160000] training loss: 560.5491924285889\n",
            "epoch 10 batch 13200 [132000/160000] training loss: 594.5827016830444\n",
            "epoch 10 batch 13300 [133000/160000] training loss: 575.037059545517\n",
            "epoch 10 batch 13400 [134000/160000] training loss: 548.3747162818909\n",
            "epoch 10 batch 13500 [135000/160000] training loss: 563.6492331027985\n",
            "epoch 10 batch 13600 [136000/160000] training loss: 542.648199558258\n",
            "epoch 10 batch 13700 [137000/160000] training loss: 597.6388523578644\n",
            "epoch 10 batch 13800 [138000/160000] training loss: 577.7640988826752\n",
            "epoch 10 batch 13900 [139000/160000] training loss: 598.0857186317444\n",
            "epoch 10 batch 14000 [140000/160000] training loss: 571.8379399776459\n",
            "epoch 10 batch 14100 [141000/160000] training loss: 552.6713426113129\n",
            "epoch 10 batch 14200 [142000/160000] training loss: 568.5045409202576\n",
            "epoch 10 batch 14300 [143000/160000] training loss: 569.1242847442627\n",
            "epoch 10 batch 14400 [144000/160000] training loss: 582.9320826530457\n",
            "epoch 10 batch 14500 [145000/160000] training loss: 568.3309369087219\n",
            "epoch 10 batch 14600 [146000/160000] training loss: 564.9398736953735\n",
            "epoch 10 batch 14700 [147000/160000] training loss: 557.1955587863922\n",
            "epoch 10 batch 14800 [148000/160000] training loss: 569.3237452507019\n",
            "epoch 10 batch 14900 [149000/160000] training loss: 559.8708736896515\n",
            "epoch 10 batch 15000 [150000/160000] training loss: 601.7878184318542\n",
            "epoch 10 batch 15100 [151000/160000] training loss: 571.6199977397919\n",
            "epoch 10 batch 15200 [152000/160000] training loss: 602.972861289978\n",
            "epoch 10 batch 15300 [153000/160000] training loss: 566.194100856781\n",
            "epoch 10 batch 15400 [154000/160000] training loss: 589.9658343791962\n",
            "epoch 10 batch 15500 [155000/160000] training loss: 557.9956467151642\n",
            "epoch 10 batch 15600 [156000/160000] training loss: 538.3610517978668\n",
            "epoch 10 batch 15700 [157000/160000] training loss: 550.2605118751526\n",
            "epoch 10 batch 15800 [158000/160000] training loss: 561.010307431221\n",
            "epoch 10 batch 15900 [159000/160000] training loss: 544.8780388832092\n",
            "epoch 11 batch 0 [0/160000] training loss: 5.516384124755859\n",
            "epoch 11 batch 100 [1000/160000] training loss: 571.2839093208313\n",
            "epoch 11 batch 200 [2000/160000] training loss: 573.0991337299347\n",
            "epoch 11 batch 300 [3000/160000] training loss: 570.0105180740356\n",
            "epoch 11 batch 400 [4000/160000] training loss: 575.7770392894745\n",
            "epoch 11 batch 500 [5000/160000] training loss: 578.7111673355103\n",
            "epoch 11 batch 600 [6000/160000] training loss: 566.8103535175323\n",
            "epoch 11 batch 700 [7000/160000] training loss: 554.8825652599335\n",
            "epoch 11 batch 800 [8000/160000] training loss: 583.9611529111862\n",
            "epoch 11 batch 900 [9000/160000] training loss: 574.7147769927979\n",
            "epoch 11 batch 1000 [10000/160000] training loss: 579.169581413269\n",
            "epoch 11 batch 1100 [11000/160000] training loss: 558.8290865421295\n",
            "epoch 11 batch 1200 [12000/160000] training loss: 560.7904000282288\n",
            "epoch 11 batch 1300 [13000/160000] training loss: 595.3235595226288\n",
            "epoch 11 batch 1400 [14000/160000] training loss: 562.7277874946594\n",
            "epoch 11 batch 1500 [15000/160000] training loss: 541.7932934761047\n",
            "epoch 11 batch 1600 [16000/160000] training loss: 577.4963622093201\n",
            "epoch 11 batch 1700 [17000/160000] training loss: 580.7240138053894\n",
            "epoch 11 batch 1800 [18000/160000] training loss: 568.2972812652588\n",
            "epoch 11 batch 1900 [19000/160000] training loss: 562.384191274643\n",
            "epoch 11 batch 2000 [20000/160000] training loss: 549.9474420547485\n",
            "epoch 11 batch 2100 [21000/160000] training loss: 544.4630539417267\n",
            "epoch 11 batch 2200 [22000/160000] training loss: 589.3916329145432\n",
            "epoch 11 batch 2300 [23000/160000] training loss: 571.1785147190094\n",
            "epoch 11 batch 2400 [24000/160000] training loss: 557.0212631225586\n",
            "epoch 11 batch 2500 [25000/160000] training loss: 593.1830105781555\n",
            "epoch 11 batch 2600 [26000/160000] training loss: 554.7919597625732\n",
            "epoch 11 batch 2700 [27000/160000] training loss: 583.385106086731\n",
            "epoch 11 batch 2800 [28000/160000] training loss: 533.4395086765289\n",
            "epoch 11 batch 2900 [29000/160000] training loss: 612.8414571285248\n",
            "epoch 11 batch 3000 [30000/160000] training loss: 577.1300573348999\n",
            "epoch 11 batch 3100 [31000/160000] training loss: 585.8225173950195\n",
            "epoch 11 batch 3200 [32000/160000] training loss: 560.5944111347198\n",
            "epoch 11 batch 3300 [33000/160000] training loss: 574.0470495223999\n",
            "epoch 11 batch 3400 [34000/160000] training loss: 570.2100310325623\n",
            "epoch 11 batch 3500 [35000/160000] training loss: 581.1531783342361\n",
            "epoch 11 batch 3600 [36000/160000] training loss: 566.3167035579681\n",
            "epoch 11 batch 3700 [37000/160000] training loss: 547.9932963848114\n",
            "epoch 11 batch 3800 [38000/160000] training loss: 563.7498428821564\n",
            "epoch 11 batch 3900 [39000/160000] training loss: 572.5690517425537\n",
            "epoch 11 batch 4000 [40000/160000] training loss: 563.1030764579773\n",
            "epoch 11 batch 4100 [41000/160000] training loss: 574.4376044273376\n",
            "epoch 11 batch 4200 [42000/160000] training loss: 593.3290693759918\n",
            "epoch 11 batch 4300 [43000/160000] training loss: 574.2905426025391\n",
            "epoch 11 batch 4400 [44000/160000] training loss: 553.9814344644547\n",
            "epoch 11 batch 4500 [45000/160000] training loss: 575.8742849826813\n",
            "epoch 11 batch 4600 [46000/160000] training loss: 535.283519744873\n",
            "epoch 11 batch 4700 [47000/160000] training loss: 546.0795135498047\n",
            "epoch 11 batch 4800 [48000/160000] training loss: 565.3204251527786\n",
            "epoch 11 batch 4900 [49000/160000] training loss: 567.7235856056213\n",
            "epoch 11 batch 5000 [50000/160000] training loss: 560.206485748291\n",
            "epoch 11 batch 5100 [51000/160000] training loss: 555.4443397521973\n",
            "epoch 11 batch 5200 [52000/160000] training loss: 566.7881455421448\n",
            "epoch 11 batch 5300 [53000/160000] training loss: 569.3873959779739\n",
            "epoch 11 batch 5400 [54000/160000] training loss: 565.325700044632\n",
            "epoch 11 batch 5500 [55000/160000] training loss: 555.2812571525574\n",
            "epoch 11 batch 5600 [56000/160000] training loss: 576.4156980514526\n",
            "epoch 11 batch 5700 [57000/160000] training loss: 570.7472474575043\n",
            "epoch 11 batch 5800 [58000/160000] training loss: 568.0412248373032\n",
            "epoch 11 batch 5900 [59000/160000] training loss: 544.6851046085358\n",
            "epoch 11 batch 6000 [60000/160000] training loss: 585.9074909687042\n",
            "epoch 11 batch 6100 [61000/160000] training loss: 572.2647850513458\n",
            "epoch 11 batch 6200 [62000/160000] training loss: 590.0700943470001\n",
            "epoch 11 batch 6300 [63000/160000] training loss: 558.4830074310303\n",
            "epoch 11 batch 6400 [64000/160000] training loss: 548.7736420631409\n",
            "epoch 11 batch 6500 [65000/160000] training loss: 561.7228507995605\n",
            "epoch 11 batch 6600 [66000/160000] training loss: 539.4279744625092\n",
            "epoch 11 batch 6700 [67000/160000] training loss: 582.7733902931213\n",
            "epoch 11 batch 6800 [68000/160000] training loss: 540.6770899295807\n",
            "epoch 11 batch 6900 [69000/160000] training loss: 550.205540895462\n",
            "epoch 11 batch 7000 [70000/160000] training loss: 544.3934895992279\n",
            "epoch 11 batch 7100 [71000/160000] training loss: 532.0374902486801\n",
            "epoch 11 batch 7200 [72000/160000] training loss: 561.7247054576874\n",
            "epoch 11 batch 7300 [73000/160000] training loss: 551.4029664993286\n",
            "epoch 11 batch 7400 [74000/160000] training loss: 545.8996860980988\n",
            "epoch 11 batch 7500 [75000/160000] training loss: 560.4221014976501\n",
            "epoch 11 batch 7600 [76000/160000] training loss: 570.2321984767914\n",
            "epoch 11 batch 7700 [77000/160000] training loss: 588.8932437896729\n",
            "epoch 11 batch 7800 [78000/160000] training loss: 589.6551463603973\n",
            "epoch 11 batch 7900 [79000/160000] training loss: 554.8312921524048\n",
            "epoch 11 batch 8000 [80000/160000] training loss: 555.8751275539398\n",
            "epoch 11 batch 8100 [81000/160000] training loss: 547.1342426538467\n",
            "epoch 11 batch 8200 [82000/160000] training loss: 539.122341632843\n",
            "epoch 11 batch 8300 [83000/160000] training loss: 576.9063911437988\n",
            "epoch 11 batch 8400 [84000/160000] training loss: 574.1731910705566\n",
            "epoch 11 batch 8500 [85000/160000] training loss: 582.3568341732025\n",
            "epoch 11 batch 8600 [86000/160000] training loss: 548.4665455818176\n",
            "epoch 11 batch 8700 [87000/160000] training loss: 559.1914958953857\n",
            "epoch 11 batch 8800 [88000/160000] training loss: 577.5232656002045\n",
            "epoch 11 batch 8900 [89000/160000] training loss: 576.1697038412094\n",
            "epoch 11 batch 9000 [90000/160000] training loss: 586.5173387527466\n",
            "epoch 11 batch 9100 [91000/160000] training loss: 584.1340029239655\n",
            "epoch 11 batch 9200 [92000/160000] training loss: 524.7464911937714\n",
            "epoch 11 batch 9300 [93000/160000] training loss: 578.9304037094116\n",
            "epoch 11 batch 9400 [94000/160000] training loss: 533.4993579387665\n",
            "epoch 11 batch 9500 [95000/160000] training loss: 571.0537108182907\n",
            "epoch 11 batch 9600 [96000/160000] training loss: 556.3104596138\n",
            "epoch 11 batch 9700 [97000/160000] training loss: 556.7852654457092\n",
            "epoch 11 batch 9800 [98000/160000] training loss: 560.3274841308594\n",
            "epoch 11 batch 9900 [99000/160000] training loss: 565.1594042778015\n",
            "epoch 11 batch 10000 [100000/160000] training loss: 593.4012064933777\n",
            "epoch 11 batch 10100 [101000/160000] training loss: 568.7913718223572\n",
            "epoch 11 batch 10200 [102000/160000] training loss: 538.1622316837311\n",
            "epoch 11 batch 10300 [103000/160000] training loss: 571.9541301727295\n",
            "epoch 11 batch 10400 [104000/160000] training loss: 569.3199729919434\n",
            "epoch 11 batch 10500 [105000/160000] training loss: 603.3657031059265\n",
            "epoch 11 batch 10600 [106000/160000] training loss: 589.8551470041275\n",
            "epoch 11 batch 10700 [107000/160000] training loss: 564.4461282491684\n",
            "epoch 11 batch 10800 [108000/160000] training loss: 546.3439266681671\n",
            "epoch 11 batch 10900 [109000/160000] training loss: 548.5609179735184\n",
            "epoch 11 batch 11000 [110000/160000] training loss: 565.1561290025711\n",
            "epoch 11 batch 11100 [111000/160000] training loss: 554.0712158679962\n",
            "epoch 11 batch 11200 [112000/160000] training loss: 570.4378027915955\n",
            "epoch 11 batch 11300 [113000/160000] training loss: 554.5775707960129\n",
            "epoch 11 batch 11400 [114000/160000] training loss: 572.1540734767914\n",
            "epoch 11 batch 11500 [115000/160000] training loss: 571.918657541275\n",
            "epoch 11 batch 11600 [116000/160000] training loss: 591.9502038955688\n",
            "epoch 11 batch 11700 [117000/160000] training loss: 544.2174339294434\n",
            "epoch 11 batch 11800 [118000/160000] training loss: 554.7832170724869\n",
            "epoch 11 batch 11900 [119000/160000] training loss: 547.4703732728958\n",
            "epoch 11 batch 12000 [120000/160000] training loss: 567.1234067678452\n",
            "epoch 11 batch 12100 [121000/160000] training loss: 590.7809913158417\n",
            "epoch 11 batch 12200 [122000/160000] training loss: 554.3609323501587\n",
            "epoch 11 batch 12300 [123000/160000] training loss: 565.8582434654236\n",
            "epoch 11 batch 12400 [124000/160000] training loss: 571.3209855556488\n",
            "epoch 11 batch 12500 [125000/160000] training loss: 555.1619997024536\n",
            "epoch 11 batch 12600 [126000/160000] training loss: 568.5606840848923\n",
            "epoch 11 batch 12700 [127000/160000] training loss: 553.6239441633224\n",
            "epoch 11 batch 12800 [128000/160000] training loss: 577.9223916530609\n",
            "epoch 11 batch 12900 [129000/160000] training loss: 577.7942469120026\n",
            "epoch 11 batch 13000 [130000/160000] training loss: 585.557736158371\n",
            "epoch 11 batch 13100 [131000/160000] training loss: 561.1052551269531\n",
            "epoch 11 batch 13200 [132000/160000] training loss: 594.0048331022263\n",
            "epoch 11 batch 13300 [133000/160000] training loss: 574.2888541221619\n",
            "epoch 11 batch 13400 [134000/160000] training loss: 548.2315773963928\n",
            "epoch 11 batch 13500 [135000/160000] training loss: 563.6672880649567\n",
            "epoch 11 batch 13600 [136000/160000] training loss: 542.1859469413757\n",
            "epoch 11 batch 13700 [137000/160000] training loss: 597.85362803936\n",
            "epoch 11 batch 13800 [138000/160000] training loss: 578.4604823589325\n",
            "epoch 11 batch 13900 [139000/160000] training loss: 598.2666773796082\n",
            "epoch 11 batch 14000 [140000/160000] training loss: 571.226554274559\n",
            "epoch 11 batch 14100 [141000/160000] training loss: 551.4094738960266\n",
            "epoch 11 batch 14200 [142000/160000] training loss: 568.0531430244446\n",
            "epoch 11 batch 14300 [143000/160000] training loss: 568.2020690441132\n",
            "epoch 11 batch 14400 [144000/160000] training loss: 582.539274930954\n",
            "epoch 11 batch 14500 [145000/160000] training loss: 567.8118426799774\n",
            "epoch 11 batch 14600 [146000/160000] training loss: 563.9535455703735\n",
            "epoch 11 batch 14700 [147000/160000] training loss: 556.4065117835999\n",
            "epoch 11 batch 14800 [148000/160000] training loss: 568.8536731004715\n",
            "epoch 11 batch 14900 [149000/160000] training loss: 558.8252210617065\n",
            "epoch 11 batch 15000 [150000/160000] training loss: 601.0144901275635\n",
            "epoch 11 batch 15100 [151000/160000] training loss: 571.214926481247\n",
            "epoch 11 batch 15200 [152000/160000] training loss: 602.3384709358215\n",
            "epoch 11 batch 15300 [153000/160000] training loss: 565.7526383399963\n",
            "epoch 11 batch 15400 [154000/160000] training loss: 589.3669548034668\n",
            "epoch 11 batch 15500 [155000/160000] training loss: 557.263557434082\n",
            "epoch 11 batch 15600 [156000/160000] training loss: 538.0437703132629\n",
            "epoch 11 batch 15700 [157000/160000] training loss: 549.4155570268631\n",
            "epoch 11 batch 15800 [158000/160000] training loss: 560.7441701889038\n",
            "epoch 11 batch 15900 [159000/160000] training loss: 544.90802693367\n",
            "epoch 12 batch 0 [0/160000] training loss: 5.505118370056152\n",
            "epoch 12 batch 100 [1000/160000] training loss: 571.1971943378448\n",
            "epoch 12 batch 200 [2000/160000] training loss: 573.2120577096939\n",
            "epoch 12 batch 300 [3000/160000] training loss: 569.8610601425171\n",
            "epoch 12 batch 400 [4000/160000] training loss: 575.4291067123413\n",
            "epoch 12 batch 500 [5000/160000] training loss: 578.9390020370483\n",
            "epoch 12 batch 600 [6000/160000] training loss: 566.7792888879776\n",
            "epoch 12 batch 700 [7000/160000] training loss: 554.3109776973724\n",
            "epoch 12 batch 800 [8000/160000] training loss: 584.4264841079712\n",
            "epoch 12 batch 900 [9000/160000] training loss: 574.0423624515533\n",
            "epoch 12 batch 1000 [10000/160000] training loss: 579.3219294548035\n",
            "epoch 12 batch 1100 [11000/160000] training loss: 558.3244029283524\n",
            "epoch 12 batch 1200 [12000/160000] training loss: 559.4829208850861\n",
            "epoch 12 batch 1300 [13000/160000] training loss: 595.068868637085\n",
            "epoch 12 batch 1400 [14000/160000] training loss: 562.801904797554\n",
            "epoch 12 batch 1500 [15000/160000] training loss: 541.2479767799377\n",
            "epoch 12 batch 1600 [16000/160000] training loss: 576.8514316082001\n",
            "epoch 12 batch 1700 [17000/160000] training loss: 580.0688014030457\n",
            "epoch 12 batch 1800 [18000/160000] training loss: 567.8666055202484\n",
            "epoch 12 batch 1900 [19000/160000] training loss: 561.7592468261719\n",
            "epoch 12 batch 2000 [20000/160000] training loss: 548.954342842102\n",
            "epoch 12 batch 2100 [21000/160000] training loss: 544.6139516830444\n",
            "epoch 12 batch 2200 [22000/160000] training loss: 589.3678479194641\n",
            "epoch 12 batch 2300 [23000/160000] training loss: 570.9955959320068\n",
            "epoch 12 batch 2400 [24000/160000] training loss: 556.6708326339722\n",
            "epoch 12 batch 2500 [25000/160000] training loss: 592.5297844409943\n",
            "epoch 12 batch 2600 [26000/160000] training loss: 554.5986649990082\n",
            "epoch 12 batch 2700 [27000/160000] training loss: 583.2672544717789\n",
            "epoch 12 batch 2800 [28000/160000] training loss: 532.7919728755951\n",
            "epoch 12 batch 2900 [29000/160000] training loss: 612.2396063804626\n",
            "epoch 12 batch 3000 [30000/160000] training loss: 577.3484456539154\n",
            "epoch 12 batch 3100 [31000/160000] training loss: 585.494838476181\n",
            "epoch 12 batch 3200 [32000/160000] training loss: 559.4682862758636\n",
            "epoch 12 batch 3300 [33000/160000] training loss: 573.5276973247528\n",
            "epoch 12 batch 3400 [34000/160000] training loss: 570.4469177722931\n",
            "epoch 12 batch 3500 [35000/160000] training loss: 580.4885671138763\n",
            "epoch 12 batch 3600 [36000/160000] training loss: 566.2813880443573\n",
            "epoch 12 batch 3700 [37000/160000] training loss: 547.5826461315155\n",
            "epoch 12 batch 3800 [38000/160000] training loss: 563.3506891727448\n",
            "epoch 12 batch 3900 [39000/160000] training loss: 572.8269820213318\n",
            "epoch 12 batch 4000 [40000/160000] training loss: 563.0403428077698\n",
            "epoch 12 batch 4100 [41000/160000] training loss: 574.5814476013184\n",
            "epoch 12 batch 4200 [42000/160000] training loss: 592.9490430355072\n",
            "epoch 12 batch 4300 [43000/160000] training loss: 573.8869615793228\n",
            "epoch 12 batch 4400 [44000/160000] training loss: 553.7280461788177\n",
            "epoch 12 batch 4500 [45000/160000] training loss: 576.0429730415344\n",
            "epoch 12 batch 4600 [46000/160000] training loss: 535.1788568496704\n",
            "epoch 12 batch 4700 [47000/160000] training loss: 545.8137023448944\n",
            "epoch 12 batch 4800 [48000/160000] training loss: 565.0826535224915\n",
            "epoch 12 batch 4900 [49000/160000] training loss: 567.6967718601227\n",
            "epoch 12 batch 5000 [50000/160000] training loss: 559.9042963981628\n",
            "epoch 12 batch 5100 [51000/160000] training loss: 554.9701581001282\n",
            "epoch 12 batch 5200 [52000/160000] training loss: 566.8649535179138\n",
            "epoch 12 batch 5300 [53000/160000] training loss: 568.6381105184555\n",
            "epoch 12 batch 5400 [54000/160000] training loss: 565.8877811431885\n",
            "epoch 12 batch 5500 [55000/160000] training loss: 555.3327510356903\n",
            "epoch 12 batch 5600 [56000/160000] training loss: 576.1561514139175\n",
            "epoch 12 batch 5700 [57000/160000] training loss: 570.0679790973663\n",
            "epoch 12 batch 5800 [58000/160000] training loss: 567.9001061916351\n",
            "epoch 12 batch 5900 [59000/160000] training loss: 544.2494885921478\n",
            "epoch 12 batch 6000 [60000/160000] training loss: 585.440952539444\n",
            "epoch 12 batch 6100 [61000/160000] training loss: 571.1676287651062\n",
            "epoch 12 batch 6200 [62000/160000] training loss: 590.0951192378998\n",
            "epoch 12 batch 6300 [63000/160000] training loss: 559.0023720264435\n",
            "epoch 12 batch 6400 [64000/160000] training loss: 548.2881445884705\n",
            "epoch 12 batch 6500 [65000/160000] training loss: 560.7526190280914\n",
            "epoch 12 batch 6600 [66000/160000] training loss: 538.6941616535187\n",
            "epoch 12 batch 6700 [67000/160000] training loss: 582.7469627857208\n",
            "epoch 12 batch 6800 [68000/160000] training loss: 540.0186219215393\n",
            "epoch 12 batch 6900 [69000/160000] training loss: 549.768208026886\n",
            "epoch 12 batch 7000 [70000/160000] training loss: 543.3919970989227\n",
            "epoch 12 batch 7100 [71000/160000] training loss: 531.7213709354401\n",
            "epoch 12 batch 7200 [72000/160000] training loss: 561.7153451442719\n",
            "epoch 12 batch 7300 [73000/160000] training loss: 550.9362123012543\n",
            "epoch 12 batch 7400 [74000/160000] training loss: 544.6952958106995\n",
            "epoch 12 batch 7500 [75000/160000] training loss: 559.6671898365021\n",
            "epoch 12 batch 7600 [76000/160000] training loss: 569.5465887784958\n",
            "epoch 12 batch 7700 [77000/160000] training loss: 588.4897770881653\n",
            "epoch 12 batch 7800 [78000/160000] training loss: 589.2875199317932\n",
            "epoch 12 batch 7900 [79000/160000] training loss: 554.5072243213654\n",
            "epoch 12 batch 8000 [80000/160000] training loss: 555.3636875152588\n",
            "epoch 12 batch 8100 [81000/160000] training loss: 546.3046524524689\n",
            "epoch 12 batch 8200 [82000/160000] training loss: 538.85364818573\n",
            "epoch 12 batch 8300 [83000/160000] training loss: 577.0034853219986\n",
            "epoch 12 batch 8400 [84000/160000] training loss: 573.8425691127777\n",
            "epoch 12 batch 8500 [85000/160000] training loss: 582.2493711709976\n",
            "epoch 12 batch 8600 [86000/160000] training loss: 548.4577746391296\n",
            "epoch 12 batch 8700 [87000/160000] training loss: 558.9461572170258\n",
            "epoch 12 batch 8800 [88000/160000] training loss: 576.8891024589539\n",
            "epoch 12 batch 8900 [89000/160000] training loss: 574.8885245323181\n",
            "epoch 12 batch 9000 [90000/160000] training loss: 586.5774936676025\n",
            "epoch 12 batch 9100 [91000/160000] training loss: 584.168420791626\n",
            "epoch 12 batch 9200 [92000/160000] training loss: 524.344322681427\n",
            "epoch 12 batch 9300 [93000/160000] training loss: 578.7497935295105\n",
            "epoch 12 batch 9400 [94000/160000] training loss: 532.9279271364212\n",
            "epoch 12 batch 9500 [95000/160000] training loss: 571.3738096952438\n",
            "epoch 12 batch 9600 [96000/160000] training loss: 555.756409406662\n",
            "epoch 12 batch 9700 [97000/160000] training loss: 555.8893895149231\n",
            "epoch 12 batch 9800 [98000/160000] training loss: 559.4866590499878\n",
            "epoch 12 batch 9900 [99000/160000] training loss: 565.171541929245\n",
            "epoch 12 batch 10000 [100000/160000] training loss: 592.636234998703\n",
            "epoch 12 batch 10100 [101000/160000] training loss: 568.9061037302017\n",
            "epoch 12 batch 10200 [102000/160000] training loss: 538.4025042057037\n",
            "epoch 12 batch 10300 [103000/160000] training loss: 571.4052801132202\n",
            "epoch 12 batch 10400 [104000/160000] training loss: 569.4416596889496\n",
            "epoch 12 batch 10500 [105000/160000] training loss: 603.0812265872955\n",
            "epoch 12 batch 10600 [106000/160000] training loss: 589.4598224163055\n",
            "epoch 12 batch 10700 [107000/160000] training loss: 563.6269354820251\n",
            "epoch 12 batch 10800 [108000/160000] training loss: 546.0044372081757\n",
            "epoch 12 batch 10900 [109000/160000] training loss: 547.64548432827\n",
            "epoch 12 batch 11000 [110000/160000] training loss: 565.223294377327\n",
            "epoch 12 batch 11100 [111000/160000] training loss: 553.5816099643707\n",
            "epoch 12 batch 11200 [112000/160000] training loss: 570.0520603656769\n",
            "epoch 12 batch 11300 [113000/160000] training loss: 554.5275077819824\n",
            "epoch 12 batch 11400 [114000/160000] training loss: 571.1955687999725\n",
            "epoch 12 batch 11500 [115000/160000] training loss: 571.8029901981354\n",
            "epoch 12 batch 11600 [116000/160000] training loss: 591.987854719162\n",
            "epoch 12 batch 11700 [117000/160000] training loss: 543.7539765834808\n",
            "epoch 12 batch 11800 [118000/160000] training loss: 554.1904853582382\n",
            "epoch 12 batch 11900 [119000/160000] training loss: 547.4466915130615\n",
            "epoch 12 batch 12000 [120000/160000] training loss: 566.9448750019073\n",
            "epoch 12 batch 12100 [121000/160000] training loss: 590.2918627262115\n",
            "epoch 12 batch 12200 [122000/160000] training loss: 554.5630979537964\n",
            "epoch 12 batch 12300 [123000/160000] training loss: 565.4685297012329\n",
            "epoch 12 batch 12400 [124000/160000] training loss: 571.0982022285461\n",
            "epoch 12 batch 12500 [125000/160000] training loss: 554.7324690818787\n",
            "epoch 12 batch 12600 [126000/160000] training loss: 568.5291936397552\n",
            "epoch 12 batch 12700 [127000/160000] training loss: 553.0729689598083\n",
            "epoch 12 batch 12800 [128000/160000] training loss: 578.0006041526794\n",
            "epoch 12 batch 12900 [129000/160000] training loss: 577.5897810459137\n",
            "epoch 12 batch 13000 [130000/160000] training loss: 585.4327929019928\n",
            "epoch 12 batch 13100 [131000/160000] training loss: 561.6851072311401\n",
            "epoch 12 batch 13200 [132000/160000] training loss: 593.5514498949051\n",
            "epoch 12 batch 13300 [133000/160000] training loss: 573.6564574241638\n",
            "epoch 12 batch 13400 [134000/160000] training loss: 548.087557554245\n",
            "epoch 12 batch 13500 [135000/160000] training loss: 563.6944632530212\n",
            "epoch 12 batch 13600 [136000/160000] training loss: 541.8338861465454\n",
            "epoch 12 batch 13700 [137000/160000] training loss: 598.09741294384\n",
            "epoch 12 batch 13800 [138000/160000] training loss: 579.125072479248\n",
            "epoch 12 batch 13900 [139000/160000] training loss: 598.4347109794617\n",
            "epoch 12 batch 14000 [140000/160000] training loss: 570.7526031732559\n",
            "epoch 12 batch 14100 [141000/160000] training loss: 550.2420938014984\n",
            "epoch 12 batch 14200 [142000/160000] training loss: 567.654812335968\n",
            "epoch 12 batch 14300 [143000/160000] training loss: 567.372960805893\n",
            "epoch 12 batch 14400 [144000/160000] training loss: 582.2038817405701\n",
            "epoch 12 batch 14500 [145000/160000] training loss: 567.3461446762085\n",
            "epoch 12 batch 14600 [146000/160000] training loss: 563.1269745826721\n",
            "epoch 12 batch 14700 [147000/160000] training loss: 555.7358343601227\n",
            "epoch 12 batch 14800 [148000/160000] training loss: 568.4377599954605\n",
            "epoch 12 batch 14900 [149000/160000] training loss: 557.923680305481\n",
            "epoch 12 batch 15000 [150000/160000] training loss: 600.3472207784653\n",
            "epoch 12 batch 15100 [151000/160000] training loss: 570.8623239994049\n",
            "epoch 12 batch 15200 [152000/160000] training loss: 601.7952601909637\n",
            "epoch 12 batch 15300 [153000/160000] training loss: 565.386819601059\n",
            "epoch 12 batch 15400 [154000/160000] training loss: 588.789707660675\n",
            "epoch 12 batch 15500 [155000/160000] training loss: 556.6472692489624\n",
            "epoch 12 batch 15600 [156000/160000] training loss: 537.8558592796326\n",
            "epoch 12 batch 15700 [157000/160000] training loss: 548.6478279829025\n",
            "epoch 12 batch 15800 [158000/160000] training loss: 560.5133409500122\n",
            "epoch 12 batch 15900 [159000/160000] training loss: 545.0630333423615\n",
            "epoch 13 batch 0 [0/160000] training loss: 5.499942302703857\n",
            "epoch 13 batch 100 [1000/160000] training loss: 571.1350635290146\n",
            "epoch 13 batch 200 [2000/160000] training loss: 573.2974364757538\n",
            "epoch 13 batch 300 [3000/160000] training loss: 569.7416739463806\n",
            "epoch 13 batch 400 [4000/160000] training loss: 575.1514620780945\n",
            "epoch 13 batch 500 [5000/160000] training loss: 579.0953495502472\n",
            "epoch 13 batch 600 [6000/160000] training loss: 566.7399191856384\n",
            "epoch 13 batch 700 [7000/160000] training loss: 553.7579908370972\n",
            "epoch 13 batch 800 [8000/160000] training loss: 584.9369208812714\n",
            "epoch 13 batch 900 [9000/160000] training loss: 573.4445452690125\n",
            "epoch 13 batch 1000 [10000/160000] training loss: 579.4927995204926\n",
            "epoch 13 batch 1100 [11000/160000] training loss: 557.912186384201\n",
            "epoch 13 batch 1200 [12000/160000] training loss: 558.3661878108978\n",
            "epoch 13 batch 1300 [13000/160000] training loss: 594.8190016746521\n",
            "epoch 13 batch 1400 [14000/160000] training loss: 562.8996230363846\n",
            "epoch 13 batch 1500 [15000/160000] training loss: 540.800927400589\n",
            "epoch 13 batch 1600 [16000/160000] training loss: 576.2760698795319\n",
            "epoch 13 batch 1700 [17000/160000] training loss: 579.448245048523\n",
            "epoch 13 batch 1800 [18000/160000] training loss: 567.4938937425613\n",
            "epoch 13 batch 1900 [19000/160000] training loss: 561.2321918010712\n",
            "epoch 13 batch 2000 [20000/160000] training loss: 548.0973439216614\n",
            "epoch 13 batch 2100 [21000/160000] training loss: 544.7509028911591\n",
            "epoch 13 batch 2200 [22000/160000] training loss: 589.3562828302383\n",
            "epoch 13 batch 2300 [23000/160000] training loss: 570.8152109384537\n",
            "epoch 13 batch 2400 [24000/160000] training loss: 556.4174245595932\n",
            "epoch 13 batch 2500 [25000/160000] training loss: 591.960289478302\n",
            "epoch 13 batch 2600 [26000/160000] training loss: 554.4194316864014\n",
            "epoch 13 batch 2700 [27000/160000] training loss: 583.219025015831\n",
            "epoch 13 batch 2800 [28000/160000] training loss: 532.2238576412201\n",
            "epoch 13 batch 2900 [29000/160000] training loss: 611.6341679096222\n",
            "epoch 13 batch 3000 [30000/160000] training loss: 577.5530664920807\n",
            "epoch 13 batch 3100 [31000/160000] training loss: 585.2070469856262\n",
            "epoch 13 batch 3200 [32000/160000] training loss: 558.4506459236145\n",
            "epoch 13 batch 3300 [33000/160000] training loss: 573.0698492527008\n",
            "epoch 13 batch 3400 [34000/160000] training loss: 570.654470205307\n",
            "epoch 13 batch 3500 [35000/160000] training loss: 579.8754918575287\n",
            "epoch 13 batch 3600 [36000/160000] training loss: 566.2781150341034\n",
            "epoch 13 batch 3700 [37000/160000] training loss: 547.2128117084503\n",
            "epoch 13 batch 3800 [38000/160000] training loss: 563.036783695221\n",
            "epoch 13 batch 3900 [39000/160000] training loss: 573.1002566814423\n",
            "epoch 13 batch 4000 [40000/160000] training loss: 563.0415916442871\n",
            "epoch 13 batch 4100 [41000/160000] training loss: 574.6622395515442\n",
            "epoch 13 batch 4200 [42000/160000] training loss: 592.6621663570404\n",
            "epoch 13 batch 4300 [43000/160000] training loss: 573.4812513589859\n",
            "epoch 13 batch 4400 [44000/160000] training loss: 553.5169395208359\n",
            "epoch 13 batch 4500 [45000/160000] training loss: 576.258969783783\n",
            "epoch 13 batch 4600 [46000/160000] training loss: 535.0961608886719\n",
            "epoch 13 batch 4700 [47000/160000] training loss: 545.5886738300323\n",
            "epoch 13 batch 4800 [48000/160000] training loss: 564.9153183698654\n",
            "epoch 13 batch 4900 [49000/160000] training loss: 567.7174818515778\n",
            "epoch 13 batch 5000 [50000/160000] training loss: 559.6607797145844\n",
            "epoch 13 batch 5100 [51000/160000] training loss: 554.5576840639114\n",
            "epoch 13 batch 5200 [52000/160000] training loss: 566.9578411579132\n",
            "epoch 13 batch 5300 [53000/160000] training loss: 568.0383223295212\n",
            "epoch 13 batch 5400 [54000/160000] training loss: 566.4265720844269\n",
            "epoch 13 batch 5500 [55000/160000] training loss: 555.3660349845886\n",
            "epoch 13 batch 5600 [56000/160000] training loss: 575.93665599823\n",
            "epoch 13 batch 5700 [57000/160000] training loss: 569.4968492984772\n",
            "epoch 13 batch 5800 [58000/160000] training loss: 567.8529299497604\n",
            "epoch 13 batch 5900 [59000/160000] training loss: 543.847528219223\n",
            "epoch 13 batch 6000 [60000/160000] training loss: 585.045257806778\n",
            "epoch 13 batch 6100 [61000/160000] training loss: 570.147659778595\n",
            "epoch 13 batch 6200 [62000/160000] training loss: 590.1799385547638\n",
            "epoch 13 batch 6300 [63000/160000] training loss: 559.5346958637238\n",
            "epoch 13 batch 6400 [64000/160000] training loss: 547.8917906284332\n",
            "epoch 13 batch 6500 [65000/160000] training loss: 559.936295747757\n",
            "epoch 13 batch 6600 [66000/160000] training loss: 538.0660643577576\n",
            "epoch 13 batch 6700 [67000/160000] training loss: 582.7422810792923\n",
            "epoch 13 batch 6800 [68000/160000] training loss: 539.3527698516846\n",
            "epoch 13 batch 6900 [69000/160000] training loss: 549.46875\n",
            "epoch 13 batch 7000 [70000/160000] training loss: 542.4601438045502\n",
            "epoch 13 batch 7100 [71000/160000] training loss: 531.5090579986572\n",
            "epoch 13 batch 7200 [72000/160000] training loss: 561.713760137558\n",
            "epoch 13 batch 7300 [73000/160000] training loss: 550.5459668636322\n",
            "epoch 13 batch 7400 [74000/160000] training loss: 543.6151676177979\n",
            "epoch 13 batch 7500 [75000/160000] training loss: 558.967284321785\n",
            "epoch 13 batch 7600 [76000/160000] training loss: 568.9147207736969\n",
            "epoch 13 batch 7700 [77000/160000] training loss: 588.1980941295624\n",
            "epoch 13 batch 7800 [78000/160000] training loss: 588.902015209198\n",
            "epoch 13 batch 7900 [79000/160000] training loss: 554.2349953651428\n",
            "epoch 13 batch 8000 [80000/160000] training loss: 554.9414339065552\n",
            "epoch 13 batch 8100 [81000/160000] training loss: 545.5878039598465\n",
            "epoch 13 batch 8200 [82000/160000] training loss: 538.6183128356934\n",
            "epoch 13 batch 8300 [83000/160000] training loss: 577.1228415966034\n",
            "epoch 13 batch 8400 [84000/160000] training loss: 573.5503768920898\n",
            "epoch 13 batch 8500 [85000/160000] training loss: 582.2115544080734\n",
            "epoch 13 batch 8600 [86000/160000] training loss: 548.510826587677\n",
            "epoch 13 batch 8700 [87000/160000] training loss: 558.744624376297\n",
            "epoch 13 batch 8800 [88000/160000] training loss: 576.3299860954285\n",
            "epoch 13 batch 8900 [89000/160000] training loss: 573.6902821063995\n",
            "epoch 13 batch 9000 [90000/160000] training loss: 586.6401562690735\n",
            "epoch 13 batch 9100 [91000/160000] training loss: 584.2433295249939\n",
            "epoch 13 batch 9200 [92000/160000] training loss: 524.038410782814\n",
            "epoch 13 batch 9300 [93000/160000] training loss: 578.6320910453796\n",
            "epoch 13 batch 9400 [94000/160000] training loss: 532.4835073947906\n",
            "epoch 13 batch 9500 [95000/160000] training loss: 571.7086690664291\n",
            "epoch 13 batch 9600 [96000/160000] training loss: 555.3178144693375\n",
            "epoch 13 batch 9700 [97000/160000] training loss: 555.0922515392303\n",
            "epoch 13 batch 9800 [98000/160000] training loss: 558.6891398429871\n",
            "epoch 13 batch 9900 [99000/160000] training loss: 565.1927242279053\n",
            "epoch 13 batch 10000 [100000/160000] training loss: 591.8923757076263\n",
            "epoch 13 batch 10100 [101000/160000] training loss: 569.0291264057159\n",
            "epoch 13 batch 10200 [102000/160000] training loss: 538.6809916496277\n",
            "epoch 13 batch 10300 [103000/160000] training loss: 570.8979859352112\n",
            "epoch 13 batch 10400 [104000/160000] training loss: 569.5586538314819\n",
            "epoch 13 batch 10500 [105000/160000] training loss: 602.8158667087555\n",
            "epoch 13 batch 10600 [106000/160000] training loss: 589.1312197446823\n",
            "epoch 13 batch 10700 [107000/160000] training loss: 562.8907220363617\n",
            "epoch 13 batch 10800 [108000/160000] training loss: 545.7080515623093\n",
            "epoch 13 batch 10900 [109000/160000] training loss: 546.8130388259888\n",
            "epoch 13 batch 11000 [110000/160000] training loss: 565.2391463518143\n",
            "epoch 13 batch 11100 [111000/160000] training loss: 553.1319966316223\n",
            "epoch 13 batch 11200 [112000/160000] training loss: 569.710363149643\n",
            "epoch 13 batch 11300 [113000/160000] training loss: 554.5479757785797\n",
            "epoch 13 batch 11400 [114000/160000] training loss: 570.4100120067596\n",
            "epoch 13 batch 11500 [115000/160000] training loss: 571.7223982810974\n",
            "epoch 13 batch 11600 [116000/160000] training loss: 592.0073075294495\n",
            "epoch 13 batch 11700 [117000/160000] training loss: 543.3055884838104\n",
            "epoch 13 batch 11800 [118000/160000] training loss: 553.6778230667114\n",
            "epoch 13 batch 11900 [119000/160000] training loss: 547.4197055101395\n",
            "epoch 13 batch 12000 [120000/160000] training loss: 566.7861232757568\n",
            "epoch 13 batch 12100 [121000/160000] training loss: 589.7950055599213\n",
            "epoch 13 batch 12200 [122000/160000] training loss: 554.7751407623291\n",
            "epoch 13 batch 12300 [123000/160000] training loss: 565.1353950500488\n",
            "epoch 13 batch 12400 [124000/160000] training loss: 570.9648170471191\n",
            "epoch 13 batch 12500 [125000/160000] training loss: 554.3530113697052\n",
            "epoch 13 batch 12600 [126000/160000] training loss: 568.5188367366791\n",
            "epoch 13 batch 12700 [127000/160000] training loss: 552.5467129945755\n",
            "epoch 13 batch 12800 [128000/160000] training loss: 578.1453022956848\n",
            "epoch 13 batch 12900 [129000/160000] training loss: 577.37166929245\n",
            "epoch 13 batch 13000 [130000/160000] training loss: 585.3221619129181\n",
            "epoch 13 batch 13100 [131000/160000] training loss: 562.2752485275269\n",
            "epoch 13 batch 13200 [132000/160000] training loss: 593.1872701644897\n",
            "epoch 13 batch 13300 [133000/160000] training loss: 573.1285727024078\n",
            "epoch 13 batch 13400 [134000/160000] training loss: 547.9463646411896\n",
            "epoch 13 batch 13500 [135000/160000] training loss: 563.7252109050751\n",
            "epoch 13 batch 13600 [136000/160000] training loss: 541.5726299285889\n",
            "epoch 13 batch 13700 [137000/160000] training loss: 598.3550021648407\n",
            "epoch 13 batch 13800 [138000/160000] training loss: 579.7534992694855\n",
            "epoch 13 batch 13900 [139000/160000] training loss: 598.582192659378\n",
            "epoch 13 batch 14000 [140000/160000] training loss: 570.3871384859085\n",
            "epoch 13 batch 14100 [141000/160000] training loss: 549.1552972793579\n",
            "epoch 13 batch 14200 [142000/160000] training loss: 567.2943623065948\n",
            "epoch 13 batch 14300 [143000/160000] training loss: 566.6208944320679\n",
            "epoch 13 batch 14400 [144000/160000] training loss: 581.9107420444489\n",
            "epoch 13 batch 14500 [145000/160000] training loss: 566.9199514389038\n",
            "epoch 13 batch 14600 [146000/160000] training loss: 562.4345716238022\n",
            "epoch 13 batch 14700 [147000/160000] training loss: 555.1604909896851\n",
            "epoch 13 batch 14800 [148000/160000] training loss: 568.0688771009445\n",
            "epoch 13 batch 14900 [149000/160000] training loss: 557.1508300304413\n",
            "epoch 13 batch 15000 [150000/160000] training loss: 599.7679531574249\n",
            "epoch 13 batch 15100 [151000/160000] training loss: 570.55078125\n",
            "epoch 13 batch 15200 [152000/160000] training loss: 601.3260133266449\n",
            "epoch 13 batch 15300 [153000/160000] training loss: 565.081561088562\n",
            "epoch 13 batch 15400 [154000/160000] training loss: 588.2284815311432\n",
            "epoch 13 batch 15500 [155000/160000] training loss: 556.127697467804\n",
            "epoch 13 batch 15600 [156000/160000] training loss: 537.7710134983063\n",
            "epoch 13 batch 15700 [157000/160000] training loss: 547.950340628624\n",
            "epoch 13 batch 15800 [158000/160000] training loss: 560.3091053962708\n",
            "epoch 13 batch 15900 [159000/160000] training loss: 545.3170099258423\n",
            "epoch 14 batch 0 [0/160000] training loss: 5.499794006347656\n",
            "epoch 14 batch 100 [1000/160000] training loss: 571.086420416832\n",
            "epoch 14 batch 200 [2000/160000] training loss: 573.3595043420792\n",
            "epoch 14 batch 300 [3000/160000] training loss: 569.648402929306\n",
            "epoch 14 batch 400 [4000/160000] training loss: 574.9270012378693\n",
            "epoch 14 batch 500 [5000/160000] training loss: 579.1921365261078\n",
            "epoch 14 batch 600 [6000/160000] training loss: 566.6835610866547\n",
            "epoch 14 batch 700 [7000/160000] training loss: 553.233404636383\n",
            "epoch 14 batch 800 [8000/160000] training loss: 585.4690916538239\n",
            "epoch 14 batch 900 [9000/160000] training loss: 572.9155707359314\n",
            "epoch 14 batch 1000 [10000/160000] training loss: 579.6703135967255\n",
            "epoch 14 batch 1100 [11000/160000] training loss: 557.584599852562\n",
            "epoch 14 batch 1200 [12000/160000] training loss: 557.405748128891\n",
            "epoch 14 batch 1300 [13000/160000] training loss: 594.569329738617\n",
            "epoch 14 batch 1400 [14000/160000] training loss: 563.0049629211426\n",
            "epoch 14 batch 1500 [15000/160000] training loss: 540.4432928562164\n",
            "epoch 14 batch 1600 [16000/160000] training loss: 575.7649688720703\n",
            "epoch 14 batch 1700 [17000/160000] training loss: 578.8589181900024\n",
            "epoch 14 batch 1800 [18000/160000] training loss: 567.1676127910614\n",
            "epoch 14 batch 1900 [19000/160000] training loss: 560.7846829891205\n",
            "epoch 14 batch 2000 [20000/160000] training loss: 547.3465800285339\n",
            "epoch 14 batch 2100 [21000/160000] training loss: 544.8780843019485\n",
            "epoch 14 batch 2200 [22000/160000] training loss: 589.3491579294205\n",
            "epoch 14 batch 2300 [23000/160000] training loss: 570.62985932827\n",
            "epoch 14 batch 2400 [24000/160000] training loss: 556.2453762292862\n",
            "epoch 14 batch 2500 [25000/160000] training loss: 591.4589321613312\n",
            "epoch 14 batch 2600 [26000/160000] training loss: 554.246505022049\n",
            "epoch 14 batch 2700 [27000/160000] training loss: 583.2211073637009\n",
            "epoch 14 batch 2800 [28000/160000] training loss: 531.7238292694092\n",
            "epoch 14 batch 2900 [29000/160000] training loss: 611.0367832183838\n",
            "epoch 14 batch 3000 [30000/160000] training loss: 577.7422094345093\n",
            "epoch 14 batch 3100 [31000/160000] training loss: 584.9526362419128\n",
            "epoch 14 batch 3200 [32000/160000] training loss: 557.5257241725922\n",
            "epoch 14 batch 3300 [33000/160000] training loss: 572.6635050773621\n",
            "epoch 14 batch 3400 [34000/160000] training loss: 570.8373928070068\n",
            "epoch 14 batch 3500 [35000/160000] training loss: 579.3104367256165\n",
            "epoch 14 batch 3600 [36000/160000] training loss: 566.292179107666\n",
            "epoch 14 batch 3700 [37000/160000] training loss: 546.8625180721283\n",
            "epoch 14 batch 3800 [38000/160000] training loss: 562.7834506034851\n",
            "epoch 14 batch 3900 [39000/160000] training loss: 573.3793189525604\n",
            "epoch 14 batch 4000 [40000/160000] training loss: 563.0988187789917\n",
            "epoch 14 batch 4100 [41000/160000] training loss: 574.6972777843475\n",
            "epoch 14 batch 4200 [42000/160000] training loss: 592.455472946167\n",
            "epoch 14 batch 4300 [43000/160000] training loss: 573.0837924480438\n",
            "epoch 14 batch 4400 [44000/160000] training loss: 553.3380560874939\n",
            "epoch 14 batch 4500 [45000/160000] training loss: 576.5043249130249\n",
            "epoch 14 batch 4600 [46000/160000] training loss: 535.0253250598907\n",
            "epoch 14 batch 4700 [47000/160000] training loss: 545.4027123451233\n",
            "epoch 14 batch 4800 [48000/160000] training loss: 564.8108214139938\n",
            "epoch 14 batch 4900 [49000/160000] training loss: 567.7721421718597\n",
            "epoch 14 batch 5000 [50000/160000] training loss: 559.4688262939453\n",
            "epoch 14 batch 5100 [51000/160000] training loss: 554.1978751420975\n",
            "epoch 14 batch 5200 [52000/160000] training loss: 567.0583789348602\n",
            "epoch 14 batch 5300 [53000/160000] training loss: 567.5483573675156\n",
            "epoch 14 batch 5400 [54000/160000] training loss: 566.9371438026428\n",
            "epoch 14 batch 5500 [55000/160000] training loss: 555.3746683597565\n",
            "epoch 14 batch 5600 [56000/160000] training loss: 575.7495301961899\n",
            "epoch 14 batch 5700 [57000/160000] training loss: 569.0171852111816\n",
            "epoch 14 batch 5800 [58000/160000] training loss: 567.8885055780411\n",
            "epoch 14 batch 5900 [59000/160000] training loss: 543.4781107902527\n",
            "epoch 14 batch 6000 [60000/160000] training loss: 584.7030653953552\n",
            "epoch 14 batch 6100 [61000/160000] training loss: 569.1994483470917\n",
            "epoch 14 batch 6200 [62000/160000] training loss: 590.3105235099792\n",
            "epoch 14 batch 6300 [63000/160000] training loss: 560.0710632801056\n",
            "epoch 14 batch 6400 [64000/160000] training loss: 547.5787360668182\n",
            "epoch 14 batch 6500 [65000/160000] training loss: 559.2426841259003\n",
            "epoch 14 batch 6600 [66000/160000] training loss: 537.5113234519958\n",
            "epoch 14 batch 6700 [67000/160000] training loss: 582.7555146217346\n",
            "epoch 14 batch 6800 [68000/160000] training loss: 538.6845791339874\n",
            "epoch 14 batch 6900 [69000/160000] training loss: 549.2848782539368\n",
            "epoch 14 batch 7000 [70000/160000] training loss: 541.5828177928925\n",
            "epoch 14 batch 7100 [71000/160000] training loss: 531.3768193721771\n",
            "epoch 14 batch 7200 [72000/160000] training loss: 561.7089104652405\n",
            "epoch 14 batch 7300 [73000/160000] training loss: 550.2193112373352\n",
            "epoch 14 batch 7400 [74000/160000] training loss: 542.6403138637543\n",
            "epoch 14 batch 7500 [75000/160000] training loss: 558.3267209529877\n",
            "epoch 14 batch 7600 [76000/160000] training loss: 568.327932715416\n",
            "epoch 14 batch 7700 [77000/160000] training loss: 587.9908092021942\n",
            "epoch 14 batch 7800 [78000/160000] training loss: 588.4971287250519\n",
            "epoch 14 batch 7900 [79000/160000] training loss: 554.0050044059753\n",
            "epoch 14 batch 8000 [80000/160000] training loss: 554.5983457565308\n",
            "epoch 14 batch 8100 [81000/160000] training loss: 544.9678127765656\n",
            "epoch 14 batch 8200 [82000/160000] training loss: 538.4021754264832\n",
            "epoch 14 batch 8300 [83000/160000] training loss: 577.257306933403\n",
            "epoch 14 batch 8400 [84000/160000] training loss: 573.291974067688\n",
            "epoch 14 batch 8500 [85000/160000] training loss: 582.2275966405869\n",
            "epoch 14 batch 8600 [86000/160000] training loss: 548.6088182926178\n",
            "epoch 14 batch 8700 [87000/160000] training loss: 558.5655035972595\n",
            "epoch 14 batch 8800 [88000/160000] training loss: 575.8384492397308\n",
            "epoch 14 batch 8900 [89000/160000] training loss: 572.5701463222504\n",
            "epoch 14 batch 9000 [90000/160000] training loss: 586.6915938854218\n",
            "epoch 14 batch 9100 [91000/160000] training loss: 584.344306230545\n",
            "epoch 14 batch 9200 [92000/160000] training loss: 523.8043141365051\n",
            "epoch 14 batch 9300 [93000/160000] training loss: 578.567845582962\n",
            "epoch 14 batch 9400 [94000/160000] training loss: 532.1410435438156\n",
            "epoch 14 batch 9500 [95000/160000] training loss: 572.0480449199677\n",
            "epoch 14 batch 9600 [96000/160000] training loss: 554.978907585144\n",
            "epoch 14 batch 9700 [97000/160000] training loss: 554.3791749477386\n",
            "epoch 14 batch 9800 [98000/160000] training loss: 557.9274771213531\n",
            "epoch 14 batch 9900 [99000/160000] training loss: 565.2135183811188\n",
            "epoch 14 batch 10000 [100000/160000] training loss: 591.173951625824\n",
            "epoch 14 batch 10100 [101000/160000] training loss: 569.1520531177521\n",
            "epoch 14 batch 10200 [102000/160000] training loss: 538.9864478111267\n",
            "epoch 14 batch 10300 [103000/160000] training loss: 570.4221603870392\n",
            "epoch 14 batch 10400 [104000/160000] training loss: 569.6654627323151\n",
            "epoch 14 batch 10500 [105000/160000] training loss: 602.5653614997864\n",
            "epoch 14 batch 10600 [106000/160000] training loss: 588.8614896535873\n",
            "epoch 14 batch 10700 [107000/160000] training loss: 562.2234126329422\n",
            "epoch 14 batch 10800 [108000/160000] training loss: 545.439560174942\n",
            "epoch 14 batch 10900 [109000/160000] training loss: 546.0500665903091\n",
            "epoch 14 batch 11000 [110000/160000] training loss: 565.2078986167908\n",
            "epoch 14 batch 11100 [111000/160000] training loss: 552.6997718811035\n",
            "epoch 14 batch 11200 [112000/160000] training loss: 569.4106386899948\n",
            "epoch 14 batch 11300 [113000/160000] training loss: 554.6248197555542\n",
            "epoch 14 batch 11400 [114000/160000] training loss: 569.7659208774567\n",
            "epoch 14 batch 11500 [115000/160000] training loss: 571.6638813018799\n",
            "epoch 14 batch 11600 [116000/160000] training loss: 592.0112917423248\n",
            "epoch 14 batch 11700 [117000/160000] training loss: 542.8717730045319\n",
            "epoch 14 batch 11800 [118000/160000] training loss: 553.228521823883\n",
            "epoch 14 batch 11900 [119000/160000] training loss: 547.3825083971024\n",
            "epoch 14 batch 12000 [120000/160000] training loss: 566.6482436656952\n",
            "epoch 14 batch 12100 [121000/160000] training loss: 589.2996308803558\n",
            "epoch 14 batch 12200 [122000/160000] training loss: 554.9841389656067\n",
            "epoch 14 batch 12300 [123000/160000] training loss: 564.8566961288452\n",
            "epoch 14 batch 12400 [124000/160000] training loss: 570.9050467014313\n",
            "epoch 14 batch 12500 [125000/160000] training loss: 554.0194718837738\n",
            "epoch 14 batch 12600 [126000/160000] training loss: 568.5257014036179\n",
            "epoch 14 batch 12700 [127000/160000] training loss: 552.039116024971\n",
            "epoch 14 batch 12800 [128000/160000] training loss: 578.344161272049\n",
            "epoch 14 batch 12900 [129000/160000] training loss: 577.1394654512405\n",
            "epoch 14 batch 13000 [130000/160000] training loss: 585.2191097736359\n",
            "epoch 14 batch 13100 [131000/160000] training loss: 562.8681166172028\n",
            "epoch 14 batch 13200 [132000/160000] training loss: 592.8852069377899\n",
            "epoch 14 batch 13300 [133000/160000] training loss: 572.6956861019135\n",
            "epoch 14 batch 13400 [134000/160000] training loss: 547.8110949993134\n",
            "epoch 14 batch 13500 [135000/160000] training loss: 563.7559018135071\n",
            "epoch 14 batch 13600 [136000/160000] training loss: 541.3852937221527\n",
            "epoch 14 batch 13700 [137000/160000] training loss: 598.6137562990189\n",
            "epoch 14 batch 13800 [138000/160000] training loss: 580.3407833576202\n",
            "epoch 14 batch 13900 [139000/160000] training loss: 598.7049572467804\n",
            "epoch 14 batch 14000 [140000/160000] training loss: 570.1085418462753\n",
            "epoch 14 batch 14100 [141000/160000] training loss: 548.1379995346069\n",
            "epoch 14 batch 14200 [142000/160000] training loss: 566.9605777263641\n",
            "epoch 14 batch 14300 [143000/160000] training loss: 565.935622215271\n",
            "epoch 14 batch 14400 [144000/160000] training loss: 581.6506004333496\n",
            "epoch 14 batch 14500 [145000/160000] training loss: 566.52423620224\n",
            "epoch 14 batch 14600 [146000/160000] training loss: 561.8538992404938\n",
            "epoch 14 batch 14700 [147000/160000] training loss: 554.6631073951721\n",
            "epoch 14 batch 14800 [148000/160000] training loss: 567.7391560077667\n",
            "epoch 14 batch 14900 [149000/160000] training loss: 556.4948587417603\n",
            "epoch 14 batch 15000 [150000/160000] training loss: 599.261960029602\n",
            "epoch 14 batch 15100 [151000/160000] training loss: 570.2748718261719\n",
            "epoch 14 batch 15200 [152000/160000] training loss: 600.9142963886261\n",
            "epoch 14 batch 15300 [153000/160000] training loss: 564.8235337734222\n",
            "epoch 14 batch 15400 [154000/160000] training loss: 587.683191537857\n",
            "epoch 14 batch 15500 [155000/160000] training loss: 555.6902115345001\n",
            "epoch 14 batch 15600 [156000/160000] training loss: 537.7654051780701\n",
            "epoch 14 batch 15700 [157000/160000] training loss: 547.3184330463409\n",
            "epoch 14 batch 15800 [158000/160000] training loss: 560.1254444122314\n",
            "epoch 14 batch 15900 [159000/160000] training loss: 545.6479794979095\n",
            "epoch 15 batch 0 [0/160000] training loss: 5.503695487976074\n",
            "epoch 15 batch 100 [1000/160000] training loss: 571.0428272485733\n",
            "epoch 15 batch 200 [2000/160000] training loss: 573.4065948724747\n",
            "epoch 15 batch 300 [3000/160000] training loss: 569.5788607597351\n",
            "epoch 15 batch 400 [4000/160000] training loss: 574.7410082817078\n",
            "epoch 15 batch 500 [5000/160000] training loss: 579.2407925128937\n",
            "epoch 15 batch 600 [6000/160000] training loss: 566.6081314086914\n",
            "epoch 15 batch 700 [7000/160000] training loss: 552.743289232254\n",
            "epoch 15 batch 800 [8000/160000] training loss: 586.0069739818573\n",
            "epoch 15 batch 900 [9000/160000] training loss: 572.4512373209\n",
            "epoch 15 batch 1000 [10000/160000] training loss: 579.8473324775696\n",
            "epoch 15 batch 1100 [11000/160000] training loss: 557.3340233564377\n",
            "epoch 15 batch 1200 [12000/160000] training loss: 556.5724804401398\n",
            "epoch 15 batch 1300 [13000/160000] training loss: 594.3207310438156\n",
            "epoch 15 batch 1400 [14000/160000] training loss: 563.1083700656891\n",
            "epoch 15 batch 1500 [15000/160000] training loss: 540.1670074462891\n",
            "epoch 15 batch 1600 [16000/160000] training loss: 575.3134753704071\n",
            "epoch 15 batch 1700 [17000/160000] training loss: 578.2990374565125\n",
            "epoch 15 batch 1800 [18000/160000] training loss: 566.8788067102432\n",
            "epoch 15 batch 1900 [19000/160000] training loss: 560.4033004045486\n",
            "epoch 15 batch 2000 [20000/160000] training loss: 546.679737329483\n",
            "epoch 15 batch 2100 [21000/160000] training loss: 544.9988415241241\n",
            "epoch 15 batch 2200 [22000/160000] training loss: 589.3394521474838\n",
            "epoch 15 batch 2300 [23000/160000] training loss: 570.4350522756577\n",
            "epoch 15 batch 2400 [24000/160000] training loss: 556.1411609649658\n",
            "epoch 15 batch 2500 [25000/160000] training loss: 591.0155637264252\n",
            "epoch 15 batch 2600 [26000/160000] training loss: 554.0748898983002\n",
            "epoch 15 batch 2700 [27000/160000] training loss: 583.2591032981873\n",
            "epoch 15 batch 2800 [28000/160000] training loss: 531.2828888893127\n",
            "epoch 15 batch 2900 [29000/160000] training loss: 610.4579064846039\n",
            "epoch 15 batch 3000 [30000/160000] training loss: 577.9122123718262\n",
            "epoch 15 batch 3100 [31000/160000] training loss: 584.7273094654083\n",
            "epoch 15 batch 3200 [32000/160000] training loss: 556.6795718669891\n",
            "epoch 15 batch 3300 [33000/160000] training loss: 572.3001861572266\n",
            "epoch 15 batch 3400 [34000/160000] training loss: 571.0017580986023\n",
            "epoch 15 batch 3500 [35000/160000] training loss: 578.792657494545\n",
            "epoch 15 batch 3600 [36000/160000] training loss: 566.3143062591553\n",
            "epoch 15 batch 3700 [37000/160000] training loss: 546.5187965631485\n",
            "epoch 15 batch 3800 [38000/160000] training loss: 562.5738525390625\n",
            "epoch 15 batch 3900 [39000/160000] training loss: 573.6583967208862\n",
            "epoch 15 batch 4000 [40000/160000] training loss: 563.204512834549\n",
            "epoch 15 batch 4100 [41000/160000] training loss: 574.6997330188751\n",
            "epoch 15 batch 4200 [42000/160000] training loss: 592.3190312385559\n",
            "epoch 15 batch 4300 [43000/160000] training loss: 572.7027069330215\n",
            "epoch 15 batch 4400 [44000/160000] training loss: 553.1847676038742\n",
            "epoch 15 batch 4500 [45000/160000] training loss: 576.7645080089569\n",
            "epoch 15 batch 4600 [46000/160000] training loss: 534.960865855217\n",
            "epoch 15 batch 4700 [47000/160000] training loss: 545.2516379356384\n",
            "epoch 15 batch 4800 [48000/160000] training loss: 564.7607464790344\n",
            "epoch 15 batch 4900 [49000/160000] training loss: 567.8527460098267\n",
            "epoch 15 batch 5000 [50000/160000] training loss: 559.3216791152954\n",
            "epoch 15 batch 5100 [51000/160000] training loss: 553.8803790807724\n",
            "epoch 15 batch 5200 [52000/160000] training loss: 567.1634321212769\n",
            "epoch 15 batch 5300 [53000/160000] training loss: 567.1393381357193\n",
            "epoch 15 batch 5400 [54000/160000] training loss: 567.4168410301208\n",
            "epoch 15 batch 5500 [55000/160000] training loss: 555.3552196025848\n",
            "epoch 15 batch 5600 [56000/160000] training loss: 575.5895373821259\n",
            "epoch 15 batch 5700 [57000/160000] training loss: 568.6144018173218\n",
            "epoch 15 batch 5800 [58000/160000] training loss: 567.995793223381\n",
            "epoch 15 batch 5900 [59000/160000] training loss: 543.1407334804535\n",
            "epoch 15 batch 6000 [60000/160000] training loss: 584.4007930755615\n",
            "epoch 15 batch 6100 [61000/160000] training loss: 568.3192229270935\n",
            "epoch 15 batch 6200 [62000/160000] training loss: 590.4745473861694\n",
            "epoch 15 batch 6300 [63000/160000] training loss: 560.6054015159607\n",
            "epoch 15 batch 6400 [64000/160000] training loss: 547.3427817821503\n",
            "epoch 15 batch 6500 [65000/160000] training loss: 558.6464900970459\n",
            "epoch 15 batch 6600 [66000/160000] training loss: 537.0064389705658\n",
            "epoch 15 batch 6700 [67000/160000] training loss: 582.7825642824173\n",
            "epoch 15 batch 6800 [68000/160000] training loss: 538.0177178382874\n",
            "epoch 15 batch 6900 [69000/160000] training loss: 549.1977982521057\n",
            "epoch 15 batch 7000 [70000/160000] training loss: 540.7502205371857\n",
            "epoch 15 batch 7100 [71000/160000] training loss: 531.3069590330124\n",
            "epoch 15 batch 7200 [72000/160000] training loss: 561.6948056221008\n",
            "epoch 15 batch 7300 [73000/160000] training loss: 549.9463942050934\n",
            "epoch 15 batch 7400 [74000/160000] training loss: 541.7560305595398\n",
            "epoch 15 batch 7500 [75000/160000] training loss: 557.7465106248856\n",
            "epoch 15 batch 7600 [76000/160000] training loss: 567.7818400859833\n",
            "epoch 15 batch 7700 [77000/160000] training loss: 587.845198392868\n",
            "epoch 15 batch 7800 [78000/160000] training loss: 588.0733947753906\n",
            "epoch 15 batch 7900 [79000/160000] training loss: 553.8092186450958\n",
            "epoch 15 batch 8000 [80000/160000] training loss: 554.3258481025696\n",
            "epoch 15 batch 8100 [81000/160000] training loss: 544.4305350780487\n",
            "epoch 15 batch 8200 [82000/160000] training loss: 538.1955907344818\n",
            "epoch 15 batch 8300 [83000/160000] training loss: 577.4020887613297\n",
            "epoch 15 batch 8400 [84000/160000] training loss: 573.0649449825287\n",
            "epoch 15 batch 8500 [85000/160000] training loss: 582.284522652626\n",
            "epoch 15 batch 8600 [86000/160000] training loss: 548.739527463913\n",
            "epoch 15 batch 8700 [87000/160000] training loss: 558.3942270278931\n",
            "epoch 15 batch 8800 [88000/160000] training loss: 575.4068827629089\n",
            "epoch 15 batch 8900 [89000/160000] training loss: 571.5248185396194\n",
            "epoch 15 batch 9000 [90000/160000] training loss: 586.723224401474\n",
            "epoch 15 batch 9100 [91000/160000] training loss: 584.4606745243073\n",
            "epoch 15 batch 9200 [92000/160000] training loss: 523.6257570981979\n",
            "epoch 15 batch 9300 [93000/160000] training loss: 578.5471563339233\n",
            "epoch 15 batch 9400 [94000/160000] training loss: 531.8810542821884\n",
            "epoch 15 batch 9500 [95000/160000] training loss: 572.3847163915634\n",
            "epoch 15 batch 9600 [96000/160000] training loss: 554.726175904274\n",
            "epoch 15 batch 9700 [97000/160000] training loss: 553.7373983860016\n",
            "epoch 15 batch 9800 [98000/160000] training loss: 557.1960098743439\n",
            "epoch 15 batch 9900 [99000/160000] training loss: 565.228907585144\n",
            "epoch 15 batch 10000 [100000/160000] training loss: 590.4843065738678\n",
            "epoch 15 batch 10100 [101000/160000] training loss: 569.26806640625\n",
            "epoch 15 batch 10200 [102000/160000] training loss: 539.311708688736\n",
            "epoch 15 batch 10300 [103000/160000] training loss: 569.969188451767\n",
            "epoch 15 batch 10400 [104000/160000] training loss: 569.7606562376022\n",
            "epoch 15 batch 10500 [105000/160000] training loss: 602.3296074867249\n",
            "epoch 15 batch 10600 [106000/160000] training loss: 588.6458436250687\n",
            "epoch 15 batch 10700 [107000/160000] training loss: 561.6138216257095\n",
            "epoch 15 batch 10800 [108000/160000] training loss: 545.1898913383484\n",
            "epoch 15 batch 10900 [109000/160000] training loss: 545.3462746143341\n",
            "epoch 15 batch 11000 [110000/160000] training loss: 565.1317338943481\n",
            "epoch 15 batch 11100 [111000/160000] training loss: 552.2713582515717\n",
            "epoch 15 batch 11200 [112000/160000] training loss: 569.1512145996094\n",
            "epoch 15 batch 11300 [113000/160000] training loss: 554.7461622953415\n",
            "epoch 15 batch 11400 [114000/160000] training loss: 569.2373621463776\n",
            "epoch 15 batch 11500 [115000/160000] training loss: 571.6171860694885\n",
            "epoch 15 batch 11600 [116000/160000] training loss: 592.0012347698212\n",
            "epoch 15 batch 11700 [117000/160000] training loss: 542.4541630744934\n",
            "epoch 15 batch 11800 [118000/160000] training loss: 552.8281148672104\n",
            "epoch 15 batch 11900 [119000/160000] training loss: 547.3318009376526\n",
            "epoch 15 batch 12000 [120000/160000] training loss: 566.532084107399\n",
            "epoch 15 batch 12100 [121000/160000] training loss: 588.8129496574402\n",
            "epoch 15 batch 12200 [122000/160000] training loss: 555.1814105510712\n",
            "epoch 15 batch 12300 [123000/160000] training loss: 564.6299538612366\n",
            "epoch 15 batch 12400 [124000/160000] training loss: 570.9062333106995\n",
            "epoch 15 batch 12500 [125000/160000] training loss: 553.7293033599854\n",
            "epoch 15 batch 12600 [126000/160000] training loss: 568.5470416545868\n",
            "epoch 15 batch 12700 [127000/160000] training loss: 551.5468555688858\n",
            "epoch 15 batch 12800 [128000/160000] training loss: 578.5881514549255\n",
            "epoch 15 batch 12900 [129000/160000] training loss: 576.8948121070862\n",
            "epoch 15 batch 13000 [130000/160000] training loss: 585.1216897964478\n",
            "epoch 15 batch 13100 [131000/160000] training loss: 563.4598202705383\n",
            "epoch 15 batch 13200 [132000/160000] training loss: 592.6251935958862\n",
            "epoch 15 batch 13300 [133000/160000] training loss: 572.3470997810364\n",
            "epoch 15 batch 13400 [134000/160000] training loss: 547.6834726333618\n",
            "epoch 15 batch 13500 [135000/160000] training loss: 563.7836811542511\n",
            "epoch 15 batch 13600 [136000/160000] training loss: 541.2579622268677\n",
            "epoch 15 batch 13700 [137000/160000] training loss: 598.864022731781\n",
            "epoch 15 batch 13800 [138000/160000] training loss: 580.8826727867126\n",
            "epoch 15 batch 13900 [139000/160000] training loss: 598.8016638755798\n",
            "epoch 15 batch 14000 [140000/160000] training loss: 569.8993995189667\n",
            "epoch 15 batch 14100 [141000/160000] training loss: 547.1810970306396\n",
            "epoch 15 batch 14200 [142000/160000] training loss: 566.6448154449463\n",
            "epoch 15 batch 14300 [143000/160000] training loss: 565.3114030361176\n",
            "epoch 15 batch 14400 [144000/160000] training loss: 581.4185135364532\n",
            "epoch 15 batch 14500 [145000/160000] training loss: 566.1535835266113\n",
            "epoch 15 batch 14600 [146000/160000] training loss: 561.3656030893326\n",
            "epoch 15 batch 14700 [147000/160000] training loss: 554.2309186458588\n",
            "epoch 15 batch 14800 [148000/160000] training loss: 567.4405232667923\n",
            "epoch 15 batch 14900 [149000/160000] training loss: 555.9465463161469\n",
            "epoch 15 batch 15000 [150000/160000] training loss: 598.8166806697845\n",
            "epoch 15 batch 15100 [151000/160000] training loss: 570.0319426059723\n",
            "epoch 15 batch 15200 [152000/160000] training loss: 600.5450403690338\n",
            "epoch 15 batch 15300 [153000/160000] training loss: 564.602373123169\n",
            "epoch 15 batch 15400 [154000/160000] training loss: 587.1574296951294\n",
            "epoch 15 batch 15500 [155000/160000] training loss: 555.3225784301758\n",
            "epoch 15 batch 15600 [156000/160000] training loss: 537.8179240226746\n",
            "epoch 15 batch 15700 [157000/160000] training loss: 546.7482198476791\n",
            "epoch 15 batch 15800 [158000/160000] training loss: 559.9589691162109\n",
            "epoch 15 batch 15900 [159000/160000] training loss: 546.0366401672363\n",
            "epoch 16 batch 0 [0/160000] training loss: 5.510761260986328\n",
            "epoch 16 batch 100 [1000/160000] training loss: 570.9983528852463\n",
            "epoch 16 batch 200 [2000/160000] training loss: 573.4475666284561\n",
            "epoch 16 batch 300 [3000/160000] training loss: 569.5318696498871\n",
            "epoch 16 batch 400 [4000/160000] training loss: 574.5827629566193\n",
            "epoch 16 batch 500 [5000/160000] training loss: 579.2508687973022\n",
            "epoch 16 batch 600 [6000/160000] training loss: 566.5157103538513\n",
            "epoch 16 batch 700 [7000/160000] training loss: 552.2904498577118\n",
            "epoch 16 batch 800 [8000/160000] training loss: 586.5404877662659\n",
            "epoch 16 batch 900 [9000/160000] training loss: 572.0481171607971\n",
            "epoch 16 batch 1000 [10000/160000] training loss: 580.0197954177856\n",
            "epoch 16 batch 1100 [11000/160000] training loss: 557.1530705690384\n",
            "epoch 16 batch 1200 [12000/160000] training loss: 555.842479467392\n",
            "epoch 16 batch 1300 [13000/160000] training loss: 594.0762708187103\n",
            "epoch 16 batch 1400 [14000/160000] training loss: 563.2053807973862\n",
            "epoch 16 batch 1500 [15000/160000] training loss: 539.9635689258575\n",
            "epoch 16 batch 1600 [16000/160000] training loss: 574.9177680015564\n",
            "epoch 16 batch 1700 [17000/160000] training loss: 577.7670304775238\n",
            "epoch 16 batch 1800 [18000/160000] training loss: 566.6197986602783\n",
            "epoch 16 batch 1900 [19000/160000] training loss: 560.0770350694656\n",
            "epoch 16 batch 2000 [20000/160000] training loss: 546.0796999931335\n",
            "epoch 16 batch 2100 [21000/160000] training loss: 545.1149345636368\n",
            "epoch 16 batch 2200 [22000/160000] training loss: 589.3230863809586\n",
            "epoch 16 batch 2300 [23000/160000] training loss: 570.2278573513031\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a2fbf055905c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassif_trainer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-babb10648985>\u001b[0m in \u001b[0;36mclassif_trainer_2\u001b[0;34m(final_model, num_epoch, max_norm, epoch_b4_stopping, epoch_losses, classif_layer_only, clipping)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0;31m# compute batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss_classif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0;31m# Compute clipped gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-c657b9ada8a2>\u001b[0m in \u001b[0;36mbatch_loss_classif\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mvotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-00c90eff0da5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mconversation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msentence_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_layer_encoders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcorpus_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-00c90eff0da5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mconversation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msentence_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_layer_encoders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcorpus_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d6e1a8a6ac88>\u001b[0m in \u001b[0;36membedder\u001b[0;34m(token_ids)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfastText_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-d6e1a8a6ac88>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfastText_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m   1984\u001b[0m                 \u001b[0mngram_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ft_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mngram_hash\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash2index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m                     \u001b[0mword_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mngram_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m                     \u001b[0mngrams_found\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /!\\ run this cell if training was interrupted by colab /\n",
        "with open('./drive/MyDrive/final_training_params_saving_no_pretraining', 'rb') as f:\n",
        "  final_training_params_saving = pickle.load(f)\n",
        "\n",
        "final_model = torch.load('./drive/MyDrive/final_model_no_pretraining')\n",
        "classif_trainer_2(final_model, num_epoch = 30, epoch_b4_stopping = final_training_params_saving['epoch_b4_stopping'],\n",
        "                epoch_losses = final_training_params_saving['epoch_losses'], classif_layer_only = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5c53603b1a09459dbbdd21a089c81567",
            "b06cf1b9bc9b4563bf19895cf284d115",
            "15f5074dc19a478f808cf43118b322c9",
            "85ff2ab9e8b842d49203e59a4f5aa119",
            "d45a7078abb740ffa6579007a8e15ac1",
            "d17364cbfc6a40ad88574efb3cb7f1fa",
            "deb6c143365d42a19d82de7320142e56",
            "3512e41fd7c345789574d8f7fbed7200",
            "aff798a186694d679c9f4f069a798f69",
            "831d920986d54f8788f5feb323907cb0",
            "02b7d169840d4ecbbbc5779dc2a0be3f"
          ]
        },
        "id": "cXomWTveUt5E",
        "outputId": "ec668d60-9084-47b9-bc96-3b89f9adc21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c53603b1a09459dbbdd21a089c81567"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16 batch 0 [0/160000] training loss: 3.081878185272217\n",
            "epoch 16 batch 100 [1000/160000] training loss: 573.7484266757965\n",
            "epoch 16 batch 200 [2000/160000] training loss: 579.3174465894699\n",
            "epoch 16 batch 300 [3000/160000] training loss: 566.7664355039597\n",
            "epoch 16 batch 400 [4000/160000] training loss: 543.5041004419327\n",
            "epoch 16 batch 500 [5000/160000] training loss: 576.3152234554291\n",
            "epoch 16 batch 600 [6000/160000] training loss: 568.8788695335388\n",
            "epoch 16 batch 700 [7000/160000] training loss: 568.49640583992\n",
            "epoch 16 batch 800 [8000/160000] training loss: 527.4498383998871\n",
            "epoch 16 batch 900 [9000/160000] training loss: 522.7220499515533\n",
            "epoch 16 batch 1000 [10000/160000] training loss: 576.153591632843\n",
            "epoch 16 batch 1100 [11000/160000] training loss: 557.0251854658127\n",
            "epoch 16 batch 1200 [12000/160000] training loss: 555.9404444694519\n",
            "epoch 16 batch 1300 [13000/160000] training loss: 557.6600561141968\n",
            "epoch 16 batch 1400 [14000/160000] training loss: 564.1210851669312\n",
            "epoch 16 batch 1500 [15000/160000] training loss: 578.9023993015289\n",
            "epoch 16 batch 1600 [16000/160000] training loss: 550.4369515180588\n",
            "epoch 16 batch 1700 [17000/160000] training loss: 563.6906492710114\n",
            "epoch 16 batch 1800 [18000/160000] training loss: 565.929696559906\n",
            "epoch 16 batch 1900 [19000/160000] training loss: 539.2397899627686\n",
            "epoch 16 batch 2000 [20000/160000] training loss: 601.2490150928497\n",
            "epoch 16 batch 2100 [21000/160000] training loss: 559.9213490486145\n",
            "epoch 16 batch 2200 [22000/160000] training loss: 571.1583946943283\n",
            "epoch 16 batch 2300 [23000/160000] training loss: 566.0671520233154\n",
            "epoch 16 batch 2400 [24000/160000] training loss: 579.7227284908295\n",
            "epoch 16 batch 2500 [25000/160000] training loss: 546.9265730381012\n",
            "epoch 16 batch 2600 [26000/160000] training loss: 562.581665635109\n",
            "epoch 16 batch 2700 [27000/160000] training loss: 550.0959174633026\n",
            "epoch 16 batch 2800 [28000/160000] training loss: 609.0870621204376\n",
            "epoch 16 batch 2900 [29000/160000] training loss: 590.9651930332184\n",
            "epoch 16 batch 3000 [30000/160000] training loss: 548.2641544342041\n",
            "epoch 16 batch 3100 [31000/160000] training loss: 561.1752568483353\n",
            "epoch 16 batch 3200 [32000/160000] training loss: 578.935227394104\n",
            "epoch 16 batch 3300 [33000/160000] training loss: 566.929502248764\n",
            "epoch 16 batch 3400 [34000/160000] training loss: 532.4550549983978\n",
            "epoch 16 batch 3500 [35000/160000] training loss: 538.8738169670105\n",
            "epoch 16 batch 3600 [36000/160000] training loss: 565.5282647609711\n",
            "epoch 16 batch 3700 [37000/160000] training loss: 572.436551451683\n",
            "epoch 16 batch 3800 [38000/160000] training loss: 532.7050824165344\n",
            "epoch 16 batch 3900 [39000/160000] training loss: 572.2591366767883\n",
            "epoch 16 batch 4000 [40000/160000] training loss: 587.5188901424408\n",
            "epoch 16 batch 4100 [41000/160000] training loss: 573.3526520729065\n",
            "epoch 16 batch 4200 [42000/160000] training loss: 546.4723598957062\n",
            "epoch 16 batch 4300 [43000/160000] training loss: 554.3942275047302\n",
            "epoch 16 batch 4400 [44000/160000] training loss: 528.216629743576\n",
            "epoch 16 batch 4500 [45000/160000] training loss: 578.1601166725159\n",
            "epoch 16 batch 4600 [46000/160000] training loss: 563.5392293930054\n",
            "epoch 16 batch 4700 [47000/160000] training loss: 592.9237625598907\n",
            "epoch 16 batch 4800 [48000/160000] training loss: 571.6950500011444\n",
            "epoch 16 batch 4900 [49000/160000] training loss: 547.0405423641205\n",
            "epoch 16 batch 5000 [50000/160000] training loss: 616.7821912765503\n",
            "epoch 16 batch 5100 [51000/160000] training loss: 555.1352212429047\n",
            "epoch 16 batch 5200 [52000/160000] training loss: 553.2215782403946\n",
            "epoch 16 batch 5300 [53000/160000] training loss: 553.9576565027237\n",
            "epoch 16 batch 5400 [54000/160000] training loss: 530.6690194606781\n",
            "epoch 16 batch 5500 [55000/160000] training loss: 554.0305769443512\n",
            "epoch 16 batch 5600 [56000/160000] training loss: 575.554719209671\n",
            "epoch 16 batch 5700 [57000/160000] training loss: 576.5024609565735\n",
            "epoch 16 batch 5800 [58000/160000] training loss: 568.0523219108582\n",
            "epoch 16 batch 5900 [59000/160000] training loss: 562.2138497829437\n",
            "epoch 16 batch 6000 [60000/160000] training loss: 568.9485809803009\n",
            "epoch 16 batch 6100 [61000/160000] training loss: 568.7441790103912\n",
            "epoch 16 batch 6200 [62000/160000] training loss: 567.5838508605957\n",
            "epoch 16 batch 6300 [63000/160000] training loss: 555.9944043159485\n",
            "epoch 16 batch 6400 [64000/160000] training loss: 567.2543065547943\n",
            "epoch 16 batch 6500 [65000/160000] training loss: 582.5178246498108\n",
            "epoch 16 batch 6600 [66000/160000] training loss: 540.8175065517426\n",
            "epoch 16 batch 6700 [67000/160000] training loss: 561.6899673938751\n",
            "epoch 16 batch 6800 [68000/160000] training loss: 572.5516221523285\n",
            "epoch 16 batch 6900 [69000/160000] training loss: 558.5921883583069\n",
            "epoch 16 batch 7000 [70000/160000] training loss: 541.1543287038803\n",
            "epoch 16 batch 7100 [71000/160000] training loss: 591.5880700349808\n",
            "epoch 16 batch 7200 [72000/160000] training loss: 586.0818297863007\n",
            "epoch 16 batch 7300 [73000/160000] training loss: 572.2834558486938\n",
            "epoch 16 batch 7400 [74000/160000] training loss: 549.8249309062958\n",
            "epoch 16 batch 7500 [75000/160000] training loss: 555.8148148059845\n",
            "epoch 16 batch 7600 [76000/160000] training loss: 567.3440744876862\n",
            "epoch 16 batch 7700 [77000/160000] training loss: 573.1539698839188\n",
            "epoch 16 batch 7800 [78000/160000] training loss: 576.2273149490356\n",
            "epoch 16 batch 7900 [79000/160000] training loss: 570.5863790512085\n",
            "epoch 16 batch 8000 [80000/160000] training loss: 574.7002322673798\n",
            "epoch 16 batch 8100 [81000/160000] training loss: 554.6154706478119\n",
            "epoch 16 batch 8200 [82000/160000] training loss: 582.958368062973\n",
            "epoch 16 batch 8300 [83000/160000] training loss: 577.8356481790543\n",
            "epoch 16 batch 8400 [84000/160000] training loss: 567.6843576431274\n",
            "epoch 16 batch 8500 [85000/160000] training loss: 576.3681263923645\n",
            "epoch 16 batch 8600 [86000/160000] training loss: 585.2430379390717\n",
            "epoch 16 batch 8700 [87000/160000] training loss: 553.1837005615234\n",
            "epoch 16 batch 8800 [88000/160000] training loss: 553.1753025054932\n",
            "epoch 16 batch 8900 [89000/160000] training loss: 557.6501324176788\n",
            "epoch 16 batch 9000 [90000/160000] training loss: 578.2743191719055\n",
            "epoch 16 batch 9100 [91000/160000] training loss: 545.9756915569305\n",
            "epoch 16 batch 9200 [92000/160000] training loss: 589.2861132621765\n",
            "epoch 16 batch 9300 [93000/160000] training loss: 554.6540446281433\n",
            "epoch 16 batch 9400 [94000/160000] training loss: 542.9558560848236\n",
            "epoch 16 batch 9500 [95000/160000] training loss: 586.6669435501099\n",
            "epoch 16 batch 9600 [96000/160000] training loss: 554.651026725769\n",
            "epoch 16 batch 9700 [97000/160000] training loss: 560.9668941497803\n",
            "epoch 16 batch 9800 [98000/160000] training loss: 556.2991428375244\n",
            "epoch 16 batch 9900 [99000/160000] training loss: 586.8765952587128\n",
            "epoch 16 batch 10000 [100000/160000] training loss: 559.0242080688477\n",
            "epoch 16 batch 10100 [101000/160000] training loss: 584.3066759109497\n",
            "epoch 16 batch 10200 [102000/160000] training loss: 559.2570848464966\n",
            "epoch 16 batch 10300 [103000/160000] training loss: 558.3191139698029\n",
            "epoch 16 batch 10400 [104000/160000] training loss: 550.3553075790405\n",
            "epoch 16 batch 10500 [105000/160000] training loss: 556.7054526805878\n",
            "epoch 16 batch 10600 [106000/160000] training loss: 555.6966903209686\n",
            "epoch 16 batch 10700 [107000/160000] training loss: 534.5523509979248\n",
            "epoch 16 batch 10800 [108000/160000] training loss: 594.0350016355515\n",
            "epoch 16 batch 10900 [109000/160000] training loss: 556.854464173317\n",
            "epoch 16 batch 11000 [110000/160000] training loss: 562.9920518398285\n",
            "epoch 16 batch 11100 [111000/160000] training loss: 551.2686150074005\n",
            "epoch 16 batch 11200 [112000/160000] training loss: 553.322559595108\n",
            "epoch 16 batch 11300 [113000/160000] training loss: 532.8075108528137\n",
            "epoch 16 batch 11400 [114000/160000] training loss: 584.9413938522339\n",
            "epoch 16 batch 11500 [115000/160000] training loss: 573.2129709720612\n",
            "epoch 16 batch 11600 [116000/160000] training loss: 580.0554175376892\n",
            "epoch 16 batch 11700 [117000/160000] training loss: 543.8247685432434\n",
            "epoch 16 batch 11800 [118000/160000] training loss: 563.041820526123\n",
            "epoch 16 batch 11900 [119000/160000] training loss: 559.261302947998\n",
            "epoch 16 batch 12000 [120000/160000] training loss: 610.3615865707397\n",
            "epoch 16 batch 12100 [121000/160000] training loss: 546.9408462047577\n",
            "epoch 16 batch 12200 [122000/160000] training loss: 585.4150865077972\n",
            "epoch 16 batch 12300 [123000/160000] training loss: 575.1645497083664\n",
            "epoch 16 batch 12400 [124000/160000] training loss: 583.2996742725372\n",
            "epoch 16 batch 12500 [125000/160000] training loss: 583.8220281600952\n",
            "epoch 16 batch 12600 [126000/160000] training loss: 576.7025532722473\n",
            "epoch 16 batch 12700 [127000/160000] training loss: 579.7612736225128\n",
            "epoch 16 batch 12800 [128000/160000] training loss: 557.4952154159546\n",
            "epoch 16 batch 12900 [129000/160000] training loss: 569.7770991325378\n",
            "epoch 16 batch 13000 [130000/160000] training loss: 567.2895746231079\n",
            "epoch 16 batch 13100 [131000/160000] training loss: 592.1558222770691\n",
            "epoch 16 batch 13200 [132000/160000] training loss: 567.1540024280548\n",
            "epoch 16 batch 13300 [133000/160000] training loss: 572.0159692764282\n",
            "epoch 16 batch 13400 [134000/160000] training loss: 572.4695675373077\n",
            "epoch 16 batch 13500 [135000/160000] training loss: 549.9189269542694\n",
            "epoch 16 batch 13600 [136000/160000] training loss: 564.0541417598724\n",
            "epoch 16 batch 13700 [137000/160000] training loss: 569.2021520137787\n",
            "epoch 16 batch 13800 [138000/160000] training loss: 579.1389496326447\n",
            "epoch 16 batch 13900 [139000/160000] training loss: 579.779990196228\n",
            "epoch 16 batch 14000 [140000/160000] training loss: 587.1746037006378\n",
            "epoch 16 batch 14100 [141000/160000] training loss: 604.7290728092194\n",
            "epoch 16 batch 14200 [142000/160000] training loss: 600.9634857177734\n",
            "epoch 16 batch 14300 [143000/160000] training loss: 596.8030033111572\n",
            "epoch 16 batch 14400 [144000/160000] training loss: 555.3238005638123\n",
            "epoch 16 batch 14500 [145000/160000] training loss: 589.8971761465073\n",
            "epoch 16 batch 14600 [146000/160000] training loss: 558.132693529129\n",
            "epoch 16 batch 14700 [147000/160000] training loss: 560.6060082912445\n",
            "epoch 16 batch 14800 [148000/160000] training loss: 564.5599298477173\n",
            "epoch 16 batch 14900 [149000/160000] training loss: 541.0813775062561\n",
            "epoch 16 batch 15000 [150000/160000] training loss: 553.3849648237228\n",
            "epoch 16 batch 15100 [151000/160000] training loss: 560.171993970871\n",
            "epoch 16 batch 15200 [152000/160000] training loss: 548.4749981164932\n",
            "epoch 16 batch 15300 [153000/160000] training loss: 554.4703396558762\n",
            "epoch 16 batch 15400 [154000/160000] training loss: 566.8633694648743\n",
            "epoch 16 batch 15500 [155000/160000] training loss: 525.1567599773407\n",
            "epoch 16 batch 15600 [156000/160000] training loss: 599.2688866853714\n",
            "epoch 16 batch 15700 [157000/160000] training loss: 551.4242068529129\n",
            "epoch 16 batch 15800 [158000/160000] training loss: 569.5334812402725\n",
            "epoch 16 batch 15900 [159000/160000] training loss: 549.4569017887115\n",
            "epoch 17 batch 0 [0/160000] training loss: 6.665769100189209\n",
            "epoch 17 batch 100 [1000/160000] training loss: 557.8186094760895\n",
            "epoch 17 batch 200 [2000/160000] training loss: 561.0552041530609\n",
            "epoch 17 batch 300 [3000/160000] training loss: 557.7981994152069\n",
            "epoch 17 batch 400 [4000/160000] training loss: 566.4305589199066\n",
            "epoch 17 batch 500 [5000/160000] training loss: 560.4507236480713\n",
            "epoch 17 batch 600 [6000/160000] training loss: 554.8420149087906\n",
            "epoch 17 batch 700 [7000/160000] training loss: 543.3924126625061\n",
            "epoch 17 batch 800 [8000/160000] training loss: 578.3495328426361\n",
            "epoch 17 batch 900 [9000/160000] training loss: 569.2390706539154\n",
            "epoch 17 batch 1000 [10000/160000] training loss: 572.895192027092\n",
            "epoch 17 batch 1100 [11000/160000] training loss: 546.3851351737976\n",
            "epoch 17 batch 1200 [12000/160000] training loss: 549.9676778316498\n",
            "epoch 17 batch 1300 [13000/160000] training loss: 584.1065956354141\n",
            "epoch 17 batch 1400 [14000/160000] training loss: 555.2418005466461\n",
            "epoch 17 batch 1500 [15000/160000] training loss: 536.6527111530304\n",
            "epoch 17 batch 1600 [16000/160000] training loss: 568.1216349601746\n",
            "epoch 17 batch 1700 [17000/160000] training loss: 564.0276017189026\n",
            "epoch 17 batch 1800 [18000/160000] training loss: 556.378783583641\n",
            "epoch 17 batch 1900 [19000/160000] training loss: 552.9266104698181\n",
            "epoch 17 batch 2000 [20000/160000] training loss: 536.6614689826965\n",
            "epoch 17 batch 2100 [21000/160000] training loss: 545.8484119176865\n",
            "epoch 17 batch 2200 [22000/160000] training loss: 588.0609471797943\n",
            "epoch 17 batch 2300 [23000/160000] training loss: 571.4166576862335\n",
            "epoch 17 batch 2400 [24000/160000] training loss: 549.288444519043\n",
            "epoch 17 batch 2500 [25000/160000] training loss: 585.8788621425629\n",
            "epoch 17 batch 2600 [26000/160000] training loss: 553.148365020752\n",
            "epoch 17 batch 2700 [27000/160000] training loss: 582.7757606506348\n",
            "epoch 17 batch 2800 [28000/160000] training loss: 526.5472868680954\n",
            "epoch 17 batch 2900 [29000/160000] training loss: 608.4434043169022\n",
            "epoch 17 batch 3000 [30000/160000] training loss: 571.9453299045563\n",
            "epoch 17 batch 3100 [31000/160000] training loss: 580.783460855484\n",
            "epoch 17 batch 3200 [32000/160000] training loss: 555.1583664417267\n",
            "epoch 17 batch 3300 [33000/160000] training loss: 574.0524847507477\n",
            "epoch 17 batch 3400 [34000/160000] training loss: 571.2291786670685\n",
            "epoch 17 batch 3500 [35000/160000] training loss: 578.080603480339\n",
            "epoch 17 batch 3600 [36000/160000] training loss: 562.0684745311737\n",
            "epoch 17 batch 3700 [37000/160000] training loss: 543.5507575273514\n",
            "epoch 17 batch 3800 [38000/160000] training loss: 563.6925842761993\n",
            "epoch 17 batch 3900 [39000/160000] training loss: 576.1370401382446\n",
            "epoch 17 batch 4000 [40000/160000] training loss: 562.117516040802\n",
            "epoch 17 batch 4100 [41000/160000] training loss: 575.0507102012634\n",
            "epoch 17 batch 4200 [42000/160000] training loss: 593.07856798172\n",
            "epoch 17 batch 4300 [43000/160000] training loss: 573.226361989975\n",
            "epoch 17 batch 4400 [44000/160000] training loss: 554.6915806531906\n",
            "epoch 17 batch 4500 [45000/160000] training loss: 574.34264087677\n",
            "epoch 17 batch 4600 [46000/160000] training loss: 538.4420907497406\n",
            "epoch 17 batch 4700 [47000/160000] training loss: 545.7146961688995\n",
            "epoch 17 batch 4800 [48000/160000] training loss: 564.3140392303467\n",
            "epoch 17 batch 4900 [49000/160000] training loss: 569.0206875801086\n",
            "epoch 17 batch 5000 [50000/160000] training loss: 557.3810319900513\n",
            "epoch 17 batch 5100 [51000/160000] training loss: 548.8678398132324\n",
            "epoch 17 batch 5200 [52000/160000] training loss: 568.336354970932\n",
            "epoch 17 batch 5300 [53000/160000] training loss: 567.179055929184\n",
            "epoch 17 batch 5400 [54000/160000] training loss: 565.7796974182129\n",
            "epoch 17 batch 5500 [55000/160000] training loss: 557.6140201091766\n",
            "epoch 17 batch 5600 [56000/160000] training loss: 574.4227902889252\n",
            "epoch 17 batch 5700 [57000/160000] training loss: 567.5165576934814\n",
            "epoch 17 batch 5800 [58000/160000] training loss: 567.9259679317474\n",
            "epoch 17 batch 5900 [59000/160000] training loss: 542.3666151762009\n",
            "epoch 17 batch 6000 [60000/160000] training loss: 584.3136755228043\n",
            "epoch 17 batch 6100 [61000/160000] training loss: 570.2384217977524\n",
            "epoch 17 batch 6200 [62000/160000] training loss: 590.7365324497223\n",
            "epoch 17 batch 6300 [63000/160000] training loss: 562.9746930599213\n",
            "epoch 17 batch 6400 [64000/160000] training loss: 548.805073261261\n",
            "epoch 17 batch 6500 [65000/160000] training loss: 560.5827014446259\n",
            "epoch 17 batch 6600 [66000/160000] training loss: 535.3475284576416\n",
            "epoch 17 batch 6700 [67000/160000] training loss: 581.202312707901\n",
            "epoch 17 batch 6800 [68000/160000] training loss: 535.3321549892426\n",
            "epoch 17 batch 6900 [69000/160000] training loss: 549.3171803951263\n",
            "epoch 17 batch 7000 [70000/160000] training loss: 537.1703007221222\n",
            "epoch 17 batch 7100 [71000/160000] training loss: 531.9576811790466\n",
            "epoch 17 batch 7200 [72000/160000] training loss: 564.2703764438629\n",
            "epoch 17 batch 7300 [73000/160000] training loss: 549.4986953735352\n",
            "epoch 17 batch 7400 [74000/160000] training loss: 539.5255746841431\n",
            "epoch 17 batch 7500 [75000/160000] training loss: 558.7741178274155\n",
            "epoch 17 batch 7600 [76000/160000] training loss: 568.3150641918182\n",
            "epoch 17 batch 7700 [77000/160000] training loss: 587.794600725174\n",
            "epoch 17 batch 7800 [78000/160000] training loss: 585.3306515216827\n",
            "epoch 17 batch 7900 [79000/160000] training loss: 553.262709736824\n",
            "epoch 17 batch 8000 [80000/160000] training loss: 553.7619371414185\n",
            "epoch 17 batch 8100 [81000/160000] training loss: 542.6461944580078\n",
            "epoch 17 batch 8200 [82000/160000] training loss: 537.01433634758\n",
            "epoch 17 batch 8300 [83000/160000] training loss: 577.7991557121277\n",
            "epoch 17 batch 8400 [84000/160000] training loss: 571.8678064346313\n",
            "epoch 17 batch 8500 [85000/160000] training loss: 582.5337985754013\n",
            "epoch 17 batch 8600 [86000/160000] training loss: 549.0757801532745\n",
            "epoch 17 batch 8700 [87000/160000] training loss: 559.8002824783325\n",
            "epoch 17 batch 8800 [88000/160000] training loss: 575.4533822536469\n",
            "epoch 17 batch 8900 [89000/160000] training loss: 570.2936679124832\n",
            "epoch 17 batch 9000 [90000/160000] training loss: 587.4303562641144\n",
            "epoch 17 batch 9100 [91000/160000] training loss: 584.9018204212189\n",
            "epoch 17 batch 9200 [92000/160000] training loss: 523.777506351471\n",
            "epoch 17 batch 9300 [93000/160000] training loss: 580.1518032550812\n",
            "epoch 17 batch 9400 [94000/160000] training loss: 531.1077386140823\n",
            "epoch 17 batch 9500 [95000/160000] training loss: 574.1641628742218\n",
            "epoch 17 batch 9600 [96000/160000] training loss: 556.9201203584671\n",
            "epoch 17 batch 9700 [97000/160000] training loss: 552.9774544239044\n",
            "epoch 17 batch 9800 [98000/160000] training loss: 554.5196344852448\n",
            "epoch 17 batch 9900 [99000/160000] training loss: 562.6482248306274\n",
            "epoch 17 batch 10000 [100000/160000] training loss: 588.5697169303894\n",
            "epoch 17 batch 10100 [101000/160000] training loss: 572.6545920372009\n",
            "epoch 17 batch 10200 [102000/160000] training loss: 540.6716214418411\n",
            "epoch 17 batch 10300 [103000/160000] training loss: 573.6983751058578\n",
            "epoch 17 batch 10400 [104000/160000] training loss: 571.3795145750046\n",
            "epoch 17 batch 10500 [105000/160000] training loss: 602.9794445037842\n",
            "epoch 17 batch 10600 [106000/160000] training loss: 589.2101273536682\n",
            "epoch 17 batch 10700 [107000/160000] training loss: 558.897785782814\n",
            "epoch 17 batch 10800 [108000/160000] training loss: 545.2397198677063\n",
            "epoch 17 batch 10900 [109000/160000] training loss: 547.470136642456\n",
            "epoch 17 batch 11000 [110000/160000] training loss: 564.5200071334839\n",
            "epoch 17 batch 11100 [111000/160000] training loss: 553.6416492462158\n",
            "epoch 17 batch 11200 [112000/160000] training loss: 567.0931171178818\n",
            "epoch 17 batch 11300 [113000/160000] training loss: 554.2968820333481\n",
            "epoch 17 batch 11400 [114000/160000] training loss: 569.1014845371246\n",
            "epoch 17 batch 11500 [115000/160000] training loss: 569.8379292488098\n",
            "epoch 17 batch 11600 [116000/160000] training loss: 595.2766561508179\n",
            "epoch 17 batch 11700 [117000/160000] training loss: 540.7645642757416\n",
            "epoch 17 batch 11800 [118000/160000] training loss: 553.25909948349\n",
            "epoch 17 batch 11900 [119000/160000] training loss: 546.6109212636948\n",
            "epoch 17 batch 12000 [120000/160000] training loss: 565.9059666395187\n",
            "epoch 17 batch 12100 [121000/160000] training loss: 589.6271262168884\n",
            "epoch 17 batch 12200 [122000/160000] training loss: 554.1833848953247\n",
            "epoch 17 batch 12300 [123000/160000] training loss: 563.2729864120483\n",
            "epoch 17 batch 12400 [124000/160000] training loss: 571.2200094461441\n",
            "epoch 17 batch 12500 [125000/160000] training loss: 557.4293156862259\n",
            "epoch 17 batch 12600 [126000/160000] training loss: 568.6472407579422\n",
            "epoch 17 batch 12700 [127000/160000] training loss: 552.2936717271805\n",
            "epoch 17 batch 12800 [128000/160000] training loss: 578.9930317401886\n",
            "epoch 17 batch 12900 [129000/160000] training loss: 575.5377633571625\n",
            "epoch 17 batch 13000 [130000/160000] training loss: 584.4963688850403\n",
            "epoch 17 batch 13100 [131000/160000] training loss: 563.5794186592102\n",
            "epoch 17 batch 13200 [132000/160000] training loss: 591.917616724968\n",
            "epoch 17 batch 13300 [133000/160000] training loss: 574.4868659973145\n",
            "epoch 17 batch 13400 [134000/160000] training loss: 546.731508731842\n",
            "epoch 17 batch 13500 [135000/160000] training loss: 563.8031210899353\n",
            "epoch 17 batch 13600 [136000/160000] training loss: 542.330305814743\n",
            "epoch 17 batch 13700 [137000/160000] training loss: 600.645032286644\n",
            "epoch 17 batch 13800 [138000/160000] training loss: 580.999338388443\n",
            "epoch 17 batch 13900 [139000/160000] training loss: 600.5250058174133\n",
            "epoch 17 batch 14000 [140000/160000] training loss: 571.0233939886093\n",
            "epoch 17 batch 14100 [141000/160000] training loss: 545.5889505147934\n",
            "epoch 17 batch 14200 [142000/160000] training loss: 564.5594341754913\n",
            "epoch 17 batch 14300 [143000/160000] training loss: 564.1959826946259\n",
            "epoch 17 batch 14400 [144000/160000] training loss: 583.011260509491\n",
            "epoch 17 batch 14500 [145000/160000] training loss: 566.1928007602692\n",
            "epoch 17 batch 14600 [146000/160000] training loss: 558.889040350914\n",
            "epoch 17 batch 14700 [147000/160000] training loss: 555.1835985183716\n",
            "epoch 17 batch 14800 [148000/160000] training loss: 565.5432045459747\n",
            "epoch 17 batch 14900 [149000/160000] training loss: 557.2326173782349\n",
            "epoch 17 batch 15000 [150000/160000] training loss: 597.6123547554016\n",
            "epoch 17 batch 15100 [151000/160000] training loss: 570.5723958015442\n",
            "epoch 17 batch 15200 [152000/160000] training loss: 599.9800183773041\n",
            "epoch 17 batch 15300 [153000/160000] training loss: 562.8899087905884\n",
            "epoch 17 batch 15400 [154000/160000] training loss: 586.2850961685181\n",
            "epoch 17 batch 15500 [155000/160000] training loss: 556.0190453529358\n",
            "epoch 17 batch 15600 [156000/160000] training loss: 539.5383553504944\n",
            "epoch 17 batch 15700 [157000/160000] training loss: 543.7823246717453\n",
            "epoch 17 batch 15800 [158000/160000] training loss: 562.0973646640778\n",
            "epoch 17 batch 15900 [159000/160000] training loss: 549.1232674121857\n",
            "epoch 18 batch 0 [0/160000] training loss: 5.483733177185059\n",
            "epoch 18 batch 100 [1000/160000] training loss: 566.1848905086517\n",
            "epoch 18 batch 200 [2000/160000] training loss: 568.3546357154846\n",
            "epoch 18 batch 300 [3000/160000] training loss: 565.2146887779236\n",
            "epoch 18 batch 400 [4000/160000] training loss: 569.6579523086548\n",
            "epoch 18 batch 500 [5000/160000] training loss: 573.1513514518738\n",
            "epoch 18 batch 600 [6000/160000] training loss: 560.2711240053177\n",
            "epoch 18 batch 700 [7000/160000] training loss: 547.5379076004028\n",
            "epoch 18 batch 800 [8000/160000] training loss: 582.0654970407486\n",
            "epoch 18 batch 900 [9000/160000] training loss: 567.7374349832535\n",
            "epoch 18 batch 1000 [10000/160000] training loss: 576.4390285015106\n",
            "epoch 18 batch 1100 [11000/160000] training loss: 552.1363016366959\n",
            "epoch 18 batch 1200 [12000/160000] training loss: 551.0102772712708\n",
            "epoch 18 batch 1300 [13000/160000] training loss: 590.2567472457886\n",
            "epoch 18 batch 1400 [14000/160000] training loss: 560.0166376829147\n",
            "epoch 18 batch 1500 [15000/160000] training loss: 536.8828911781311\n",
            "epoch 18 batch 1600 [16000/160000] training loss: 570.3773665428162\n",
            "epoch 18 batch 1700 [17000/160000] training loss: 571.2869601249695\n",
            "epoch 18 batch 1800 [18000/160000] training loss: 561.4468374252319\n",
            "epoch 18 batch 1900 [19000/160000] training loss: 555.982010602951\n",
            "epoch 18 batch 2000 [20000/160000] training loss: 540.3209457397461\n",
            "epoch 18 batch 2100 [21000/160000] training loss: 546.008602142334\n",
            "epoch 18 batch 2200 [22000/160000] training loss: 589.937816619873\n",
            "epoch 18 batch 2300 [23000/160000] training loss: 569.278475522995\n",
            "epoch 18 batch 2400 [24000/160000] training loss: 553.5471006631851\n",
            "epoch 18 batch 2500 [25000/160000] training loss: 588.6851379871368\n",
            "epoch 18 batch 2600 [26000/160000] training loss: 553.9016613960266\n",
            "epoch 18 batch 2700 [27000/160000] training loss: 581.7704831361771\n",
            "epoch 18 batch 2800 [28000/160000] training loss: 528.6089344024658\n",
            "epoch 18 batch 2900 [29000/160000] training loss: 608.0240718126297\n",
            "epoch 18 batch 3000 [30000/160000] training loss: 576.2661061286926\n",
            "epoch 18 batch 3100 [31000/160000] training loss: 582.9781808853149\n",
            "epoch 18 batch 3200 [32000/160000] training loss: 554.699743270874\n",
            "epoch 18 batch 3300 [33000/160000] training loss: 570.7841732501984\n",
            "epoch 18 batch 3400 [34000/160000] training loss: 570.4536747932434\n",
            "epoch 18 batch 3500 [35000/160000] training loss: 578.0109004974365\n",
            "epoch 18 batch 3600 [36000/160000] training loss: 564.6946983337402\n",
            "epoch 18 batch 3700 [37000/160000] training loss: 544.6978535652161\n",
            "epoch 18 batch 3800 [38000/160000] training loss: 562.3037474155426\n",
            "epoch 18 batch 3900 [39000/160000] training loss: 575.0423130989075\n",
            "epoch 18 batch 4000 [40000/160000] training loss: 562.8810157775879\n",
            "epoch 18 batch 4100 [41000/160000] training loss: 574.6770875453949\n",
            "epoch 18 batch 4200 [42000/160000] training loss: 591.8223240375519\n",
            "epoch 18 batch 4300 [43000/160000] training loss: 572.2117210626602\n",
            "epoch 18 batch 4400 [44000/160000] training loss: 553.1618064641953\n",
            "epoch 18 batch 4500 [45000/160000] training loss: 575.99542760849\n",
            "epoch 18 batch 4600 [46000/160000] training loss: 536.0004348754883\n",
            "epoch 18 batch 4700 [47000/160000] training loss: 545.1822326183319\n",
            "epoch 18 batch 4800 [48000/160000] training loss: 564.2137875556946\n",
            "epoch 18 batch 4900 [49000/160000] training loss: 568.0742623806\n",
            "epoch 18 batch 5000 [50000/160000] training loss: 557.8712041378021\n",
            "epoch 18 batch 5100 [51000/160000] training loss: 551.5748534202576\n",
            "epoch 18 batch 5200 [52000/160000] training loss: 567.6890575885773\n",
            "epoch 18 batch 5300 [53000/160000] training loss: 566.0077888965607\n",
            "epoch 18 batch 5400 [54000/160000] training loss: 567.6138954162598\n",
            "epoch 18 batch 5500 [55000/160000] training loss: 555.8667228221893\n",
            "epoch 18 batch 5600 [56000/160000] training loss: 574.6877384185791\n",
            "epoch 18 batch 5700 [57000/160000] training loss: 568.0734677314758\n",
            "epoch 18 batch 5800 [58000/160000] training loss: 568.3423364162445\n",
            "epoch 18 batch 5900 [59000/160000] training loss: 542.5257453918457\n",
            "epoch 18 batch 6000 [60000/160000] training loss: 583.7968026399612\n",
            "epoch 18 batch 6100 [61000/160000] training loss: 566.6340708732605\n",
            "epoch 18 batch 6200 [62000/160000] training loss: 590.7661578655243\n",
            "epoch 18 batch 6300 [63000/160000] training loss: 563.5488610267639\n",
            "epoch 18 batch 6400 [64000/160000] training loss: 547.1175851821899\n",
            "epoch 18 batch 6500 [65000/160000] training loss: 557.5404770374298\n",
            "epoch 18 batch 6600 [66000/160000] training loss: 535.6971065998077\n",
            "epoch 18 batch 6700 [67000/160000] training loss: 582.2008645534515\n",
            "epoch 18 batch 6800 [68000/160000] training loss: 535.6764214038849\n",
            "epoch 18 batch 6900 [69000/160000] training loss: 549.2204072475433\n",
            "epoch 18 batch 7000 [70000/160000] training loss: 537.8938574790955\n",
            "epoch 18 batch 7100 [71000/160000] training loss: 531.6450310945511\n",
            "epoch 18 batch 7200 [72000/160000] training loss: 562.7329212427139\n",
            "epoch 18 batch 7300 [73000/160000] training loss: 548.6414439678192\n",
            "epoch 18 batch 7400 [74000/160000] training loss: 538.328634262085\n",
            "epoch 18 batch 7500 [75000/160000] training loss: 557.3569014072418\n",
            "epoch 18 batch 7600 [76000/160000] training loss: 566.4702649116516\n",
            "epoch 18 batch 7700 [77000/160000] training loss: 587.0369443893433\n",
            "epoch 18 batch 7800 [78000/160000] training loss: 586.1828145980835\n",
            "epoch 18 batch 7900 [79000/160000] training loss: 552.8495795726776\n",
            "epoch 18 batch 8000 [80000/160000] training loss: 553.8281922340393\n",
            "epoch 18 batch 8100 [81000/160000] training loss: 541.9237501621246\n",
            "epoch 18 batch 8200 [82000/160000] training loss: 537.2489030361176\n",
            "epoch 18 batch 8300 [83000/160000] training loss: 578.7832964658737\n",
            "epoch 18 batch 8400 [84000/160000] training loss: 571.9447512626648\n",
            "epoch 18 batch 8500 [85000/160000] training loss: 582.2582132816315\n",
            "epoch 18 batch 8600 [86000/160000] training loss: 548.8699104785919\n",
            "epoch 18 batch 8700 [87000/160000] training loss: 558.8192462921143\n",
            "epoch 18 batch 8800 [88000/160000] training loss: 574.6235675811768\n",
            "epoch 18 batch 8900 [89000/160000] training loss: 568.6255279779434\n",
            "epoch 18 batch 9000 [90000/160000] training loss: 587.0303754806519\n",
            "epoch 18 batch 9100 [91000/160000] training loss: 584.9823656082153\n",
            "epoch 18 batch 9200 [92000/160000] training loss: 523.3620076179504\n",
            "epoch 18 batch 9300 [93000/160000] training loss: 579.5882894992828\n",
            "epoch 18 batch 9400 [94000/160000] training loss: 531.3416208028793\n",
            "epoch 18 batch 9500 [95000/160000] training loss: 573.8714210987091\n",
            "epoch 18 batch 9600 [96000/160000] training loss: 554.9749709367752\n",
            "epoch 18 batch 9700 [97000/160000] training loss: 551.9987540245056\n",
            "epoch 18 batch 9800 [98000/160000] training loss: 554.8980896472931\n",
            "epoch 18 batch 9900 [99000/160000] training loss: 563.477611541748\n",
            "epoch 18 batch 10000 [100000/160000] training loss: 587.6926400661469\n",
            "epoch 18 batch 10100 [101000/160000] training loss: 571.1499947309494\n",
            "epoch 18 batch 10200 [102000/160000] training loss: 540.3441631793976\n",
            "epoch 18 batch 10300 [103000/160000] training loss: 570.9415463209152\n",
            "epoch 18 batch 10400 [104000/160000] training loss: 570.6645344495773\n",
            "epoch 18 batch 10500 [105000/160000] training loss: 601.751953125\n",
            "epoch 18 batch 10600 [106000/160000] training loss: 588.1726421117783\n",
            "epoch 18 batch 10700 [107000/160000] training loss: 559.0050623416901\n",
            "epoch 18 batch 10800 [108000/160000] training loss: 544.3906629085541\n",
            "epoch 18 batch 10900 [109000/160000] training loss: 544.7327673435211\n",
            "epoch 18 batch 11000 [110000/160000] training loss: 564.8606370687485\n",
            "epoch 18 batch 11100 [111000/160000] training loss: 552.2837951183319\n",
            "epoch 18 batch 11200 [112000/160000] training loss: 567.4767507314682\n",
            "epoch 18 batch 11300 [113000/160000] training loss: 554.7797322273254\n",
            "epoch 18 batch 11400 [114000/160000] training loss: 568.4677686691284\n",
            "epoch 18 batch 11500 [115000/160000] training loss: 570.0605494976044\n",
            "epoch 18 batch 11600 [116000/160000] training loss: 593.6437101364136\n",
            "epoch 18 batch 11700 [117000/160000] training loss: 540.9924408197403\n",
            "epoch 18 batch 11800 [118000/160000] training loss: 552.1845149993896\n",
            "epoch 18 batch 11900 [119000/160000] training loss: 546.2937457561493\n",
            "epoch 18 batch 12000 [120000/160000] training loss: 566.1086956262589\n",
            "epoch 18 batch 12100 [121000/160000] training loss: 587.8132734298706\n",
            "epoch 18 batch 12200 [122000/160000] training loss: 555.2409682273865\n",
            "epoch 18 batch 12300 [123000/160000] training loss: 563.459431886673\n",
            "epoch 18 batch 12400 [124000/160000] training loss: 571.4573038816452\n",
            "epoch 18 batch 12500 [125000/160000] training loss: 555.550949215889\n",
            "epoch 18 batch 12600 [126000/160000] training loss: 568.3866763114929\n",
            "epoch 18 batch 12700 [127000/160000] training loss: 551.0921283960342\n",
            "epoch 18 batch 12800 [128000/160000] training loss: 579.3365228176117\n",
            "epoch 18 batch 12900 [129000/160000] training loss: 575.2806428670883\n",
            "epoch 18 batch 13000 [130000/160000] training loss: 584.495100736618\n",
            "epoch 18 batch 13100 [131000/160000] training loss: 564.0430889129639\n",
            "epoch 18 batch 13200 [132000/160000] training loss: 591.2808310985565\n",
            "epoch 18 batch 13300 [133000/160000] training loss: 572.844316482544\n",
            "epoch 18 batch 13400 [134000/160000] training loss: 546.7397286891937\n",
            "epoch 18 batch 13500 [135000/160000] training loss: 563.3383193016052\n",
            "epoch 18 batch 13600 [136000/160000] training loss: 541.5829465389252\n",
            "epoch 18 batch 13700 [137000/160000] training loss: 599.9775614738464\n",
            "epoch 18 batch 13800 [138000/160000] training loss: 581.7876358032227\n",
            "epoch 18 batch 13900 [139000/160000] training loss: 599.1762883663177\n",
            "epoch 18 batch 14000 [140000/160000] training loss: 570.0145273208618\n",
            "epoch 18 batch 14100 [141000/160000] training loss: 544.9282947778702\n",
            "epoch 18 batch 14200 [142000/160000] training loss: 564.4748570919037\n",
            "epoch 18 batch 14300 [143000/160000] training loss: 563.1062210798264\n",
            "epoch 18 batch 14400 [144000/160000] training loss: 582.1163098812103\n",
            "epoch 18 batch 14500 [145000/160000] training loss: 565.6610481739044\n",
            "epoch 18 batch 14600 [146000/160000] training loss: 558.6384867429733\n",
            "epoch 18 batch 14700 [147000/160000] training loss: 553.6616842746735\n",
            "epoch 18 batch 14800 [148000/160000] training loss: 565.4878525733948\n",
            "epoch 18 batch 14900 [149000/160000] training loss: 555.9113643169403\n",
            "epoch 18 batch 15000 [150000/160000] training loss: 596.9919166564941\n",
            "epoch 18 batch 15100 [151000/160000] training loss: 569.7202715873718\n",
            "epoch 18 batch 15200 [152000/160000] training loss: 599.4418165683746\n",
            "epoch 18 batch 15300 [153000/160000] training loss: 562.966105222702\n",
            "epoch 18 batch 15400 [154000/160000] training loss: 585.9443247318268\n",
            "epoch 18 batch 15500 [155000/160000] training loss: 554.9321508407593\n",
            "epoch 18 batch 15600 [156000/160000] training loss: 539.1818392276764\n",
            "epoch 18 batch 15700 [157000/160000] training loss: 543.7653589248657\n",
            "epoch 18 batch 15800 [158000/160000] training loss: 560.8528785705566\n",
            "epoch 18 batch 15900 [159000/160000] training loss: 548.4633252620697\n",
            "epoch 19 batch 0 [0/160000] training loss: 5.515799522399902\n",
            "epoch 19 batch 100 [1000/160000] training loss: 567.994623541832\n",
            "epoch 19 batch 200 [2000/160000] training loss: 570.1795793771744\n",
            "epoch 19 batch 300 [3000/160000] training loss: 566.7155334949493\n",
            "epoch 19 batch 400 [4000/160000] training loss: 570.6137931346893\n",
            "epoch 19 batch 500 [5000/160000] training loss: 575.1912572383881\n",
            "epoch 19 batch 600 [6000/160000] training loss: 562.1861983537674\n",
            "epoch 19 batch 700 [7000/160000] training loss: 548.5201833248138\n",
            "epoch 19 batch 800 [8000/160000] training loss: 584.0167597532272\n",
            "epoch 19 batch 900 [9000/160000] training loss: 568.5258319377899\n",
            "epoch 19 batch 1000 [10000/160000] training loss: 577.6462771892548\n",
            "epoch 19 batch 1100 [11000/160000] training loss: 553.838191986084\n",
            "epoch 19 batch 1200 [12000/160000] training loss: 551.7797849178314\n",
            "epoch 19 batch 1300 [13000/160000] training loss: 591.2101078033447\n",
            "epoch 19 batch 1400 [14000/160000] training loss: 561.2167267799377\n",
            "epoch 19 batch 1500 [15000/160000] training loss: 537.567615032196\n",
            "epoch 19 batch 1600 [16000/160000] training loss: 571.3900442123413\n",
            "epoch 19 batch 1700 [17000/160000] training loss: 573.0368914604187\n",
            "epoch 19 batch 1800 [18000/160000] training loss: 563.0089118480682\n",
            "epoch 19 batch 1900 [19000/160000] training loss: 557.0708695650101\n",
            "epoch 19 batch 2000 [20000/160000] training loss: 541.4146803617477\n",
            "epoch 19 batch 2100 [21000/160000] training loss: 545.9087821245193\n",
            "epoch 19 batch 2200 [22000/160000] training loss: 590.0125648975372\n",
            "epoch 19 batch 2300 [23000/160000] training loss: 569.1098679304123\n",
            "epoch 19 batch 2400 [24000/160000] training loss: 554.4684399366379\n",
            "epoch 19 batch 2500 [25000/160000] training loss: 589.3343155384064\n",
            "epoch 19 batch 2600 [26000/160000] training loss: 553.7434346675873\n",
            "epoch 19 batch 2700 [27000/160000] training loss: 582.5769791603088\n",
            "epoch 19 batch 2800 [28000/160000] training loss: 528.6930289268494\n",
            "epoch 19 batch 2900 [29000/160000] training loss: 608.1148700714111\n",
            "epoch 19 batch 3000 [30000/160000] training loss: 577.0264604091644\n",
            "epoch 19 batch 3100 [31000/160000] training loss: 583.3943486213684\n",
            "epoch 19 batch 3200 [32000/160000] training loss: 554.404611825943\n",
            "epoch 19 batch 3300 [33000/160000] training loss: 570.7092819213867\n",
            "epoch 19 batch 3400 [34000/160000] training loss: 570.8487048149109\n",
            "epoch 19 batch 3500 [35000/160000] training loss: 577.6164481639862\n",
            "epoch 19 batch 3600 [36000/160000] training loss: 565.1508858203888\n",
            "epoch 19 batch 3700 [37000/160000] training loss: 544.608504652977\n",
            "epoch 19 batch 3800 [38000/160000] training loss: 562.0177583694458\n",
            "epoch 19 batch 3900 [39000/160000] training loss: 575.1474623680115\n",
            "epoch 19 batch 4000 [40000/160000] training loss: 563.5530691146851\n",
            "epoch 19 batch 4100 [41000/160000] training loss: 574.4981458187103\n",
            "epoch 19 batch 4200 [42000/160000] training loss: 591.8496794700623\n",
            "epoch 19 batch 4300 [43000/160000] training loss: 571.9752390384674\n",
            "epoch 19 batch 4400 [44000/160000] training loss: 552.770565032959\n",
            "epoch 19 batch 4500 [45000/160000] training loss: 576.7039856910706\n",
            "epoch 19 batch 4600 [46000/160000] training loss: 535.1209479570389\n",
            "epoch 19 batch 4700 [47000/160000] training loss: 544.9627764225006\n",
            "epoch 19 batch 4800 [48000/160000] training loss: 564.5214756727219\n",
            "epoch 19 batch 4900 [49000/160000] training loss: 568.3792266845703\n",
            "epoch 19 batch 5000 [50000/160000] training loss: 558.2182967662811\n",
            "epoch 19 batch 5100 [51000/160000] training loss: 552.4007124900818\n",
            "epoch 19 batch 5200 [52000/160000] training loss: 567.9162383079529\n",
            "epoch 19 batch 5300 [53000/160000] training loss: 565.8576216697693\n",
            "epoch 19 batch 5400 [54000/160000] training loss: 568.2455489635468\n",
            "epoch 19 batch 5500 [55000/160000] training loss: 555.441045999527\n",
            "epoch 19 batch 5600 [56000/160000] training loss: 574.5669991970062\n",
            "epoch 19 batch 5700 [57000/160000] training loss: 567.9759542942047\n",
            "epoch 19 batch 5800 [58000/160000] training loss: 568.7920607328415\n",
            "epoch 19 batch 5900 [59000/160000] training loss: 542.3211069107056\n",
            "epoch 19 batch 6000 [60000/160000] training loss: 583.4447665214539\n",
            "epoch 19 batch 6100 [61000/160000] training loss: 565.3543092012405\n",
            "epoch 19 batch 6200 [62000/160000] training loss: 591.1128361225128\n",
            "epoch 19 batch 6300 [63000/160000] training loss: 563.9785096645355\n",
            "epoch 19 batch 6400 [64000/160000] training loss: 546.8268196582794\n",
            "epoch 19 batch 6500 [65000/160000] training loss: 556.9008104801178\n",
            "epoch 19 batch 6600 [66000/160000] training loss: 535.4444122314453\n",
            "epoch 19 batch 6700 [67000/160000] training loss: 582.6786482334137\n",
            "epoch 19 batch 6800 [68000/160000] training loss: 535.161979675293\n",
            "epoch 19 batch 6900 [69000/160000] training loss: 549.4148561954498\n",
            "epoch 19 batch 7000 [70000/160000] training loss: 537.5125148296356\n",
            "epoch 19 batch 7100 [71000/160000] training loss: 531.4844566583633\n",
            "epoch 19 batch 7200 [72000/160000] training loss: 562.0220930576324\n",
            "epoch 19 batch 7300 [73000/160000] training loss: 548.6610672473907\n",
            "epoch 19 batch 7400 [74000/160000] training loss: 537.7324311733246\n",
            "epoch 19 batch 7500 [75000/160000] training loss: 556.78047144413\n",
            "epoch 19 batch 7600 [76000/160000] training loss: 565.9991998672485\n",
            "epoch 19 batch 7700 [77000/160000] training loss: 587.1359226703644\n",
            "epoch 19 batch 7800 [78000/160000] training loss: 585.9938170909882\n",
            "epoch 19 batch 7900 [79000/160000] training loss: 552.8122398853302\n",
            "epoch 19 batch 8000 [80000/160000] training loss: 553.9900343418121\n",
            "epoch 19 batch 8100 [81000/160000] training loss: 541.9832438230515\n",
            "epoch 19 batch 8200 [82000/160000] training loss: 537.1700866222382\n",
            "epoch 19 batch 8300 [83000/160000] training loss: 578.9479418992996\n",
            "epoch 19 batch 8400 [84000/160000] training loss: 571.7623913288116\n",
            "epoch 19 batch 8500 [85000/160000] training loss: 582.2907812595367\n",
            "epoch 19 batch 8600 [86000/160000] training loss: 548.9876472949982\n",
            "epoch 19 batch 8700 [87000/160000] training loss: 558.2159700393677\n",
            "epoch 19 batch 8800 [88000/160000] training loss: 574.257730960846\n",
            "epoch 19 batch 8900 [89000/160000] training loss: 567.8635512590408\n",
            "epoch 19 batch 9000 [90000/160000] training loss: 586.8144154548645\n",
            "epoch 19 batch 9100 [91000/160000] training loss: 585.1491830348969\n",
            "epoch 19 batch 9200 [92000/160000] training loss: 523.3332979679108\n",
            "epoch 19 batch 9300 [93000/160000] training loss: 579.307354092598\n",
            "epoch 19 batch 9400 [94000/160000] training loss: 531.3834240436554\n",
            "epoch 19 batch 9500 [95000/160000] training loss: 573.8272775411606\n",
            "epoch 19 batch 9600 [96000/160000] training loss: 554.6462332010269\n",
            "epoch 19 batch 9700 [97000/160000] training loss: 551.4510476589203\n",
            "epoch 19 batch 9800 [98000/160000] training loss: 554.5514950752258\n",
            "epoch 19 batch 9900 [99000/160000] training loss: 563.9077708721161\n",
            "epoch 19 batch 10000 [100000/160000] training loss: 587.1764023303986\n",
            "epoch 19 batch 10100 [101000/160000] training loss: 570.6480464935303\n",
            "epoch 19 batch 10200 [102000/160000] training loss: 540.4922621250153\n",
            "epoch 19 batch 10300 [103000/160000] training loss: 569.7748812437057\n",
            "epoch 19 batch 10400 [104000/160000] training loss: 570.4126695394516\n",
            "epoch 19 batch 10500 [105000/160000] training loss: 601.3891232013702\n",
            "epoch 19 batch 10600 [106000/160000] training loss: 588.0140805244446\n",
            "epoch 19 batch 10700 [107000/160000] training loss: 558.6980788707733\n",
            "epoch 19 batch 10800 [108000/160000] training loss: 544.201171040535\n",
            "epoch 19 batch 10900 [109000/160000] training loss: 543.4555149078369\n",
            "epoch 19 batch 11000 [110000/160000] training loss: 564.626944065094\n",
            "epoch 19 batch 11100 [111000/160000] training loss: 551.4685261249542\n",
            "epoch 19 batch 11200 [112000/160000] training loss: 567.6162856817245\n",
            "epoch 19 batch 11300 [113000/160000] training loss: 555.2008763551712\n",
            "epoch 19 batch 11400 [114000/160000] training loss: 568.1222684383392\n",
            "epoch 19 batch 11500 [115000/160000] training loss: 570.3412487506866\n",
            "epoch 19 batch 11600 [116000/160000] training loss: 593.1941709518433\n",
            "epoch 19 batch 11700 [117000/160000] training loss: 540.8097724914551\n",
            "epoch 19 batch 11800 [118000/160000] training loss: 551.4690444469452\n",
            "epoch 19 batch 11900 [119000/160000] training loss: 546.280149936676\n",
            "epoch 19 batch 12000 [120000/160000] training loss: 566.1483987569809\n",
            "epoch 19 batch 12100 [121000/160000] training loss: 587.0073759555817\n",
            "epoch 19 batch 12200 [122000/160000] training loss: 555.7165024280548\n",
            "epoch 19 batch 12300 [123000/160000] training loss: 563.5848453044891\n",
            "epoch 19 batch 12400 [124000/160000] training loss: 571.6895225048065\n",
            "epoch 19 batch 12500 [125000/160000] training loss: 554.7027777433395\n",
            "epoch 19 batch 12600 [126000/160000] training loss: 568.4752615690231\n",
            "epoch 19 batch 12700 [127000/160000] training loss: 550.4063860177994\n",
            "epoch 19 batch 12800 [128000/160000] training loss: 579.6922714710236\n",
            "epoch 19 batch 12900 [129000/160000] training loss: 575.0754078626633\n",
            "epoch 19 batch 13000 [130000/160000] training loss: 584.4911577701569\n",
            "epoch 19 batch 13100 [131000/160000] training loss: 564.7692432403564\n",
            "epoch 19 batch 13200 [132000/160000] training loss: 591.205372095108\n",
            "epoch 19 batch 13300 [133000/160000] training loss: 572.1226377487183\n",
            "epoch 19 batch 13400 [134000/160000] training loss: 546.6993079185486\n",
            "epoch 19 batch 13500 [135000/160000] training loss: 563.3763842582703\n",
            "epoch 19 batch 13600 [136000/160000] training loss: 541.3929765224457\n",
            "epoch 19 batch 13700 [137000/160000] training loss: 599.8657350540161\n",
            "epoch 19 batch 13800 [138000/160000] training loss: 582.3466761112213\n",
            "epoch 19 batch 13900 [139000/160000] training loss: 598.7586579322815\n",
            "epoch 19 batch 14000 [140000/160000] training loss: 569.5891952514648\n",
            "epoch 19 batch 14100 [141000/160000] training loss: 544.1056044101715\n",
            "epoch 19 batch 14200 [142000/160000] training loss: 564.4328352212906\n",
            "epoch 19 batch 14300 [143000/160000] training loss: 562.5362327098846\n",
            "epoch 19 batch 14400 [144000/160000] training loss: 581.6180636882782\n",
            "epoch 19 batch 14500 [145000/160000] training loss: 565.2755672931671\n",
            "epoch 19 batch 14600 [146000/160000] training loss: 558.5866764783859\n",
            "epoch 19 batch 14700 [147000/160000] training loss: 553.1091659069061\n",
            "epoch 19 batch 14800 [148000/160000] training loss: 565.4791367053986\n",
            "epoch 19 batch 14900 [149000/160000] training loss: 555.3218531608582\n",
            "epoch 19 batch 15000 [150000/160000] training loss: 596.6211550235748\n",
            "epoch 19 batch 15100 [151000/160000] training loss: 569.3224365711212\n",
            "epoch 19 batch 15200 [152000/160000] training loss: 599.1788575649261\n",
            "epoch 19 batch 15300 [153000/160000] training loss: 563.1839606761932\n",
            "epoch 19 batch 15400 [154000/160000] training loss: 585.6213431358337\n",
            "epoch 19 batch 15500 [155000/160000] training loss: 554.4372143745422\n",
            "epoch 19 batch 15600 [156000/160000] training loss: 539.0635852813721\n",
            "epoch 19 batch 15700 [157000/160000] training loss: 543.7256408929825\n",
            "epoch 19 batch 15800 [158000/160000] training loss: 560.1980769634247\n",
            "epoch 19 batch 15900 [159000/160000] training loss: 548.5721995830536\n",
            "epoch 20 batch 0 [0/160000] training loss: 5.535366058349609\n",
            "epoch 20 batch 100 [1000/160000] training loss: 568.7336832284927\n",
            "epoch 20 batch 200 [2000/160000] training loss: 571.1616281270981\n",
            "epoch 20 batch 300 [3000/160000] training loss: 567.3761277198792\n",
            "epoch 20 batch 400 [4000/160000] training loss: 571.1590905189514\n",
            "epoch 20 batch 500 [5000/160000] training loss: 576.2308220863342\n",
            "epoch 20 batch 600 [6000/160000] training loss: 563.0450531244278\n",
            "epoch 20 batch 700 [7000/160000] training loss: 548.9183855056763\n",
            "epoch 20 batch 800 [8000/160000] training loss: 585.4123424291611\n",
            "epoch 20 batch 900 [9000/160000] training loss: 568.8309626579285\n",
            "epoch 20 batch 1000 [10000/160000] training loss: 578.4844069480896\n",
            "epoch 20 batch 1100 [11000/160000] training loss: 554.6283667087555\n",
            "epoch 20 batch 1200 [12000/160000] training loss: 551.880081653595\n",
            "epoch 20 batch 1300 [13000/160000] training loss: 591.563485622406\n",
            "epoch 20 batch 1400 [14000/160000] training loss: 561.8352383375168\n",
            "epoch 20 batch 1500 [15000/160000] training loss: 537.9618968963623\n",
            "epoch 20 batch 1600 [16000/160000] training loss: 571.8221654891968\n",
            "epoch 20 batch 1700 [17000/160000] training loss: 573.5532622337341\n",
            "epoch 20 batch 1800 [18000/160000] training loss: 563.607225060463\n",
            "epoch 20 batch 1900 [19000/160000] training loss: 557.4704481363297\n",
            "epoch 20 batch 2000 [20000/160000] training loss: 541.7740379571915\n",
            "epoch 20 batch 2100 [21000/160000] training loss: 545.9057967662811\n",
            "epoch 20 batch 2200 [22000/160000] training loss: 589.7778581380844\n",
            "epoch 20 batch 2300 [23000/160000] training loss: 568.844165802002\n",
            "epoch 20 batch 2400 [24000/160000] training loss: 554.9415628910065\n",
            "epoch 20 batch 2500 [25000/160000] training loss: 589.421627998352\n",
            "epoch 20 batch 2600 [26000/160000] training loss: 553.5087051391602\n",
            "epoch 20 batch 2700 [27000/160000] training loss: 583.003137588501\n",
            "epoch 20 batch 2800 [28000/160000] training loss: 528.6302480697632\n",
            "epoch 20 batch 2900 [29000/160000] training loss: 607.9893721342087\n",
            "epoch 20 batch 3000 [30000/160000] training loss: 577.3345077037811\n",
            "epoch 20 batch 3100 [31000/160000] training loss: 583.5237774848938\n",
            "epoch 20 batch 3200 [32000/160000] training loss: 553.9075722694397\n",
            "epoch 20 batch 3300 [33000/160000] training loss: 570.6483378410339\n",
            "epoch 20 batch 3400 [34000/160000] training loss: 571.1561560630798\n",
            "epoch 20 batch 3500 [35000/160000] training loss: 577.3102871179581\n",
            "epoch 20 batch 3600 [36000/160000] training loss: 565.3813583850861\n",
            "epoch 20 batch 3700 [37000/160000] training loss: 544.3389106988907\n",
            "epoch 20 batch 3800 [38000/160000] training loss: 561.8326408863068\n",
            "epoch 20 batch 3900 [39000/160000] training loss: 575.2673003673553\n",
            "epoch 20 batch 4000 [40000/160000] training loss: 564.0662395954132\n",
            "epoch 20 batch 4100 [41000/160000] training loss: 574.3977813720703\n",
            "epoch 20 batch 4200 [42000/160000] training loss: 591.9960236549377\n",
            "epoch 20 batch 4300 [43000/160000] training loss: 571.6966569423676\n",
            "epoch 20 batch 4400 [44000/160000] training loss: 552.5671805143356\n",
            "epoch 20 batch 4500 [45000/160000] training loss: 577.1260614395142\n",
            "epoch 20 batch 4600 [46000/160000] training loss: 534.7477127313614\n",
            "epoch 20 batch 4700 [47000/160000] training loss: 544.8486623764038\n",
            "epoch 20 batch 4800 [48000/160000] training loss: 564.7155959606171\n",
            "epoch 20 batch 4900 [49000/160000] training loss: 568.6609213352203\n",
            "epoch 20 batch 5000 [50000/160000] training loss: 558.4719970226288\n",
            "epoch 20 batch 5100 [51000/160000] training loss: 552.638467669487\n",
            "epoch 20 batch 5200 [52000/160000] training loss: 568.1570348739624\n",
            "epoch 20 batch 5300 [53000/160000] training loss: 565.6946089267731\n",
            "epoch 20 batch 5400 [54000/160000] training loss: 568.669002532959\n",
            "epoch 20 batch 5500 [55000/160000] training loss: 555.1569941043854\n",
            "epoch 20 batch 5600 [56000/160000] training loss: 574.4623175859451\n",
            "epoch 20 batch 5700 [57000/160000] training loss: 567.8317151069641\n",
            "epoch 20 batch 5800 [58000/160000] training loss: 569.2138848304749\n",
            "epoch 20 batch 5900 [59000/160000] training loss: 542.1225049495697\n",
            "epoch 20 batch 6000 [60000/160000] training loss: 583.2187970876694\n",
            "epoch 20 batch 6100 [61000/160000] training loss: 564.5557191371918\n",
            "epoch 20 batch 6200 [62000/160000] training loss: 591.3351595401764\n",
            "epoch 20 batch 6300 [63000/160000] training loss: 564.2892553806305\n",
            "epoch 20 batch 6400 [64000/160000] training loss: 546.7723178863525\n",
            "epoch 20 batch 6500 [65000/160000] training loss: 556.5376110076904\n",
            "epoch 20 batch 6600 [66000/160000] training loss: 535.0480661392212\n",
            "epoch 20 batch 6700 [67000/160000] training loss: 582.9363087415695\n",
            "epoch 20 batch 6800 [68000/160000] training loss: 534.5669105052948\n",
            "epoch 20 batch 6900 [69000/160000] training loss: 549.572300195694\n",
            "epoch 20 batch 7000 [70000/160000] training loss: 536.9544997215271\n",
            "epoch 20 batch 7100 [71000/160000] training loss: 531.4557328224182\n",
            "epoch 20 batch 7200 [72000/160000] training loss: 561.6953061819077\n",
            "epoch 20 batch 7300 [73000/160000] training loss: 548.7316303253174\n",
            "epoch 20 batch 7400 [74000/160000] training loss: 537.2529716491699\n",
            "epoch 20 batch 7500 [75000/160000] training loss: 556.3543251752853\n",
            "epoch 20 batch 7600 [76000/160000] training loss: 565.6550240516663\n",
            "epoch 20 batch 7700 [77000/160000] training loss: 587.2610712051392\n",
            "epoch 20 batch 7800 [78000/160000] training loss: 585.6847367286682\n",
            "epoch 20 batch 7900 [79000/160000] training loss: 552.7435209751129\n",
            "epoch 20 batch 8000 [80000/160000] training loss: 554.0617730617523\n",
            "epoch 20 batch 8100 [81000/160000] training loss: 541.937424659729\n",
            "epoch 20 batch 8200 [82000/160000] training loss: 537.0071883201599\n",
            "epoch 20 batch 8300 [83000/160000] training loss: 578.9516379833221\n",
            "epoch 20 batch 8400 [84000/160000] training loss: 571.6887903213501\n",
            "epoch 20 batch 8500 [85000/160000] training loss: 582.4029924869537\n",
            "epoch 20 batch 8600 [86000/160000] training loss: 549.1644883155823\n",
            "epoch 20 batch 8700 [87000/160000] training loss: 557.6999204158783\n",
            "epoch 20 batch 8800 [88000/160000] training loss: 574.0123333930969\n",
            "epoch 20 batch 8900 [89000/160000] training loss: 567.2112106084824\n",
            "epoch 20 batch 9000 [90000/160000] training loss: 586.6401617527008\n",
            "epoch 20 batch 9100 [91000/160000] training loss: 585.2980351448059\n",
            "epoch 20 batch 9200 [92000/160000] training loss: 523.3357094526291\n",
            "epoch 20 batch 9300 [93000/160000] training loss: 579.1911717653275\n",
            "epoch 20 batch 9400 [94000/160000] training loss: 531.3962134122849\n",
            "epoch 20 batch 9500 [95000/160000] training loss: 573.9073371887207\n",
            "epoch 20 batch 9600 [96000/160000] training loss: 554.5668150186539\n",
            "epoch 20 batch 9700 [97000/160000] training loss: 551.039955496788\n",
            "epoch 20 batch 9800 [98000/160000] training loss: 554.0306832790375\n",
            "epoch 20 batch 9900 [99000/160000] training loss: 564.1657752990723\n",
            "epoch 20 batch 10000 [100000/160000] training loss: 586.732692360878\n",
            "epoch 20 batch 10100 [101000/160000] training loss: 570.3538959026337\n",
            "epoch 20 batch 10200 [102000/160000] training loss: 540.8085353374481\n",
            "epoch 20 batch 10300 [103000/160000] training loss: 568.9841697216034\n",
            "epoch 20 batch 10400 [104000/160000] training loss: 570.3142341375351\n",
            "epoch 20 batch 10500 [105000/160000] training loss: 601.2157256603241\n",
            "epoch 20 batch 10600 [106000/160000] training loss: 587.9919692277908\n",
            "epoch 20 batch 10700 [107000/160000] training loss: 558.3515356779099\n",
            "epoch 20 batch 10800 [108000/160000] training loss: 544.0734816789627\n",
            "epoch 20 batch 10900 [109000/160000] training loss: 542.6105209589005\n",
            "epoch 20 batch 11000 [110000/160000] training loss: 564.3018133640289\n",
            "epoch 20 batch 11100 [111000/160000] training loss: 550.8059256076813\n",
            "epoch 20 batch 11200 [112000/160000] training loss: 567.6263912916183\n",
            "epoch 20 batch 11300 [113000/160000] training loss: 555.5168906450272\n",
            "epoch 20 batch 11400 [114000/160000] training loss: 567.8517105579376\n",
            "epoch 20 batch 11500 [115000/160000] training loss: 570.5166101455688\n",
            "epoch 20 batch 11600 [116000/160000] training loss: 592.9539008140564\n",
            "epoch 20 batch 11700 [117000/160000] training loss: 540.5388362407684\n",
            "epoch 20 batch 11800 [118000/160000] training loss: 550.9600276947021\n",
            "epoch 20 batch 11900 [119000/160000] training loss: 546.2820136547089\n",
            "epoch 20 batch 12000 [120000/160000] training loss: 566.1428354978561\n",
            "epoch 20 batch 12100 [121000/160000] training loss: 586.4769837856293\n",
            "epoch 20 batch 12200 [122000/160000] training loss: 555.9593217372894\n",
            "epoch 20 batch 12300 [123000/160000] training loss: 563.7107763290405\n",
            "epoch 20 batch 12400 [124000/160000] training loss: 571.8803899288177\n",
            "epoch 20 batch 12500 [125000/160000] training loss: 554.1929205656052\n",
            "epoch 20 batch 12600 [126000/160000] training loss: 568.5735609531403\n",
            "epoch 20 batch 12700 [127000/160000] training loss: 549.8694838285446\n",
            "epoch 20 batch 12800 [128000/160000] training loss: 580.0720362663269\n",
            "epoch 20 batch 12900 [129000/160000] training loss: 574.8715656995773\n",
            "epoch 20 batch 13000 [130000/160000] training loss: 584.4574735164642\n",
            "epoch 20 batch 13100 [131000/160000] training loss: 565.471569776535\n",
            "epoch 20 batch 13200 [132000/160000] training loss: 591.1716899871826\n",
            "epoch 20 batch 13300 [133000/160000] training loss: 571.7247049808502\n",
            "epoch 20 batch 13400 [134000/160000] training loss: 546.6265029907227\n",
            "epoch 20 batch 13500 [135000/160000] training loss: 563.4305753707886\n",
            "epoch 20 batch 13600 [136000/160000] training loss: 541.3581411838531\n",
            "epoch 20 batch 13700 [137000/160000] training loss: 599.8521853685379\n",
            "epoch 20 batch 13800 [138000/160000] training loss: 582.7615497112274\n",
            "epoch 20 batch 13900 [139000/160000] training loss: 598.6056146621704\n",
            "epoch 20 batch 14000 [140000/160000] training loss: 569.3750948905945\n",
            "epoch 20 batch 14100 [141000/160000] training loss: 543.300011754036\n",
            "epoch 20 batch 14200 [142000/160000] training loss: 564.3215775489807\n",
            "epoch 20 batch 14300 [143000/160000] training loss: 562.1369658708572\n",
            "epoch 20 batch 14400 [144000/160000] training loss: 581.2915335893631\n",
            "epoch 20 batch 14500 [145000/160000] training loss: 564.8947942256927\n",
            "epoch 20 batch 14600 [146000/160000] training loss: 558.565062046051\n",
            "epoch 20 batch 14700 [147000/160000] training loss: 552.7951309680939\n",
            "epoch 20 batch 14800 [148000/160000] training loss: 565.4160594940186\n",
            "epoch 20 batch 14900 [149000/160000] training loss: 554.9981470108032\n",
            "epoch 20 batch 15000 [150000/160000] training loss: 596.3139476776123\n",
            "epoch 20 batch 15100 [151000/160000] training loss: 569.0806367397308\n",
            "epoch 20 batch 15200 [152000/160000] training loss: 598.9521522521973\n",
            "epoch 20 batch 15300 [153000/160000] training loss: 563.3141446113586\n",
            "epoch 20 batch 15400 [154000/160000] training loss: 585.3008117675781\n",
            "epoch 20 batch 15500 [155000/160000] training loss: 554.1630413532257\n",
            "epoch 20 batch 15600 [156000/160000] training loss: 539.0100708007812\n",
            "epoch 20 batch 15700 [157000/160000] training loss: 543.6459747552872\n",
            "epoch 20 batch 15800 [158000/160000] training loss: 559.8046078681946\n",
            "epoch 20 batch 15900 [159000/160000] training loss: 548.8890717029572\n",
            "epoch 21 batch 0 [0/160000] training loss: 5.551250457763672\n",
            "epoch 21 batch 100 [1000/160000] training loss: 569.1240291595459\n",
            "epoch 21 batch 200 [2000/160000] training loss: 571.7887462377548\n",
            "epoch 21 batch 300 [3000/160000] training loss: 567.7983524799347\n",
            "epoch 21 batch 400 [4000/160000] training loss: 571.540590763092\n",
            "epoch 21 batch 500 [5000/160000] training loss: 576.8406047821045\n",
            "epoch 21 batch 600 [6000/160000] training loss: 563.527641415596\n",
            "epoch 21 batch 700 [7000/160000] training loss: 549.0732551813126\n",
            "epoch 21 batch 800 [8000/160000] training loss: 586.4841591119766\n",
            "epoch 21 batch 900 [9000/160000] training loss: 569.0088844299316\n",
            "epoch 21 batch 1000 [10000/160000] training loss: 579.1016035079956\n",
            "epoch 21 batch 1100 [11000/160000] training loss: 555.1261593103409\n",
            "epoch 21 batch 1200 [12000/160000] training loss: 551.7757539749146\n",
            "epoch 21 batch 1300 [13000/160000] training loss: 591.6565372943878\n",
            "epoch 21 batch 1400 [14000/160000] training loss: 562.2304602861404\n",
            "epoch 21 batch 1500 [15000/160000] training loss: 538.2560803890228\n",
            "epoch 21 batch 1600 [16000/160000] training loss: 572.049825668335\n",
            "epoch 21 batch 1700 [17000/160000] training loss: 573.6571574211121\n",
            "epoch 21 batch 1800 [18000/160000] training loss: 563.8809645175934\n",
            "epoch 21 batch 1900 [19000/160000] training loss: 557.6435451507568\n",
            "epoch 21 batch 2000 [20000/160000] training loss: 541.8480296134949\n",
            "epoch 21 batch 2100 [21000/160000] training loss: 545.959689617157\n",
            "epoch 21 batch 2200 [22000/160000] training loss: 589.5413664579391\n",
            "epoch 21 batch 2300 [23000/160000] training loss: 568.5598603487015\n",
            "epoch 21 batch 2400 [24000/160000] training loss: 555.2939567565918\n",
            "epoch 21 batch 2500 [25000/160000] training loss: 589.3449382781982\n",
            "epoch 21 batch 2600 [26000/160000] training loss: 553.2806067466736\n",
            "epoch 21 batch 2700 [27000/160000] training loss: 583.2692930698395\n",
            "epoch 21 batch 2800 [28000/160000] training loss: 528.5316169261932\n",
            "epoch 21 batch 2900 [29000/160000] training loss: 607.8068989515305\n",
            "epoch 21 batch 3000 [30000/160000] training loss: 577.5062766075134\n",
            "epoch 21 batch 3100 [31000/160000] training loss: 583.5590977668762\n",
            "epoch 21 batch 3200 [32000/160000] training loss: 553.3780205249786\n",
            "epoch 21 batch 3300 [33000/160000] training loss: 570.5480976104736\n",
            "epoch 21 batch 3400 [34000/160000] training loss: 571.4099135398865\n",
            "epoch 21 batch 3500 [35000/160000] training loss: 577.0583838224411\n",
            "epoch 21 batch 3600 [36000/160000] training loss: 565.5278990268707\n",
            "epoch 21 batch 3700 [37000/160000] training loss: 544.0345025062561\n",
            "epoch 21 batch 3800 [38000/160000] training loss: 561.6685557365417\n",
            "epoch 21 batch 3900 [39000/160000] training loss: 575.3999216556549\n",
            "epoch 21 batch 4000 [40000/160000] training loss: 564.490650177002\n",
            "epoch 21 batch 4100 [41000/160000] training loss: 574.3064730167389\n",
            "epoch 21 batch 4200 [42000/160000] training loss: 592.1526832580566\n",
            "epoch 21 batch 4300 [43000/160000] training loss: 571.4225908517838\n",
            "epoch 21 batch 4400 [44000/160000] training loss: 552.422487616539\n",
            "epoch 21 batch 4500 [45000/160000] training loss: 577.441385269165\n",
            "epoch 21 batch 4600 [46000/160000] training loss: 534.5646311044693\n",
            "epoch 21 batch 4700 [47000/160000] training loss: 544.7720139026642\n",
            "epoch 21 batch 4800 [48000/160000] training loss: 564.8675129413605\n",
            "epoch 21 batch 4900 [49000/160000] training loss: 568.8989367485046\n",
            "epoch 21 batch 5000 [50000/160000] training loss: 558.6637454032898\n",
            "epoch 21 batch 5100 [51000/160000] training loss: 552.6620837450027\n",
            "epoch 21 batch 5200 [52000/160000] training loss: 568.3764500617981\n",
            "epoch 21 batch 5300 [53000/160000] training loss: 565.4979853630066\n",
            "epoch 21 batch 5400 [54000/160000] training loss: 569.0310366153717\n",
            "epoch 21 batch 5500 [55000/160000] training loss: 554.895350933075\n",
            "epoch 21 batch 5600 [56000/160000] training loss: 574.3947645425797\n",
            "epoch 21 batch 5700 [57000/160000] training loss: 567.6885225772858\n",
            "epoch 21 batch 5800 [58000/160000] training loss: 569.6274725198746\n",
            "epoch 21 batch 5900 [59000/160000] training loss: 541.951988697052\n",
            "epoch 21 batch 6000 [60000/160000] training loss: 583.0343511104584\n",
            "epoch 21 batch 6100 [61000/160000] training loss: 563.9466454982758\n",
            "epoch 21 batch 6200 [62000/160000] training loss: 591.5311422348022\n",
            "epoch 21 batch 6300 [63000/160000] training loss: 564.5953509807587\n",
            "epoch 21 batch 6400 [64000/160000] training loss: 546.8071649074554\n",
            "epoch 21 batch 6500 [65000/160000] training loss: 556.2333450317383\n",
            "epoch 21 batch 6600 [66000/160000] training loss: 534.6155471801758\n",
            "epoch 21 batch 6700 [67000/160000] training loss: 583.0971947908401\n",
            "epoch 21 batch 6800 [68000/160000] training loss: 533.9766120910645\n",
            "epoch 21 batch 6900 [69000/160000] training loss: 549.7438737154007\n",
            "epoch 21 batch 7000 [70000/160000] training loss: 536.3630831241608\n",
            "epoch 21 batch 7100 [71000/160000] training loss: 531.5054836273193\n",
            "epoch 21 batch 7200 [72000/160000] training loss: 561.5150727033615\n",
            "epoch 21 batch 7300 [73000/160000] training loss: 548.7877068519592\n",
            "epoch 21 batch 7400 [74000/160000] training loss: 536.8259780406952\n",
            "epoch 21 batch 7500 [75000/160000] training loss: 555.9979032278061\n",
            "epoch 21 batch 7600 [76000/160000] training loss: 565.3312371969223\n",
            "epoch 21 batch 7700 [77000/160000] training loss: 587.3268172740936\n",
            "epoch 21 batch 7800 [78000/160000] training loss: 585.3090388774872\n",
            "epoch 21 batch 7900 [79000/160000] training loss: 552.6819332838058\n",
            "epoch 21 batch 8000 [80000/160000] training loss: 554.1101696491241\n",
            "epoch 21 batch 8100 [81000/160000] training loss: 541.830353140831\n",
            "epoch 21 batch 8200 [82000/160000] training loss: 536.8098402023315\n",
            "epoch 21 batch 8300 [83000/160000] training loss: 578.9571232795715\n",
            "epoch 21 batch 8400 [84000/160000] training loss: 571.6631782054901\n",
            "epoch 21 batch 8500 [85000/160000] training loss: 582.558017373085\n",
            "epoch 21 batch 8600 [86000/160000] training loss: 549.3702907562256\n",
            "epoch 21 batch 8700 [87000/160000] training loss: 557.2619199752808\n",
            "epoch 21 batch 8800 [88000/160000] training loss: 573.8207764625549\n",
            "epoch 21 batch 8900 [89000/160000] training loss: 566.5929174423218\n",
            "epoch 21 batch 9000 [90000/160000] training loss: 586.4594049453735\n",
            "epoch 21 batch 9100 [91000/160000] training loss: 585.4034028053284\n",
            "epoch 21 batch 9200 [92000/160000] training loss: 523.3502594232559\n",
            "epoch 21 batch 9300 [93000/160000] training loss: 579.1639758348465\n",
            "epoch 21 batch 9400 [94000/160000] training loss: 531.4293528795242\n",
            "epoch 21 batch 9500 [95000/160000] training loss: 574.0618065595627\n",
            "epoch 21 batch 9600 [96000/160000] training loss: 554.5839680433273\n",
            "epoch 21 batch 9700 [97000/160000] training loss: 550.6719701290131\n",
            "epoch 21 batch 9800 [98000/160000] training loss: 553.4556245803833\n",
            "epoch 21 batch 9900 [99000/160000] training loss: 564.3176383972168\n",
            "epoch 21 batch 10000 [100000/160000] training loss: 586.3271569013596\n",
            "epoch 21 batch 10100 [101000/160000] training loss: 570.1412360668182\n",
            "epoch 21 batch 10200 [102000/160000] training loss: 541.1829130649567\n",
            "epoch 21 batch 10300 [103000/160000] training loss: 568.3345504999161\n",
            "epoch 21 batch 10400 [104000/160000] training loss: 570.273677110672\n",
            "epoch 21 batch 10500 [105000/160000] training loss: 601.1111104488373\n",
            "epoch 21 batch 10600 [106000/160000] training loss: 588.0390725135803\n",
            "epoch 21 batch 10700 [107000/160000] training loss: 558.0095909833908\n",
            "epoch 21 batch 10800 [108000/160000] training loss: 543.9504302740097\n",
            "epoch 21 batch 10900 [109000/160000] training loss: 541.9688053131104\n",
            "epoch 21 batch 11000 [110000/160000] training loss: 563.9425165653229\n",
            "epoch 21 batch 11100 [111000/160000] training loss: 550.1952176094055\n",
            "epoch 21 batch 11200 [112000/160000] training loss: 567.5712586641312\n",
            "epoch 21 batch 11300 [113000/160000] training loss: 555.7878328561783\n",
            "epoch 21 batch 11400 [114000/160000] training loss: 567.6556067466736\n",
            "epoch 21 batch 11500 [115000/160000] training loss: 570.6112740039825\n",
            "epoch 21 batch 11600 [116000/160000] training loss: 592.781091928482\n",
            "epoch 21 batch 11700 [117000/160000] training loss: 540.2733323574066\n",
            "epoch 21 batch 11800 [118000/160000] training loss: 550.5657501220703\n",
            "epoch 21 batch 11900 [119000/160000] training loss: 546.2627590894699\n",
            "epoch 21 batch 12000 [120000/160000] training loss: 566.1471256017685\n",
            "epoch 21 batch 12100 [121000/160000] training loss: 586.0709173679352\n",
            "epoch 21 batch 12200 [122000/160000] training loss: 556.0978515148163\n",
            "epoch 21 batch 12300 [123000/160000] training loss: 563.8385922908783\n",
            "epoch 21 batch 12400 [124000/160000] training loss: 572.065486907959\n",
            "epoch 21 batch 12500 [125000/160000] training loss: 553.8518899679184\n",
            "epoch 21 batch 12600 [126000/160000] training loss: 568.670371055603\n",
            "epoch 21 batch 12700 [127000/160000] training loss: 549.3852545022964\n",
            "epoch 21 batch 12800 [128000/160000] training loss: 580.4708826541901\n",
            "epoch 21 batch 12900 [129000/160000] training loss: 574.6629530191422\n",
            "epoch 21 batch 13000 [130000/160000] training loss: 584.426878452301\n",
            "epoch 21 batch 13100 [131000/160000] training loss: 566.1342175006866\n",
            "epoch 21 batch 13200 [132000/160000] training loss: 591.117609500885\n",
            "epoch 21 batch 13300 [133000/160000] training loss: 571.4754762649536\n",
            "epoch 21 batch 13400 [134000/160000] training loss: 546.5373344421387\n",
            "epoch 21 batch 13500 [135000/160000] training loss: 563.4498846530914\n",
            "epoch 21 batch 13600 [136000/160000] training loss: 541.3676009178162\n",
            "epoch 21 batch 13700 [137000/160000] training loss: 599.8686940670013\n",
            "epoch 21 batch 13800 [138000/160000] training loss: 583.087441444397\n",
            "epoch 21 batch 13900 [139000/160000] training loss: 598.543862581253\n",
            "epoch 21 batch 14000 [140000/160000] training loss: 569.269734621048\n",
            "epoch 21 batch 14100 [141000/160000] training loss: 542.5440257787704\n",
            "epoch 21 batch 14200 [142000/160000] training loss: 564.1583653688431\n",
            "epoch 21 batch 14300 [143000/160000] training loss: 561.8233940601349\n",
            "epoch 21 batch 14400 [144000/160000] training loss: 581.0592972040176\n",
            "epoch 21 batch 14500 [145000/160000] training loss: 564.5374386310577\n",
            "epoch 21 batch 14600 [146000/160000] training loss: 558.5365934371948\n",
            "epoch 21 batch 14700 [147000/160000] training loss: 552.5879950523376\n",
            "epoch 21 batch 14800 [148000/160000] training loss: 565.3065365552902\n",
            "epoch 21 batch 14900 [149000/160000] training loss: 554.8166599273682\n",
            "epoch 21 batch 15000 [150000/160000] training loss: 596.0303289890289\n",
            "epoch 21 batch 15100 [151000/160000] training loss: 568.9257175922394\n",
            "epoch 21 batch 15200 [152000/160000] training loss: 598.7143483161926\n",
            "epoch 21 batch 15300 [153000/160000] training loss: 563.3824398517609\n",
            "epoch 21 batch 15400 [154000/160000] training loss: 584.9926283359528\n",
            "epoch 21 batch 15500 [155000/160000] training loss: 553.9882471561432\n",
            "epoch 21 batch 15600 [156000/160000] training loss: 539.0007901191711\n",
            "epoch 21 batch 15700 [157000/160000] training loss: 543.5392154455185\n",
            "epoch 21 batch 15800 [158000/160000] training loss: 559.5461231470108\n",
            "epoch 21 batch 15900 [159000/160000] training loss: 549.2783298492432\n",
            "epoch 22 batch 0 [0/160000] training loss: 5.565447807312012\n",
            "epoch 22 batch 100 [1000/160000] training loss: 569.335566163063\n",
            "epoch 22 batch 200 [2000/160000] training loss: 572.2439721822739\n",
            "epoch 22 batch 300 [3000/160000] training loss: 568.1288757324219\n",
            "epoch 22 batch 400 [4000/160000] training loss: 571.8160181045532\n",
            "epoch 22 batch 500 [5000/160000] training loss: 577.2221720218658\n",
            "epoch 22 batch 600 [6000/160000] training loss: 563.8404958248138\n",
            "epoch 22 batch 700 [7000/160000] training loss: 549.1112523078918\n",
            "epoch 22 batch 800 [8000/160000] training loss: 587.357682466507\n",
            "epoch 22 batch 900 [9000/160000] training loss: 569.1434160470963\n",
            "epoch 22 batch 1000 [10000/160000] training loss: 579.581552028656\n",
            "epoch 22 batch 1100 [11000/160000] training loss: 555.494894862175\n",
            "epoch 22 batch 1200 [12000/160000] training loss: 551.6086692810059\n",
            "epoch 22 batch 1300 [13000/160000] training loss: 591.5999177694321\n",
            "epoch 22 batch 1400 [14000/160000] training loss: 562.5122566223145\n",
            "epoch 22 batch 1500 [15000/160000] training loss: 538.5070164203644\n",
            "epoch 22 batch 1600 [16000/160000] training loss: 572.1865515708923\n",
            "epoch 22 batch 1700 [17000/160000] training loss: 573.5915584564209\n",
            "epoch 22 batch 1800 [18000/160000] training loss: 563.9967086315155\n",
            "epoch 22 batch 1900 [19000/160000] training loss: 557.7376766204834\n",
            "epoch 22 batch 2000 [20000/160000] training loss: 541.7948954105377\n",
            "epoch 22 batch 2100 [21000/160000] training loss: 546.0442576408386\n",
            "epoch 22 batch 2200 [22000/160000] training loss: 589.3377262353897\n",
            "epoch 22 batch 2300 [23000/160000] training loss: 568.2783880233765\n",
            "epoch 22 batch 2400 [24000/160000] training loss: 555.594386100769\n",
            "epoch 22 batch 2500 [25000/160000] training loss: 589.2254483699799\n",
            "epoch 22 batch 2600 [26000/160000] training loss: 553.0619421005249\n",
            "epoch 22 batch 2700 [27000/160000] training loss: 583.4696382284164\n",
            "epoch 22 batch 2800 [28000/160000] training loss: 528.4330612421036\n",
            "epoch 22 batch 2900 [29000/160000] training loss: 607.6086581945419\n",
            "epoch 22 batch 3000 [30000/160000] training loss: 577.6076443195343\n",
            "epoch 22 batch 3100 [31000/160000] training loss: 583.5574991703033\n",
            "epoch 22 batch 3200 [32000/160000] training loss: 552.8566908836365\n",
            "epoch 22 batch 3300 [33000/160000] training loss: 570.4197387695312\n",
            "epoch 22 batch 3400 [34000/160000] training loss: 571.6357278823853\n",
            "epoch 22 batch 3500 [35000/160000] training loss: 576.8472046852112\n",
            "epoch 22 batch 3600 [36000/160000] training loss: 565.6320104598999\n",
            "epoch 22 batch 3700 [37000/160000] training loss: 543.7252967357635\n",
            "epoch 22 batch 3800 [38000/160000] training loss: 561.5127890110016\n",
            "epoch 22 batch 3900 [39000/160000] training loss: 575.5503106117249\n",
            "epoch 22 batch 4000 [40000/160000] training loss: 564.8611817359924\n",
            "epoch 22 batch 4100 [41000/160000] training loss: 574.2101609706879\n",
            "epoch 22 batch 4200 [42000/160000] training loss: 592.3129544258118\n",
            "epoch 22 batch 4300 [43000/160000] training loss: 571.1674416065216\n",
            "epoch 22 batch 4400 [44000/160000] training loss: 552.3073482513428\n",
            "epoch 22 batch 4500 [45000/160000] training loss: 577.7000942230225\n",
            "epoch 22 batch 4600 [46000/160000] training loss: 534.4596207141876\n",
            "epoch 22 batch 4700 [47000/160000] training loss: 544.7107598781586\n",
            "epoch 22 batch 4800 [48000/160000] training loss: 565.0018863677979\n",
            "epoch 22 batch 4900 [49000/160000] training loss: 569.1184136867523\n",
            "epoch 22 batch 5000 [50000/160000] training loss: 558.8139989376068\n",
            "epoch 22 batch 5100 [51000/160000] training loss: 552.6015900373459\n",
            "epoch 22 batch 5200 [52000/160000] training loss: 568.5886597633362\n",
            "epoch 22 batch 5300 [53000/160000] training loss: 565.2844655513763\n",
            "epoch 22 batch 5400 [54000/160000] training loss: 569.355968952179\n",
            "epoch 22 batch 5500 [55000/160000] training loss: 554.6373653411865\n",
            "epoch 22 batch 5600 [56000/160000] training loss: 574.3604494333267\n",
            "epoch 22 batch 5700 [57000/160000] training loss: 567.5615870952606\n",
            "epoch 22 batch 5800 [58000/160000] training loss: 570.0393718481064\n",
            "epoch 22 batch 5900 [59000/160000] training loss: 541.8034257888794\n",
            "epoch 22 batch 6000 [60000/160000] training loss: 582.8692433834076\n",
            "epoch 22 batch 6100 [61000/160000] training loss: 563.4384603500366\n",
            "epoch 22 batch 6200 [62000/160000] training loss: 591.7209115028381\n",
            "epoch 22 batch 6300 [63000/160000] training loss: 564.9124052524567\n",
            "epoch 22 batch 6400 [64000/160000] training loss: 546.8922736644745\n",
            "epoch 22 batch 6500 [65000/160000] training loss: 555.9552257061005\n",
            "epoch 22 batch 6600 [66000/160000] training loss: 534.1779026985168\n",
            "epoch 22 batch 6700 [67000/160000] training loss: 583.2113100290298\n",
            "epoch 22 batch 6800 [68000/160000] training loss: 533.4000055789948\n",
            "epoch 22 batch 6900 [69000/160000] training loss: 549.9492383003235\n",
            "epoch 22 batch 7000 [70000/160000] training loss: 535.7727764844894\n",
            "epoch 22 batch 7100 [71000/160000] training loss: 531.6015628576279\n",
            "epoch 22 batch 7200 [72000/160000] training loss: 561.390546798706\n",
            "epoch 22 batch 7300 [73000/160000] training loss: 548.8310430049896\n",
            "epoch 22 batch 7400 [74000/160000] training loss: 536.439211845398\n",
            "epoch 22 batch 7500 [75000/160000] training loss: 555.6972507238388\n",
            "epoch 22 batch 7600 [76000/160000] training loss: 565.0217230319977\n",
            "epoch 22 batch 7700 [77000/160000] training loss: 587.3462970256805\n",
            "epoch 22 batch 7800 [78000/160000] training loss: 584.8866963386536\n",
            "epoch 22 batch 7900 [79000/160000] training loss: 552.6297037601471\n",
            "epoch 22 batch 8000 [80000/160000] training loss: 554.164059638977\n",
            "epoch 22 batch 8100 [81000/160000] training loss: 541.6946594715118\n",
            "epoch 22 batch 8200 [82000/160000] training loss: 536.5977827310562\n",
            "epoch 22 batch 8300 [83000/160000] training loss: 578.9874793291092\n",
            "epoch 22 batch 8400 [84000/160000] training loss: 571.662006855011\n",
            "epoch 22 batch 8500 [85000/160000] training loss: 582.7365579605103\n",
            "epoch 22 batch 8600 [86000/160000] training loss: 549.5936512947083\n",
            "epoch 22 batch 8700 [87000/160000] training loss: 556.8797776699066\n",
            "epoch 22 batch 8800 [88000/160000] training loss: 573.6593832969666\n",
            "epoch 22 batch 8900 [89000/160000] training loss: 566.0023901462555\n",
            "epoch 22 batch 9000 [90000/160000] training loss: 586.2631242275238\n",
            "epoch 22 batch 9100 [91000/160000] training loss: 585.4774466753006\n",
            "epoch 22 batch 9200 [92000/160000] training loss: 523.3764752149582\n",
            "epoch 22 batch 9300 [93000/160000] training loss: 579.1809149980545\n",
            "epoch 22 batch 9400 [94000/160000] training loss: 531.4885482788086\n",
            "epoch 22 batch 9500 [95000/160000] training loss: 574.2518090009689\n",
            "epoch 22 batch 9600 [96000/160000] training loss: 554.6584216356277\n",
            "epoch 22 batch 9700 [97000/160000] training loss: 550.3152985572815\n",
            "epoch 22 batch 9800 [98000/160000] training loss: 552.8667089939117\n",
            "epoch 22 batch 9900 [99000/160000] training loss: 564.4039301872253\n",
            "epoch 22 batch 10000 [100000/160000] training loss: 585.9422899484634\n",
            "epoch 22 batch 10100 [101000/160000] training loss: 569.9704301357269\n",
            "epoch 22 batch 10200 [102000/160000] training loss: 541.5751101970673\n",
            "epoch 22 batch 10300 [103000/160000] training loss: 567.7551709413528\n",
            "epoch 22 batch 10400 [104000/160000] training loss: 570.2567965984344\n",
            "epoch 22 batch 10500 [105000/160000] training loss: 601.0472824573517\n",
            "epoch 22 batch 10600 [106000/160000] training loss: 588.1368539333344\n",
            "epoch 22 batch 10700 [107000/160000] training loss: 557.674086689949\n",
            "epoch 22 batch 10800 [108000/160000] training loss: 543.83413875103\n",
            "epoch 22 batch 10900 [109000/160000] training loss: 541.4422690868378\n",
            "epoch 22 batch 11000 [110000/160000] training loss: 563.5586204528809\n",
            "epoch 22 batch 11100 [111000/160000] training loss: 549.6074185371399\n",
            "epoch 22 batch 11200 [112000/160000] training loss: 567.47900390625\n",
            "epoch 22 batch 11300 [113000/160000] training loss: 556.0356349945068\n",
            "epoch 22 batch 11400 [114000/160000] training loss: 567.5262622833252\n",
            "epoch 22 batch 11500 [115000/160000] training loss: 570.6468489170074\n",
            "epoch 22 batch 11600 [116000/160000] training loss: 592.6435378789902\n",
            "epoch 22 batch 11700 [117000/160000] training loss: 540.0373414754868\n",
            "epoch 22 batch 11800 [118000/160000] training loss: 550.2305819988251\n",
            "epoch 22 batch 11900 [119000/160000] training loss: 546.2204703092575\n",
            "epoch 22 batch 12000 [120000/160000] training loss: 566.1661919355392\n",
            "epoch 22 batch 12100 [121000/160000] training loss: 585.7353265285492\n",
            "epoch 22 batch 12200 [122000/160000] training loss: 556.1874461174011\n",
            "epoch 22 batch 12300 [123000/160000] training loss: 563.9620530605316\n",
            "epoch 22 batch 12400 [124000/160000] training loss: 572.2609969377518\n",
            "epoch 22 batch 12500 [125000/160000] training loss: 553.6165262460709\n",
            "epoch 22 batch 12600 [126000/160000] training loss: 568.7672410011292\n",
            "epoch 22 batch 12700 [127000/160000] training loss: 548.9282103776932\n",
            "epoch 22 batch 12800 [128000/160000] training loss: 580.8807978630066\n",
            "epoch 22 batch 12900 [129000/160000] training loss: 574.4487118721008\n",
            "epoch 22 batch 13000 [130000/160000] training loss: 584.4084775447845\n",
            "epoch 22 batch 13100 [131000/160000] training loss: 566.7610855102539\n",
            "epoch 22 batch 13200 [132000/160000] training loss: 591.0402100086212\n",
            "epoch 22 batch 13300 [133000/160000] training loss: 571.3027102947235\n",
            "epoch 22 batch 13400 [134000/160000] training loss: 546.4386434555054\n",
            "epoch 22 batch 13500 [135000/160000] training loss: 563.4291424751282\n",
            "epoch 22 batch 13600 [136000/160000] training loss: 541.3901216983795\n",
            "epoch 22 batch 13700 [137000/160000] training loss: 599.8954926729202\n",
            "epoch 22 batch 13800 [138000/160000] training loss: 583.3479385375977\n",
            "epoch 22 batch 13900 [139000/160000] training loss: 598.5150046348572\n",
            "epoch 22 batch 14000 [140000/160000] training loss: 569.2361925840378\n",
            "epoch 22 batch 14100 [141000/160000] training loss: 541.8417167663574\n",
            "epoch 22 batch 14200 [142000/160000] training loss: 563.9612466096878\n",
            "epoch 22 batch 14300 [143000/160000] training loss: 561.5615119934082\n",
            "epoch 22 batch 14400 [144000/160000] training loss: 580.8868796825409\n",
            "epoch 22 batch 14500 [145000/160000] training loss: 564.2050044536591\n",
            "epoch 22 batch 14600 [146000/160000] training loss: 558.4936593770981\n",
            "epoch 22 batch 14700 [147000/160000] training loss: 552.4490625858307\n",
            "epoch 22 batch 14800 [148000/160000] training loss: 565.1601001024246\n",
            "epoch 22 batch 14900 [149000/160000] training loss: 554.7312834262848\n",
            "epoch 22 batch 15000 [150000/160000] training loss: 595.7556569576263\n",
            "epoch 22 batch 15100 [151000/160000] training loss: 568.8244233131409\n",
            "epoch 22 batch 15200 [152000/160000] training loss: 598.4554963111877\n",
            "epoch 22 batch 15300 [153000/160000] training loss: 563.4162933826447\n",
            "epoch 22 batch 15400 [154000/160000] training loss: 584.7044302225113\n",
            "epoch 22 batch 15500 [155000/160000] training loss: 553.8658978939056\n",
            "epoch 22 batch 15600 [156000/160000] training loss: 539.0177929401398\n",
            "epoch 22 batch 15700 [157000/160000] training loss: 543.4197509288788\n",
            "epoch 22 batch 15800 [158000/160000] training loss: 559.3682808876038\n",
            "epoch 22 batch 15900 [159000/160000] training loss: 549.6912939548492\n",
            "epoch 23 batch 0 [0/160000] training loss: 5.578503608703613\n",
            "epoch 23 batch 100 [1000/160000] training loss: 569.435883641243\n",
            "epoch 23 batch 200 [2000/160000] training loss: 572.6126171350479\n",
            "epoch 23 batch 300 [3000/160000] training loss: 568.4188072681427\n",
            "epoch 23 batch 400 [4000/160000] training loss: 572.0135555267334\n",
            "epoch 23 batch 500 [5000/160000] training loss: 577.466423034668\n",
            "epoch 23 batch 600 [6000/160000] training loss: 564.0647053718567\n",
            "epoch 23 batch 700 [7000/160000] training loss: 549.0914170742035\n",
            "epoch 23 batch 800 [8000/160000] training loss: 588.102134346962\n",
            "epoch 23 batch 900 [9000/160000] training loss: 569.2601300477982\n",
            "epoch 23 batch 1000 [10000/160000] training loss: 579.9731562137604\n",
            "epoch 23 batch 1100 [11000/160000] training loss: 555.8000314235687\n",
            "epoch 23 batch 1200 [12000/160000] training loss: 551.4241733551025\n",
            "epoch 23 batch 1300 [13000/160000] training loss: 591.4394998550415\n",
            "epoch 23 batch 1400 [14000/160000] training loss: 562.7269343137741\n",
            "epoch 23 batch 1500 [15000/160000] training loss: 538.7364494800568\n",
            "epoch 23 batch 1600 [16000/160000] training loss: 572.2816998958588\n",
            "epoch 23 batch 1700 [17000/160000] training loss: 573.4444177150726\n",
            "epoch 23 batch 1800 [18000/160000] training loss: 564.0214847326279\n",
            "epoch 23 batch 1900 [19000/160000] training loss: 557.8033192157745\n",
            "epoch 23 batch 2000 [20000/160000] training loss: 541.6834365129471\n",
            "epoch 23 batch 2100 [21000/160000] training loss: 546.148487329483\n",
            "epoch 23 batch 2200 [22000/160000] training loss: 589.1621178388596\n",
            "epoch 23 batch 2300 [23000/160000] training loss: 567.9992690086365\n",
            "epoch 23 batch 2400 [24000/160000] training loss: 555.8686192035675\n",
            "epoch 23 batch 2500 [25000/160000] training loss: 589.102424621582\n",
            "epoch 23 batch 2600 [26000/160000] training loss: 552.8527510166168\n",
            "epoch 23 batch 2700 [27000/160000] training loss: 583.639014840126\n",
            "epoch 23 batch 2800 [28000/160000] training loss: 528.3483570814133\n",
            "epoch 23 batch 2900 [29000/160000] training loss: 607.4131315946579\n",
            "epoch 23 batch 3000 [30000/160000] training loss: 577.657965183258\n",
            "epoch 23 batch 3100 [31000/160000] training loss: 583.5421330928802\n",
            "epoch 23 batch 3200 [32000/160000] training loss: 552.3540682792664\n",
            "epoch 23 batch 3300 [33000/160000] training loss: 570.2759363651276\n",
            "epoch 23 batch 3400 [34000/160000] training loss: 571.845428943634\n",
            "epoch 23 batch 3500 [35000/160000] training loss: 576.6724292039871\n",
            "epoch 23 batch 3600 [36000/160000] training loss: 565.7121667861938\n",
            "epoch 23 batch 3700 [37000/160000] training loss: 543.4221639633179\n",
            "epoch 23 batch 3800 [38000/160000] training loss: 561.3585331439972\n",
            "epoch 23 batch 3900 [39000/160000] training loss: 575.7098815441132\n",
            "epoch 23 batch 4000 [40000/160000] training loss: 565.1985232830048\n",
            "epoch 23 batch 4100 [41000/160000] training loss: 574.1057207584381\n",
            "epoch 23 batch 4200 [42000/160000] training loss: 592.4813601970673\n",
            "epoch 23 batch 4300 [43000/160000] training loss: 570.9320727586746\n",
            "epoch 23 batch 4400 [44000/160000] training loss: 552.2106201648712\n",
            "epoch 23 batch 4500 [45000/160000] training loss: 577.9215829372406\n",
            "epoch 23 batch 4600 [46000/160000] training loss: 534.3932095766068\n",
            "epoch 23 batch 4700 [47000/160000] training loss: 544.6577944755554\n",
            "epoch 23 batch 4800 [48000/160000] training loss: 565.1249377727509\n",
            "epoch 23 batch 4900 [49000/160000] training loss: 569.3362374305725\n",
            "epoch 23 batch 5000 [50000/160000] training loss: 558.9381321668625\n",
            "epoch 23 batch 5100 [51000/160000] training loss: 552.5120093822479\n",
            "epoch 23 batch 5200 [52000/160000] training loss: 568.8057534694672\n",
            "epoch 23 batch 5300 [53000/160000] training loss: 565.0629096031189\n",
            "epoch 23 batch 5400 [54000/160000] training loss: 569.649759054184\n",
            "epoch 23 batch 5500 [55000/160000] training loss: 554.3782887458801\n",
            "epoch 23 batch 5600 [56000/160000] training loss: 574.3546695709229\n",
            "epoch 23 batch 5700 [57000/160000] training loss: 567.4543015956879\n",
            "epoch 23 batch 5800 [58000/160000] training loss: 570.4519619941711\n",
            "epoch 23 batch 5900 [59000/160000] training loss: 541.6745607852936\n",
            "epoch 23 batch 6000 [60000/160000] training loss: 582.7176538705826\n",
            "epoch 23 batch 6100 [61000/160000] training loss: 562.9941594600677\n",
            "epoch 23 batch 6200 [62000/160000] training loss: 591.9040467739105\n",
            "epoch 23 batch 6300 [63000/160000] training loss: 565.2373864650726\n",
            "epoch 23 batch 6400 [64000/160000] training loss: 547.0118811130524\n",
            "epoch 23 batch 6500 [65000/160000] training loss: 555.6971672773361\n",
            "epoch 23 batch 6600 [66000/160000] training loss: 533.7452726364136\n",
            "epoch 23 batch 6700 [67000/160000] training loss: 583.3007709980011\n",
            "epoch 23 batch 6800 [68000/160000] training loss: 532.8363778591156\n",
            "epoch 23 batch 6900 [69000/160000] training loss: 550.191780090332\n",
            "epoch 23 batch 7000 [70000/160000] training loss: 535.1967585086823\n",
            "epoch 23 batch 7100 [71000/160000] training loss: 531.7255152463913\n",
            "epoch 23 batch 7200 [72000/160000] training loss: 561.2851248979568\n",
            "epoch 23 batch 7300 [73000/160000] training loss: 548.8700611591339\n",
            "epoch 23 batch 7400 [74000/160000] training loss: 536.0878601074219\n",
            "epoch 23 batch 7500 [75000/160000] training loss: 555.4489065408707\n",
            "epoch 23 batch 7600 [76000/160000] training loss: 564.7272000312805\n",
            "epoch 23 batch 7700 [77000/160000] training loss: 587.338376045227\n",
            "epoch 23 batch 7800 [78000/160000] training loss: 584.4351251125336\n",
            "epoch 23 batch 7900 [79000/160000] training loss: 552.5825581550598\n",
            "epoch 23 batch 8000 [80000/160000] training loss: 554.2319204807281\n",
            "epoch 23 batch 8100 [81000/160000] training loss: 541.5452013015747\n",
            "epoch 23 batch 8200 [82000/160000] training loss: 536.3804790973663\n",
            "epoch 23 batch 8300 [83000/160000] training loss: 579.0383501052856\n",
            "epoch 23 batch 8400 [84000/160000] training loss: 571.678596496582\n",
            "epoch 23 batch 8500 [85000/160000] training loss: 582.9289329051971\n",
            "epoch 23 batch 8600 [86000/160000] training loss: 549.8294937610626\n",
            "epoch 23 batch 8700 [87000/160000] training loss: 556.534255027771\n",
            "epoch 23 batch 8800 [88000/160000] training loss: 573.5185177326202\n",
            "epoch 23 batch 8900 [89000/160000] training loss: 565.4413111209869\n",
            "epoch 23 batch 9000 [90000/160000] training loss: 586.0525922775269\n",
            "epoch 23 batch 9100 [91000/160000] training loss: 585.5298653841019\n",
            "epoch 23 batch 9200 [92000/160000] training loss: 523.4137887954712\n",
            "epoch 23 batch 9300 [93000/160000] training loss: 579.2207380533218\n",
            "epoch 23 batch 9400 [94000/160000] training loss: 531.5723400115967\n",
            "epoch 23 batch 9500 [95000/160000] training loss: 574.45277094841\n",
            "epoch 23 batch 9600 [96000/160000] training loss: 554.7737075090408\n",
            "epoch 23 batch 9700 [97000/160000] training loss: 549.9597271680832\n",
            "epoch 23 batch 9800 [98000/160000] training loss: 552.2826738357544\n",
            "epoch 23 batch 9900 [99000/160000] training loss: 564.4465749263763\n",
            "epoch 23 batch 10000 [100000/160000] training loss: 585.570281624794\n",
            "epoch 23 batch 10100 [101000/160000] training loss: 569.8227624893188\n",
            "epoch 23 batch 10200 [102000/160000] training loss: 541.9707918167114\n",
            "epoch 23 batch 10300 [103000/160000] training loss: 567.2170211076736\n",
            "epoch 23 batch 10400 [104000/160000] training loss: 570.2492530345917\n",
            "epoch 23 batch 10500 [105000/160000] training loss: 601.0153329372406\n",
            "epoch 23 batch 10600 [106000/160000] training loss: 588.2729645967484\n",
            "epoch 23 batch 10700 [107000/160000] training loss: 557.3426983356476\n",
            "epoch 23 batch 10800 [108000/160000] training loss: 543.7300316095352\n",
            "epoch 23 batch 10900 [109000/160000] training loss: 540.9895761013031\n",
            "epoch 23 batch 11000 [110000/160000] training loss: 563.1534579992294\n",
            "epoch 23 batch 11100 [111000/160000] training loss: 549.0338060855865\n",
            "epoch 23 batch 11200 [112000/160000] training loss: 567.3624137639999\n",
            "epoch 23 batch 11300 [113000/160000] training loss: 556.2685525417328\n",
            "epoch 23 batch 11400 [114000/160000] training loss: 567.4534792900085\n",
            "epoch 23 batch 11500 [115000/160000] training loss: 570.6360540390015\n",
            "epoch 23 batch 11600 [116000/160000] training loss: 592.5339004993439\n",
            "epoch 23 batch 11700 [117000/160000] training loss: 539.8357688188553\n",
            "epoch 23 batch 11800 [118000/160000] training loss: 549.9254409074783\n",
            "epoch 23 batch 11900 [119000/160000] training loss: 546.1601648330688\n",
            "epoch 23 batch 12000 [120000/160000] training loss: 566.1942402124405\n",
            "epoch 23 batch 12100 [121000/160000] training loss: 585.4472455978394\n",
            "epoch 23 batch 12200 [122000/160000] training loss: 556.2533550262451\n",
            "epoch 23 batch 12300 [123000/160000] training loss: 564.0782492160797\n",
            "epoch 23 batch 12400 [124000/160000] training loss: 572.4722945690155\n",
            "epoch 23 batch 12500 [125000/160000] training loss: 553.4542232751846\n",
            "epoch 23 batch 12600 [126000/160000] training loss: 568.8643944263458\n",
            "epoch 23 batch 12700 [127000/160000] training loss: 548.4895285367966\n",
            "epoch 23 batch 12800 [128000/160000] training loss: 581.2960865497589\n",
            "epoch 23 batch 12900 [129000/160000] training loss: 574.2304230928421\n",
            "epoch 23 batch 13000 [130000/160000] training loss: 584.4044179916382\n",
            "epoch 23 batch 13100 [131000/160000] training loss: 567.3583679199219\n",
            "epoch 23 batch 13200 [132000/160000] training loss: 590.9440217018127\n",
            "epoch 23 batch 13300 [133000/160000] training loss: 571.17014503479\n",
            "epoch 23 batch 13400 [134000/160000] training loss: 546.3323302268982\n",
            "epoch 23 batch 13500 [135000/160000] training loss: 563.3702673912048\n",
            "epoch 23 batch 13600 [136000/160000] training loss: 541.4169425964355\n",
            "epoch 23 batch 13700 [137000/160000] training loss: 599.9239636659622\n",
            "epoch 23 batch 13800 [138000/160000] training loss: 583.5563051700592\n",
            "epoch 23 batch 13900 [139000/160000] training loss: 598.496992111206\n",
            "epoch 23 batch 14000 [140000/160000] training loss: 569.2557406425476\n",
            "epoch 23 batch 14100 [141000/160000] training loss: 541.189602971077\n",
            "epoch 23 batch 14200 [142000/160000] training loss: 563.741735458374\n",
            "epoch 23 batch 14300 [143000/160000] training loss: 561.3343070745468\n",
            "epoch 23 batch 14400 [144000/160000] training loss: 580.7574377059937\n",
            "epoch 23 batch 14500 [145000/160000] training loss: 563.8932929039001\n",
            "epoch 23 batch 14600 [146000/160000] training loss: 558.4377913475037\n",
            "epoch 23 batch 14700 [147000/160000] training loss: 552.362776517868\n",
            "epoch 23 batch 14800 [148000/160000] training loss: 564.9841046333313\n",
            "epoch 23 batch 14900 [149000/160000] training loss: 554.7185852527618\n",
            "epoch 23 batch 15000 [150000/160000] training loss: 595.4825991392136\n",
            "epoch 23 batch 15100 [151000/160000] training loss: 568.7572989463806\n",
            "epoch 23 batch 15200 [152000/160000] training loss: 598.1751139163971\n",
            "epoch 23 batch 15300 [153000/160000] training loss: 563.4333018064499\n",
            "epoch 23 batch 15400 [154000/160000] training loss: 584.4419108629227\n",
            "epoch 23 batch 15500 [155000/160000] training loss: 553.7746162414551\n",
            "epoch 23 batch 15600 [156000/160000] training loss: 539.0459680557251\n",
            "epoch 23 batch 15700 [157000/160000] training loss: 543.2977908849716\n",
            "epoch 23 batch 15800 [158000/160000] training loss: 559.2445374727249\n",
            "epoch 23 batch 15900 [159000/160000] training loss: 550.1076655387878\n",
            "epoch 24 batch 0 [0/160000] training loss: 5.590574741363525\n",
            "epoch 24 batch 100 [1000/160000] training loss: 569.4596476554871\n",
            "epoch 24 batch 200 [2000/160000] training loss: 572.9366935491562\n",
            "epoch 24 batch 300 [3000/160000] training loss: 568.6906445026398\n",
            "epoch 24 batch 400 [4000/160000] training loss: 572.1522109508514\n",
            "epoch 24 batch 500 [5000/160000] training loss: 577.6206922531128\n",
            "epoch 24 batch 600 [6000/160000] training loss: 564.2383879423141\n",
            "epoch 24 batch 700 [7000/160000] training loss: 549.0450947284698\n",
            "epoch 24 batch 800 [8000/160000] training loss: 588.7578099966049\n",
            "epoch 24 batch 900 [9000/160000] training loss: 569.3689028024673\n",
            "epoch 24 batch 1000 [10000/160000] training loss: 580.3057534694672\n",
            "epoch 24 batch 1100 [11000/160000] training loss: 556.0714958906174\n",
            "epoch 24 batch 1200 [12000/160000] training loss: 551.238284111023\n",
            "epoch 24 batch 1300 [13000/160000] training loss: 591.198169708252\n",
            "epoch 24 batch 1400 [14000/160000] training loss: 562.8962987661362\n",
            "epoch 24 batch 1500 [15000/160000] training loss: 538.9550335407257\n",
            "epoch 24 batch 1600 [16000/160000] training loss: 572.3602902889252\n",
            "epoch 24 batch 1700 [17000/160000] training loss: 573.2538146972656\n",
            "epoch 24 batch 1800 [18000/160000] training loss: 563.9886876344681\n",
            "epoch 24 batch 1900 [19000/160000] training loss: 557.860679268837\n",
            "epoch 24 batch 2000 [20000/160000] training loss: 541.5476334095001\n",
            "epoch 24 batch 2100 [21000/160000] training loss: 546.2674376964569\n",
            "epoch 24 batch 2200 [22000/160000] training loss: 589.007328748703\n",
            "epoch 24 batch 2300 [23000/160000] training loss: 567.7210848331451\n",
            "epoch 24 batch 2400 [24000/160000] training loss: 556.128870010376\n",
            "epoch 24 batch 2500 [25000/160000] training loss: 588.9892084598541\n",
            "epoch 24 batch 2600 [26000/160000] training loss: 552.6533126831055\n",
            "epoch 24 batch 2700 [27000/160000] training loss: 583.7904685735703\n",
            "epoch 24 batch 2800 [28000/160000] training loss: 528.2825409173965\n",
            "epoch 24 batch 2900 [29000/160000] training loss: 607.2299907207489\n",
            "epoch 24 batch 3000 [30000/160000] training loss: 577.6652030944824\n",
            "epoch 24 batch 3100 [31000/160000] training loss: 583.5236184597015\n",
            "epoch 24 batch 3200 [32000/160000] training loss: 551.8722696304321\n",
            "epoch 24 batch 3300 [33000/160000] training loss: 570.1246817111969\n",
            "epoch 24 batch 3400 [34000/160000] training loss: 572.0436615943909\n",
            "epoch 24 batch 3500 [35000/160000] training loss: 576.531506061554\n",
            "epoch 24 batch 3600 [36000/160000] training loss: 565.7779808044434\n",
            "epoch 24 batch 3700 [37000/160000] training loss: 543.1297063827515\n",
            "epoch 24 batch 3800 [38000/160000] training loss: 561.2012691497803\n",
            "epoch 24 batch 3900 [39000/160000] training loss: 575.8687443733215\n",
            "epoch 24 batch 4000 [40000/160000] training loss: 565.513927936554\n",
            "epoch 24 batch 4100 [41000/160000] training loss: 573.9910941123962\n",
            "epoch 24 batch 4200 [42000/160000] training loss: 592.660756111145\n",
            "epoch 24 batch 4300 [43000/160000] training loss: 570.7144483327866\n",
            "epoch 24 batch 4400 [44000/160000] training loss: 552.1257301568985\n",
            "epoch 24 batch 4500 [45000/160000] training loss: 578.1159362792969\n",
            "epoch 24 batch 4600 [46000/160000] training loss: 534.351288318634\n",
            "epoch 24 batch 4700 [47000/160000] training loss: 544.6103694438934\n",
            "epoch 24 batch 4800 [48000/160000] training loss: 565.2377123832703\n",
            "epoch 24 batch 4900 [49000/160000] training loss: 569.5597026348114\n",
            "epoch 24 batch 5000 [50000/160000] training loss: 559.0465160608292\n",
            "epoch 24 batch 5100 [51000/160000] training loss: 552.4185432195663\n",
            "epoch 24 batch 5200 [52000/160000] training loss: 569.0351874828339\n",
            "epoch 24 batch 5300 [53000/160000] training loss: 564.8363120555878\n",
            "epoch 24 batch 5400 [54000/160000] training loss: 569.9151256084442\n",
            "epoch 24 batch 5500 [55000/160000] training loss: 554.1170597076416\n",
            "epoch 24 batch 5600 [56000/160000] training loss: 574.3737643957138\n",
            "epoch 24 batch 5700 [57000/160000] training loss: 567.3652675151825\n",
            "epoch 24 batch 5800 [58000/160000] training loss: 570.8646738529205\n",
            "epoch 24 batch 5900 [59000/160000] training loss: 541.5655281543732\n",
            "epoch 24 batch 6000 [60000/160000] training loss: 582.5786994695663\n",
            "epoch 24 batch 6100 [61000/160000] training loss: 562.5952793359756\n",
            "epoch 24 batch 6200 [62000/160000] training loss: 592.0770406723022\n",
            "epoch 24 batch 6300 [63000/160000] training loss: 565.5647494792938\n",
            "epoch 24 batch 6400 [64000/160000] training loss: 547.156617641449\n",
            "epoch 24 batch 6500 [65000/160000] training loss: 555.4566504955292\n",
            "epoch 24 batch 6600 [66000/160000] training loss: 533.3202667236328\n",
            "epoch 24 batch 6700 [67000/160000] training loss: 583.3759609460831\n",
            "epoch 24 batch 6800 [68000/160000] training loss: 532.2848665714264\n",
            "epoch 24 batch 6900 [69000/160000] training loss: 550.4680474996567\n",
            "epoch 24 batch 7000 [70000/160000] training loss: 534.6409862041473\n",
            "epoch 24 batch 7100 [71000/160000] training loss: 531.8655520677567\n",
            "epoch 24 batch 7200 [72000/160000] training loss: 561.1839739084244\n",
            "epoch 24 batch 7300 [73000/160000] training loss: 548.9110879898071\n",
            "epoch 24 batch 7400 [74000/160000] training loss: 535.7686903476715\n",
            "epoch 24 batch 7500 [75000/160000] training loss: 555.2513550519943\n",
            "epoch 24 batch 7600 [76000/160000] training loss: 564.4461705684662\n",
            "epoch 24 batch 7700 [77000/160000] training loss: 587.3163197040558\n",
            "epoch 24 batch 7800 [78000/160000] training loss: 583.9669749736786\n",
            "epoch 24 batch 7900 [79000/160000] training loss: 552.5371078252792\n",
            "epoch 24 batch 8000 [80000/160000] training loss: 554.3148672580719\n",
            "epoch 24 batch 8100 [81000/160000] training loss: 541.3890529870987\n",
            "epoch 24 batch 8200 [82000/160000] training loss: 536.1628749370575\n",
            "epoch 24 batch 8300 [83000/160000] training loss: 579.1018545627594\n",
            "epoch 24 batch 8400 [84000/160000] training loss: 571.7110033035278\n",
            "epoch 24 batch 8500 [85000/160000] training loss: 583.13008081913\n",
            "epoch 24 batch 8600 [86000/160000] training loss: 550.0746862888336\n",
            "epoch 24 batch 8700 [87000/160000] training loss: 556.2113711833954\n",
            "epoch 24 batch 8800 [88000/160000] training loss: 573.3943302631378\n",
            "epoch 24 batch 8900 [89000/160000] training loss: 564.9110606908798\n",
            "epoch 24 batch 9000 [90000/160000] training loss: 585.831699848175\n",
            "epoch 24 batch 9100 [91000/160000] training loss: 585.565676689148\n",
            "epoch 24 batch 9200 [92000/160000] training loss: 523.460862994194\n",
            "epoch 24 batch 9300 [93000/160000] training loss: 579.2732146978378\n",
            "epoch 24 batch 9400 [94000/160000] training loss: 531.6778919696808\n",
            "epoch 24 batch 9500 [95000/160000] training loss: 574.6503604650497\n",
            "epoch 24 batch 9600 [96000/160000] training loss: 554.920534491539\n",
            "epoch 24 batch 9700 [97000/160000] training loss: 549.6019601821899\n",
            "epoch 24 batch 9800 [98000/160000] training loss: 551.7134132385254\n",
            "epoch 24 batch 9900 [99000/160000] training loss: 564.4574952125549\n",
            "epoch 24 batch 10000 [100000/160000] training loss: 585.2080806493759\n",
            "epoch 24 batch 10100 [101000/160000] training loss: 569.6879861354828\n",
            "epoch 24 batch 10200 [102000/160000] training loss: 542.3646895885468\n",
            "epoch 24 batch 10300 [103000/160000] training loss: 566.706152677536\n",
            "epoch 24 batch 10400 [104000/160000] training loss: 570.2440159320831\n",
            "epoch 24 batch 10500 [105000/160000] training loss: 601.0114378929138\n",
            "epoch 24 batch 10600 [106000/160000] training loss: 588.4382507801056\n",
            "epoch 24 batch 10700 [107000/160000] training loss: 557.0143033266068\n",
            "epoch 24 batch 10800 [108000/160000] training loss: 543.6418695449829\n",
            "epoch 24 batch 10900 [109000/160000] training loss: 540.5897279977798\n",
            "epoch 24 batch 11000 [110000/160000] training loss: 562.73018181324\n",
            "epoch 24 batch 11100 [111000/160000] training loss: 548.472326040268\n",
            "epoch 24 batch 11200 [112000/160000] training loss: 567.2277144193649\n",
            "epoch 24 batch 11300 [113000/160000] training loss: 556.4900081157684\n",
            "epoch 24 batch 11400 [114000/160000] training loss: 567.4281976222992\n",
            "epoch 24 batch 11500 [115000/160000] training loss: 570.5868284702301\n",
            "epoch 24 batch 11600 [116000/160000] training loss: 592.4504806995392\n",
            "epoch 24 batch 11700 [117000/160000] training loss: 539.6685481071472\n",
            "epoch 24 batch 11800 [118000/160000] training loss: 549.6356970071793\n",
            "epoch 24 batch 11900 [119000/160000] training loss: 546.0880103111267\n",
            "epoch 24 batch 12000 [120000/160000] training loss: 566.2259922027588\n",
            "epoch 24 batch 12100 [121000/160000] training loss: 585.1959550380707\n",
            "epoch 24 batch 12200 [122000/160000] training loss: 556.3076610565186\n",
            "epoch 24 batch 12300 [123000/160000] training loss: 564.1865997314453\n",
            "epoch 24 batch 12400 [124000/160000] training loss: 572.7002165317535\n",
            "epoch 24 batch 12500 [125000/160000] training loss: 553.3451540470123\n",
            "epoch 24 batch 12600 [126000/160000] training loss: 568.9613301753998\n",
            "epoch 24 batch 12700 [127000/160000] training loss: 548.0648249387741\n",
            "epoch 24 batch 12800 [128000/160000] training loss: 581.7128250598907\n",
            "epoch 24 batch 12900 [129000/160000] training loss: 574.0101346969604\n",
            "epoch 24 batch 13000 [130000/160000] training loss: 584.4154207706451\n",
            "epoch 24 batch 13100 [131000/160000] training loss: 567.9307587146759\n",
            "epoch 24 batch 13200 [132000/160000] training loss: 590.832861661911\n",
            "epoch 24 batch 13300 [133000/160000] training loss: 571.0576226711273\n",
            "epoch 24 batch 13400 [134000/160000] training loss: 546.218733549118\n",
            "epoch 24 batch 13500 [135000/160000] training loss: 563.2753071784973\n",
            "epoch 24 batch 13600 [136000/160000] training loss: 541.4465897083282\n",
            "epoch 24 batch 13700 [137000/160000] training loss: 599.9507038593292\n",
            "epoch 24 batch 13800 [138000/160000] training loss: 583.7212808132172\n",
            "epoch 24 batch 13900 [139000/160000] training loss: 598.482164144516\n",
            "epoch 24 batch 14000 [140000/160000] training loss: 569.3163412809372\n",
            "epoch 24 batch 14100 [141000/160000] training loss: 540.5828701257706\n",
            "epoch 24 batch 14200 [142000/160000] training loss: 563.5076977014542\n",
            "epoch 24 batch 14300 [143000/160000] training loss: 561.1316903829575\n",
            "epoch 24 batch 14400 [144000/160000] training loss: 580.6609723567963\n",
            "epoch 24 batch 14500 [145000/160000] training loss: 563.597373008728\n",
            "epoch 24 batch 14600 [146000/160000] training loss: 558.3721910715103\n",
            "epoch 24 batch 14700 [147000/160000] training loss: 552.3210206031799\n",
            "epoch 24 batch 14800 [148000/160000] training loss: 564.7841123342514\n",
            "epoch 24 batch 14900 [149000/160000] training loss: 554.7639172077179\n",
            "epoch 24 batch 15000 [150000/160000] training loss: 595.2063205242157\n",
            "epoch 24 batch 15100 [151000/160000] training loss: 568.711653470993\n",
            "epoch 24 batch 15200 [152000/160000] training loss: 597.8755307197571\n",
            "epoch 24 batch 15300 [153000/160000] training loss: 563.4450263977051\n",
            "epoch 24 batch 15400 [154000/160000] training loss: 584.2083164453506\n",
            "epoch 24 batch 15500 [155000/160000] training loss: 553.7040445804596\n",
            "epoch 24 batch 15600 [156000/160000] training loss: 539.0745060443878\n",
            "epoch 24 batch 15700 [157000/160000] training loss: 543.1800456047058\n",
            "epoch 24 batch 15800 [158000/160000] training loss: 559.1601383686066\n",
            "epoch 24 batch 15900 [159000/160000] training loss: 550.5176439285278\n",
            "epoch 25 batch 0 [0/160000] training loss: 5.601649284362793\n",
            "epoch 25 batch 100 [1000/160000] training loss: 569.4275650978088\n",
            "epoch 25 batch 200 [2000/160000] training loss: 573.2367796897888\n",
            "epoch 25 batch 300 [3000/160000] training loss: 568.9551603794098\n",
            "epoch 25 batch 400 [4000/160000] training loss: 572.2470624446869\n",
            "epoch 25 batch 500 [5000/160000] training loss: 577.7115106582642\n",
            "epoch 25 batch 600 [6000/160000] training loss: 564.3816673755646\n",
            "epoch 25 batch 700 [7000/160000] training loss: 548.9901106357574\n",
            "epoch 25 batch 800 [8000/160000] training loss: 589.3489022254944\n",
            "epoch 25 batch 900 [9000/160000] training loss: 569.4743988513947\n",
            "epoch 25 batch 1000 [10000/160000] training loss: 580.5978858470917\n",
            "epoch 25 batch 1100 [11000/160000] training loss: 556.3236856460571\n",
            "epoch 25 batch 1200 [12000/160000] training loss: 551.0579676628113\n",
            "epoch 25 batch 1300 [13000/160000] training loss: 590.8900842666626\n",
            "epoch 25 batch 1400 [14000/160000] training loss: 563.0317697525024\n",
            "epoch 25 batch 1500 [15000/160000] training loss: 539.1682879924774\n",
            "epoch 25 batch 1600 [16000/160000] training loss: 572.4357030391693\n",
            "epoch 25 batch 1700 [17000/160000] training loss: 573.0382790565491\n",
            "epoch 25 batch 1800 [18000/160000] training loss: 563.9173523187637\n",
            "epoch 25 batch 1900 [19000/160000] training loss: 557.9185576438904\n",
            "epoch 25 batch 2000 [20000/160000] training loss: 541.4056260585785\n",
            "epoch 25 batch 2100 [21000/160000] training loss: 546.3988835811615\n",
            "epoch 25 batch 2200 [22000/160000] training loss: 588.8683787584305\n",
            "epoch 25 batch 2300 [23000/160000] training loss: 567.4443291425705\n",
            "epoch 25 batch 2400 [24000/160000] training loss: 556.3816255331039\n",
            "epoch 25 batch 2500 [25000/160000] training loss: 588.889750957489\n",
            "epoch 25 batch 2600 [26000/160000] training loss: 552.4638299942017\n",
            "epoch 25 batch 2700 [27000/160000] training loss: 583.9285852909088\n",
            "epoch 25 batch 2800 [28000/160000] training loss: 528.2370091676712\n",
            "epoch 25 batch 2900 [29000/160000] training loss: 607.064218044281\n",
            "epoch 25 batch 3000 [30000/160000] training loss: 577.6353306770325\n",
            "epoch 25 batch 3100 [31000/160000] training loss: 583.5081632137299\n",
            "epoch 25 batch 3200 [32000/160000] training loss: 551.4105803966522\n",
            "epoch 25 batch 3300 [33000/160000] training loss: 569.970686674118\n",
            "epoch 25 batch 3400 [34000/160000] training loss: 572.2326600551605\n",
            "epoch 25 batch 3500 [35000/160000] training loss: 576.4228068590164\n",
            "epoch 25 batch 3600 [36000/160000] training loss: 565.8349931240082\n",
            "epoch 25 batch 3700 [37000/160000] training loss: 542.8508653640747\n",
            "epoch 25 batch 3800 [38000/160000] training loss: 561.0384433269501\n",
            "epoch 25 batch 3900 [39000/160000] training loss: 576.0190708637238\n",
            "epoch 25 batch 4000 [40000/160000] training loss: 565.8133606910706\n",
            "epoch 25 batch 4100 [41000/160000] training loss: 573.8646869659424\n",
            "epoch 25 batch 4200 [42000/160000] training loss: 592.8529579639435\n",
            "epoch 25 batch 4300 [43000/160000] training loss: 570.5130163431168\n",
            "epoch 25 batch 4400 [44000/160000] training loss: 552.0476975440979\n",
            "epoch 25 batch 4500 [45000/160000] training loss: 578.2887408733368\n",
            "epoch 25 batch 4600 [46000/160000] training loss: 534.3281568288803\n",
            "epoch 25 batch 4700 [47000/160000] training loss: 544.5666694641113\n",
            "epoch 25 batch 4800 [48000/160000] training loss: 565.3396188020706\n",
            "epoch 25 batch 4900 [49000/160000] training loss: 569.7907483577728\n",
            "epoch 25 batch 5000 [50000/160000] training loss: 559.1457662582397\n",
            "epoch 25 batch 5100 [51000/160000] training loss: 552.3334225416183\n",
            "epoch 25 batch 5200 [52000/160000] training loss: 569.2812786102295\n",
            "epoch 25 batch 5300 [53000/160000] training loss: 564.6050932407379\n",
            "epoch 25 batch 5400 [54000/160000] training loss: 570.1537127494812\n",
            "epoch 25 batch 5500 [55000/160000] training loss: 553.8542721271515\n",
            "epoch 25 batch 5600 [56000/160000] training loss: 574.4144364595413\n",
            "epoch 25 batch 5700 [57000/160000] training loss: 567.2915461063385\n",
            "epoch 25 batch 5800 [58000/160000] training loss: 571.2762562036514\n",
            "epoch 25 batch 5900 [59000/160000] training loss: 541.476681470871\n",
            "epoch 25 batch 6000 [60000/160000] training loss: 582.4525302648544\n",
            "epoch 25 batch 6100 [61000/160000] training loss: 562.2312490940094\n",
            "epoch 25 batch 6200 [62000/160000] training loss: 592.2371537685394\n",
            "epoch 25 batch 6300 [63000/160000] training loss: 565.8898904323578\n",
            "epoch 25 batch 6400 [64000/160000] training loss: 547.3191862106323\n",
            "epoch 25 batch 6500 [65000/160000] training loss: 555.23126912117\n",
            "epoch 25 batch 6600 [66000/160000] training loss: 532.9037909507751\n",
            "epoch 25 batch 6700 [67000/160000] training loss: 583.442680478096\n",
            "epoch 25 batch 6800 [68000/160000] training loss: 531.7454402446747\n",
            "epoch 25 batch 6900 [69000/160000] training loss: 550.7729716300964\n",
            "epoch 25 batch 7000 [70000/160000] training loss: 534.1078536510468\n",
            "epoch 25 batch 7100 [71000/160000] training loss: 532.0129985809326\n",
            "epoch 25 batch 7200 [72000/160000] training loss: 561.0826588869095\n",
            "epoch 25 batch 7300 [73000/160000] training loss: 548.958667755127\n",
            "epoch 25 batch 7400 [74000/160000] training loss: 535.4788618087769\n",
            "epoch 25 batch 7500 [75000/160000] training loss: 555.1034156084061\n",
            "epoch 25 batch 7600 [76000/160000] training loss: 564.1752387285233\n",
            "epoch 25 batch 7700 [77000/160000] training loss: 587.2878649234772\n",
            "epoch 25 batch 7800 [78000/160000] training loss: 583.4917123317719\n",
            "epoch 25 batch 7900 [79000/160000] training loss: 552.4908261299133\n",
            "epoch 25 batch 8000 [80000/160000] training loss: 554.4112544059753\n",
            "epoch 25 batch 8100 [81000/160000] training loss: 541.2296781539917\n",
            "epoch 25 batch 8200 [82000/160000] training loss: 535.9471070766449\n",
            "epoch 25 batch 8300 [83000/160000] training loss: 579.171835064888\n",
            "epoch 25 batch 8400 [84000/160000] training loss: 571.7589378356934\n",
            "epoch 25 batch 8500 [85000/160000] training loss: 583.3376863002777\n",
            "epoch 25 batch 8600 [86000/160000] training loss: 550.3273887634277\n",
            "epoch 25 batch 8700 [87000/160000] training loss: 555.902592420578\n",
            "epoch 25 batch 8800 [88000/160000] training loss: 573.284243106842\n",
            "epoch 25 batch 8900 [89000/160000] training loss: 564.4111828804016\n",
            "epoch 25 batch 9000 [90000/160000] training loss: 585.6040008068085\n",
            "epoch 25 batch 9100 [91000/160000] training loss: 585.5866529941559\n",
            "epoch 25 batch 9200 [92000/160000] training loss: 523.515510559082\n",
            "epoch 25 batch 9300 [93000/160000] training loss: 579.3335597515106\n",
            "epoch 25 batch 9400 [94000/160000] training loss: 531.8025674819946\n",
            "epoch 25 batch 9500 [95000/160000] training loss: 574.8358256816864\n",
            "epoch 25 batch 9600 [96000/160000] training loss: 555.093186378479\n",
            "epoch 25 batch 9700 [97000/160000] training loss: 549.241201877594\n",
            "epoch 25 batch 9800 [98000/160000] training loss: 551.1647672653198\n",
            "epoch 25 batch 9900 [99000/160000] training loss: 564.4432799816132\n",
            "epoch 25 batch 10000 [100000/160000] training loss: 584.8542320728302\n",
            "epoch 25 batch 10100 [101000/160000] training loss: 569.560052394867\n",
            "epoch 25 batch 10200 [102000/160000] training loss: 542.7553033828735\n",
            "epoch 25 batch 10300 [103000/160000] training loss: 566.2159733772278\n",
            "epoch 25 batch 10400 [104000/160000] training loss: 570.2372477054596\n",
            "epoch 25 batch 10500 [105000/160000] training loss: 601.0331788063049\n",
            "epoch 25 batch 10600 [106000/160000] training loss: 588.62590944767\n",
            "epoch 25 batch 10700 [107000/160000] training loss: 556.6880900859833\n",
            "epoch 25 batch 10800 [108000/160000] training loss: 543.5714354515076\n",
            "epoch 25 batch 10900 [109000/160000] training loss: 540.2306809425354\n",
            "epoch 25 batch 11000 [110000/160000] training loss: 562.2926359176636\n",
            "epoch 25 batch 11100 [111000/160000] training loss: 547.9236805438995\n",
            "epoch 25 batch 11200 [112000/160000] training loss: 567.0779172182083\n",
            "epoch 25 batch 11300 [113000/160000] training loss: 556.7017749547958\n",
            "epoch 25 batch 11400 [114000/160000] training loss: 567.4424216747284\n",
            "epoch 25 batch 11500 [115000/160000] training loss: 570.5040140151978\n",
            "epoch 25 batch 11600 [116000/160000] training loss: 592.393975019455\n",
            "epoch 25 batch 11700 [117000/160000] training loss: 539.5332653522491\n",
            "epoch 25 batch 11800 [118000/160000] training loss: 549.3547627925873\n",
            "epoch 25 batch 11900 [119000/160000] training loss: 546.0094861984253\n",
            "epoch 25 batch 12000 [120000/160000] training loss: 566.2577726840973\n",
            "epoch 25 batch 12100 [121000/160000] training loss: 584.9758944511414\n",
            "epoch 25 batch 12200 [122000/160000] training loss: 556.3555147647858\n",
            "epoch 25 batch 12300 [123000/160000] training loss: 564.2872586250305\n",
            "epoch 25 batch 12400 [124000/160000] training loss: 572.9434078931808\n",
            "epoch 25 batch 12500 [125000/160000] training loss: 553.27568089962\n",
            "epoch 25 batch 12600 [126000/160000] training loss: 569.0575768947601\n",
            "epoch 25 batch 12700 [127000/160000] training loss: 547.6511416435242\n",
            "epoch 25 batch 12800 [128000/160000] training loss: 582.1288132667542\n",
            "epoch 25 batch 12900 [129000/160000] training loss: 573.7892931699753\n",
            "epoch 25 batch 13000 [130000/160000] training loss: 584.4411046504974\n",
            "epoch 25 batch 13100 [131000/160000] training loss: 568.4819116592407\n",
            "epoch 25 batch 13200 [132000/160000] training loss: 590.7099392414093\n",
            "epoch 25 batch 13300 [133000/160000] training loss: 570.9531953334808\n",
            "epoch 25 batch 13400 [134000/160000] training loss: 546.0973241329193\n",
            "epoch 25 batch 13500 [135000/160000] training loss: 563.1467745304108\n",
            "epoch 25 batch 13600 [136000/160000] training loss: 541.4790270328522\n",
            "epoch 25 batch 13700 [137000/160000] training loss: 599.9740778207779\n",
            "epoch 25 batch 13800 [138000/160000] training loss: 583.8493356704712\n",
            "epoch 25 batch 13900 [139000/160000] training loss: 598.4683529138565\n",
            "epoch 25 batch 14000 [140000/160000] training loss: 569.4098433256149\n",
            "epoch 25 batch 14100 [141000/160000] training loss: 540.0167381763458\n",
            "epoch 25 batch 14200 [142000/160000] training loss: 563.2641068696976\n",
            "epoch 25 batch 14300 [143000/160000] training loss: 560.9465155601501\n",
            "epoch 25 batch 14400 [144000/160000] training loss: 580.5914076566696\n",
            "epoch 25 batch 14500 [145000/160000] training loss: 563.313524723053\n",
            "epoch 25 batch 14600 [146000/160000] training loss: 558.300569653511\n",
            "epoch 25 batch 14700 [147000/160000] training loss: 552.3180768489838\n",
            "epoch 25 batch 14800 [148000/160000] training loss: 564.5646463632584\n",
            "epoch 25 batch 14900 [149000/160000] training loss: 554.8560693264008\n",
            "epoch 25 batch 15000 [150000/160000] training loss: 594.9238622188568\n",
            "epoch 25 batch 15100 [151000/160000] training loss: 568.6786258220673\n",
            "epoch 25 batch 15200 [152000/160000] training loss: 597.559103012085\n",
            "epoch 25 batch 15300 [153000/160000] training loss: 563.4585053920746\n",
            "epoch 25 batch 15400 [154000/160000] training loss: 584.0048378705978\n",
            "epoch 25 batch 15500 [155000/160000] training loss: 553.648934841156\n",
            "epoch 25 batch 15600 [156000/160000] training loss: 539.096114397049\n",
            "epoch 25 batch 15700 [157000/160000] training loss: 543.0704461336136\n",
            "epoch 25 batch 15800 [158000/160000] training loss: 559.106118440628\n",
            "epoch 25 batch 15900 [159000/160000] training loss: 550.916999578476\n",
            "epoch 26 batch 0 [0/160000] training loss: 5.611701011657715\n",
            "epoch 26 batch 100 [1000/160000] training loss: 569.3526389598846\n",
            "epoch 26 batch 200 [2000/160000] training loss: 573.5232439041138\n",
            "epoch 26 batch 300 [3000/160000] training loss: 569.2186677455902\n",
            "epoch 26 batch 400 [4000/160000] training loss: 572.3098487854004\n",
            "epoch 26 batch 500 [5000/160000] training loss: 577.7544848918915\n",
            "epoch 26 batch 600 [6000/160000] training loss: 564.5063621997833\n",
            "epoch 26 batch 700 [7000/160000] training loss: 548.93674492836\n",
            "epoch 26 batch 800 [8000/160000] training loss: 589.8905838727951\n",
            "epoch 26 batch 900 [9000/160000] training loss: 569.5787885189056\n",
            "epoch 26 batch 1000 [10000/160000] training loss: 580.8612701892853\n",
            "epoch 26 batch 1100 [11000/160000] training loss: 556.5636214017868\n",
            "epoch 26 batch 1200 [12000/160000] training loss: 550.8860156536102\n",
            "epoch 26 batch 1300 [13000/160000] training loss: 590.5263583660126\n",
            "epoch 26 batch 1400 [14000/160000] training loss: 563.1394410133362\n",
            "epoch 26 batch 1500 [15000/160000] training loss: 539.3789072036743\n",
            "epoch 26 batch 1600 [16000/160000] training loss: 572.5154500007629\n",
            "epoch 26 batch 1700 [17000/160000] training loss: 572.8085775375366\n",
            "epoch 26 batch 1800 [18000/160000] training loss: 563.8194199800491\n",
            "epoch 26 batch 1900 [19000/160000] training loss: 557.9821267127991\n",
            "epoch 26 batch 2000 [20000/160000] training loss: 541.2676000595093\n",
            "epoch 26 batch 2100 [21000/160000] training loss: 546.5415663719177\n",
            "epoch 26 batch 2200 [22000/160000] training loss: 588.7422577142715\n",
            "epoch 26 batch 2300 [23000/160000] training loss: 567.1708543300629\n",
            "epoch 26 batch 2400 [24000/160000] training loss: 556.6305739879608\n",
            "epoch 26 batch 2500 [25000/160000] training loss: 588.805002450943\n",
            "epoch 26 batch 2600 [26000/160000] training loss: 552.2849507331848\n",
            "epoch 26 batch 2700 [27000/160000] training loss: 584.054475903511\n",
            "epoch 26 batch 2800 [28000/160000] training loss: 528.2117339372635\n",
            "epoch 26 batch 2900 [29000/160000] training loss: 606.9180771112442\n",
            "epoch 26 batch 3000 [30000/160000] training loss: 577.5724997520447\n",
            "epoch 26 batch 3100 [31000/160000] training loss: 583.4990561008453\n",
            "epoch 26 batch 3200 [32000/160000] training loss: 550.9678392410278\n",
            "epoch 26 batch 3300 [33000/160000] training loss: 569.8166375160217\n",
            "epoch 26 batch 3400 [34000/160000] training loss: 572.4130084514618\n",
            "epoch 26 batch 3500 [35000/160000] training loss: 576.3442984819412\n",
            "epoch 26 batch 3600 [36000/160000] training loss: 565.8864402770996\n",
            "epoch 26 batch 3700 [37000/160000] training loss: 542.5867418050766\n",
            "epoch 26 batch 3800 [38000/160000] training loss: 560.8682689666748\n",
            "epoch 26 batch 3900 [39000/160000] training loss: 576.1550805568695\n",
            "epoch 26 batch 4000 [40000/160000] training loss: 566.0993783473969\n",
            "epoch 26 batch 4100 [41000/160000] training loss: 573.7253197431564\n",
            "epoch 26 batch 4200 [42000/160000] training loss: 593.0577120780945\n",
            "epoch 26 batch 4300 [43000/160000] training loss: 570.3265037536621\n",
            "epoch 26 batch 4400 [44000/160000] training loss: 551.9729589223862\n",
            "epoch 26 batch 4500 [45000/160000] training loss: 578.4437441825867\n",
            "epoch 26 batch 4600 [46000/160000] training loss: 534.3213728666306\n",
            "epoch 26 batch 4700 [47000/160000] training loss: 544.5252034664154\n",
            "epoch 26 batch 4800 [48000/160000] training loss: 565.4301830530167\n",
            "epoch 26 batch 4900 [49000/160000] training loss: 570.028862953186\n",
            "epoch 26 batch 5000 [50000/160000] training loss: 559.2400130033493\n",
            "epoch 26 batch 5100 [51000/160000] training loss: 552.2631781101227\n",
            "epoch 26 batch 5200 [52000/160000] training loss: 569.5459361076355\n",
            "epoch 26 batch 5300 [53000/160000] training loss: 564.3685510158539\n",
            "epoch 26 batch 5400 [54000/160000] training loss: 570.3675343990326\n",
            "epoch 26 batch 5500 [55000/160000] training loss: 553.5908801555634\n",
            "epoch 26 batch 5600 [56000/160000] training loss: 574.4740290641785\n",
            "epoch 26 batch 5700 [57000/160000] training loss: 567.230128288269\n",
            "epoch 26 batch 5800 [58000/160000] training loss: 571.6840616464615\n",
            "epoch 26 batch 5900 [59000/160000] training loss: 541.4075448513031\n",
            "epoch 26 batch 6000 [60000/160000] training loss: 582.3394529819489\n",
            "epoch 26 batch 6100 [61000/160000] training loss: 561.8951389789581\n",
            "epoch 26 batch 6200 [62000/160000] training loss: 592.3831176757812\n",
            "epoch 26 batch 6300 [63000/160000] training loss: 566.2096719741821\n",
            "epoch 26 batch 6400 [64000/160000] training loss: 547.4935274124146\n",
            "epoch 26 batch 6500 [65000/160000] training loss: 555.0182580947876\n",
            "epoch 26 batch 6600 [66000/160000] training loss: 532.4950361251831\n",
            "epoch 26 batch 6700 [67000/160000] training loss: 583.5038625001907\n",
            "epoch 26 batch 6800 [68000/160000] training loss: 531.2192606925964\n",
            "epoch 26 batch 6900 [69000/160000] training loss: 551.1013962030411\n",
            "epoch 26 batch 7000 [70000/160000] training loss: 533.5983077287674\n",
            "epoch 26 batch 7100 [71000/160000] training loss: 532.1622471809387\n",
            "epoch 26 batch 7200 [72000/160000] training loss: 560.9811010360718\n",
            "epoch 26 batch 7300 [73000/160000] training loss: 549.0152163505554\n",
            "epoch 26 batch 7400 [74000/160000] training loss: 535.2155261039734\n",
            "epoch 26 batch 7500 [75000/160000] training loss: 555.0040019750595\n",
            "epoch 26 batch 7600 [76000/160000] training loss: 563.911123752594\n",
            "epoch 26 batch 7700 [77000/160000] training loss: 587.2575023174286\n",
            "epoch 26 batch 7800 [78000/160000] training loss: 583.0159962177277\n",
            "epoch 26 batch 7900 [79000/160000] training loss: 552.4421648979187\n",
            "epoch 26 batch 8000 [80000/160000] training loss: 554.5192272663116\n",
            "epoch 26 batch 8100 [81000/160000] training loss: 541.0690085887909\n",
            "epoch 26 batch 8200 [82000/160000] training loss: 535.7344082593918\n",
            "epoch 26 batch 8300 [83000/160000] training loss: 579.2426062822342\n",
            "epoch 26 batch 8400 [84000/160000] training loss: 571.8219637870789\n",
            "epoch 26 batch 8500 [85000/160000] training loss: 583.5502367019653\n",
            "epoch 26 batch 8600 [86000/160000] training loss: 550.5858232975006\n",
            "epoch 26 batch 8700 [87000/160000] training loss: 555.6028952598572\n",
            "epoch 26 batch 8800 [88000/160000] training loss: 573.1861343383789\n",
            "epoch 26 batch 8900 [89000/160000] training loss: 563.9401171207428\n",
            "epoch 26 batch 9000 [90000/160000] training loss: 585.3723526000977\n",
            "epoch 26 batch 9100 [91000/160000] training loss: 585.5929030179977\n",
            "epoch 26 batch 9200 [92000/160000] training loss: 523.576044678688\n",
            "epoch 26 batch 9300 [93000/160000] training loss: 579.3991212844849\n",
            "epoch 26 batch 9400 [94000/160000] training loss: 531.9435163736343\n",
            "epoch 26 batch 9500 [95000/160000] training loss: 575.0040521621704\n",
            "epoch 26 batch 9600 [96000/160000] training loss: 555.2873393297195\n",
            "epoch 26 batch 9700 [97000/160000] training loss: 548.8771405220032\n",
            "epoch 26 batch 9800 [98000/160000] training loss: 550.6405513286591\n",
            "epoch 26 batch 9900 [99000/160000] training loss: 564.4080862998962\n",
            "epoch 26 batch 10000 [100000/160000] training loss: 584.5079947710037\n",
            "epoch 26 batch 10100 [101000/160000] training loss: 569.4360152482986\n",
            "epoch 26 batch 10200 [102000/160000] training loss: 543.1422245502472\n",
            "epoch 26 batch 10300 [103000/160000] training loss: 565.7434130907059\n",
            "epoch 26 batch 10400 [104000/160000] training loss: 570.226019859314\n",
            "epoch 26 batch 10500 [105000/160000] training loss: 601.0779402256012\n",
            "epoch 26 batch 10600 [106000/160000] training loss: 588.8305157423019\n",
            "epoch 26 batch 10700 [107000/160000] training loss: 556.3642235994339\n",
            "epoch 26 batch 10800 [108000/160000] training loss: 543.5193164348602\n",
            "epoch 26 batch 10900 [109000/160000] training loss: 539.9051266908646\n",
            "epoch 26 batch 11000 [110000/160000] training loss: 561.8444766998291\n",
            "epoch 26 batch 11100 [111000/160000] training loss: 547.3894214630127\n",
            "epoch 26 batch 11200 [112000/160000] training loss: 566.9154064655304\n",
            "epoch 26 batch 11300 [113000/160000] training loss: 556.9053342342377\n",
            "epoch 26 batch 11400 [114000/160000] training loss: 567.4894068241119\n",
            "epoch 26 batch 11500 [115000/160000] training loss: 570.3909304141998\n",
            "epoch 26 batch 11600 [116000/160000] training loss: 592.3652023077011\n",
            "epoch 26 batch 11700 [117000/160000] training loss: 539.4276144504547\n",
            "epoch 26 batch 11800 [118000/160000] training loss: 549.0804896354675\n",
            "epoch 26 batch 11900 [119000/160000] training loss: 545.9286072254181\n",
            "epoch 26 batch 12000 [120000/160000] training loss: 566.2872861623764\n",
            "epoch 26 batch 12100 [121000/160000] training loss: 584.7838900089264\n",
            "epoch 26 batch 12200 [122000/160000] training loss: 556.3996548652649\n",
            "epoch 26 batch 12300 [123000/160000] training loss: 564.3801379203796\n",
            "epoch 26 batch 12400 [124000/160000] training loss: 573.2002049684525\n",
            "epoch 26 batch 12500 [125000/160000] training loss: 553.2362463474274\n",
            "epoch 26 batch 12600 [126000/160000] training loss: 569.1529847383499\n",
            "epoch 26 batch 12700 [127000/160000] training loss: 547.2466957569122\n",
            "epoch 26 batch 12800 [128000/160000] training loss: 582.54066157341\n",
            "epoch 26 batch 12900 [129000/160000] training loss: 573.5693204402924\n",
            "epoch 26 batch 13000 [130000/160000] training loss: 584.4812533855438\n",
            "epoch 26 batch 13100 [131000/160000] training loss: 569.0138835906982\n",
            "epoch 26 batch 13200 [132000/160000] training loss: 590.5771241188049\n",
            "epoch 26 batch 13300 [133000/160000] training loss: 570.8497972488403\n",
            "epoch 26 batch 13400 [134000/160000] training loss: 545.9676992893219\n",
            "epoch 26 batch 13500 [135000/160000] training loss: 562.9871916770935\n",
            "epoch 26 batch 13600 [136000/160000] training loss: 541.5149126052856\n",
            "epoch 26 batch 13700 [137000/160000] training loss: 599.993754863739\n",
            "epoch 26 batch 13800 [138000/160000] training loss: 583.9448035955429\n",
            "epoch 26 batch 13900 [139000/160000] training loss: 598.4557611942291\n",
            "epoch 26 batch 14000 [140000/160000] training loss: 569.5301878452301\n",
            "epoch 26 batch 14100 [141000/160000] training loss: 539.4871656894684\n",
            "epoch 26 batch 14200 [142000/160000] training loss: 563.0146527290344\n",
            "epoch 26 batch 14300 [143000/160000] training loss: 560.7731628417969\n",
            "epoch 26 batch 14400 [144000/160000] training loss: 580.5447355508804\n",
            "epoch 26 batch 14500 [145000/160000] training loss: 563.039101600647\n",
            "epoch 26 batch 14600 [146000/160000] training loss: 558.2261672019958\n",
            "epoch 26 batch 14700 [147000/160000] training loss: 552.3496503829956\n",
            "epoch 26 batch 14800 [148000/160000] training loss: 564.3294758796692\n",
            "epoch 26 batch 14900 [149000/160000] training loss: 554.9861414432526\n",
            "epoch 26 batch 15000 [150000/160000] training loss: 594.6330209970474\n",
            "epoch 26 batch 15100 [151000/160000] training loss: 568.6516172885895\n",
            "epoch 26 batch 15200 [152000/160000] training loss: 597.2281577587128\n",
            "epoch 26 batch 15300 [153000/160000] training loss: 563.47893679142\n",
            "epoch 26 batch 15400 [154000/160000] training loss: 583.8313381671906\n",
            "epoch 26 batch 15500 [155000/160000] training loss: 553.6070058345795\n",
            "epoch 26 batch 15600 [156000/160000] training loss: 539.1061065196991\n",
            "epoch 26 batch 15700 [157000/160000] training loss: 542.9713648557663\n",
            "epoch 26 batch 15800 [158000/160000] training loss: 559.0769957304001\n",
            "epoch 26 batch 15900 [159000/160000] training loss: 551.3033468723297\n",
            "epoch 27 batch 0 [0/160000] training loss: 5.620683670043945\n",
            "epoch 27 batch 100 [1000/160000] training loss: 569.2441277503967\n",
            "epoch 27 batch 200 [2000/160000] training loss: 573.8005564212799\n",
            "epoch 27 batch 300 [3000/160000] training loss: 569.4848189353943\n",
            "epoch 27 batch 400 [4000/160000] training loss: 572.350079536438\n",
            "epoch 27 batch 500 [5000/160000] training loss: 577.7595727443695\n",
            "epoch 27 batch 600 [6000/160000] training loss: 564.6198406219482\n",
            "epoch 27 batch 700 [7000/160000] training loss: 548.8913304805756\n",
            "epoch 27 batch 800 [8000/160000] training loss: 590.3924697637558\n",
            "epoch 27 batch 900 [9000/160000] training loss: 569.6832040548325\n",
            "epoch 27 batch 1000 [10000/160000] training loss: 581.1035308837891\n",
            "epoch 27 batch 1100 [11000/160000] training loss: 556.7944707870483\n",
            "epoch 27 batch 1200 [12000/160000] training loss: 550.7238690853119\n",
            "epoch 27 batch 1300 [13000/160000] training loss: 590.1174671649933\n",
            "epoch 27 batch 1400 [14000/160000] training loss: 563.2235916852951\n",
            "epoch 27 batch 1500 [15000/160000] training loss: 539.5885541439056\n",
            "epoch 27 batch 1600 [16000/160000] training loss: 572.6039230823517\n",
            "epoch 27 batch 1700 [17000/160000] training loss: 572.5709319114685\n",
            "epoch 27 batch 1800 [18000/160000] training loss: 563.7021815776825\n",
            "epoch 27 batch 1900 [19000/160000] training loss: 558.0538054704666\n",
            "epoch 27 batch 2000 [20000/160000] training loss: 541.1388382911682\n",
            "epoch 27 batch 2100 [21000/160000] training loss: 546.6947140693665\n",
            "epoch 27 batch 2200 [22000/160000] training loss: 588.627119064331\n",
            "epoch 27 batch 2300 [23000/160000] training loss: 566.9022682905197\n",
            "epoch 27 batch 2400 [24000/160000] training loss: 556.8773883581161\n",
            "epoch 27 batch 2500 [25000/160000] training loss: 588.7345769405365\n",
            "epoch 27 batch 2600 [26000/160000] training loss: 552.1168985366821\n",
            "epoch 27 batch 2700 [27000/160000] training loss: 584.1683237552643\n",
            "epoch 27 batch 2800 [28000/160000] training loss: 528.2062042951584\n",
            "epoch 27 batch 2900 [29000/160000] training loss: 606.7923082113266\n",
            "epoch 27 batch 3000 [30000/160000] training loss: 577.4809331893921\n",
            "epoch 27 batch 3100 [31000/160000] training loss: 583.4982838630676\n",
            "epoch 27 batch 3200 [32000/160000] training loss: 550.5420770645142\n",
            "epoch 27 batch 3300 [33000/160000] training loss: 569.6641376018524\n",
            "epoch 27 batch 3400 [34000/160000] training loss: 572.58527302742\n",
            "epoch 27 batch 3500 [35000/160000] training loss: 576.2939611673355\n",
            "epoch 27 batch 3600 [36000/160000] training loss: 565.9335906505585\n",
            "epoch 27 batch 3700 [37000/160000] training loss: 542.3381654024124\n",
            "epoch 27 batch 3800 [38000/160000] training loss: 560.6899964809418\n",
            "epoch 27 batch 3900 [39000/160000] training loss: 576.2736217975616\n",
            "epoch 27 batch 4000 [40000/160000] training loss: 566.3727264404297\n",
            "epoch 27 batch 4100 [41000/160000] training loss: 573.572890162468\n",
            "epoch 27 batch 4200 [42000/160000] training loss: 593.2749226093292\n",
            "epoch 27 batch 4300 [43000/160000] training loss: 570.1546515226364\n",
            "epoch 27 batch 4400 [44000/160000] training loss: 551.8983101844788\n",
            "epoch 27 batch 4500 [45000/160000] training loss: 578.583122253418\n",
            "epoch 27 batch 4600 [46000/160000] training loss: 534.3291970491409\n",
            "epoch 27 batch 4700 [47000/160000] training loss: 544.4847292900085\n",
            "epoch 27 batch 4800 [48000/160000] training loss: 565.5095263719559\n",
            "epoch 27 batch 4900 [49000/160000] training loss: 570.2727422714233\n",
            "epoch 27 batch 5000 [50000/160000] training loss: 559.3319845199585\n",
            "epoch 27 batch 5100 [51000/160000] training loss: 552.210933804512\n",
            "epoch 27 batch 5200 [52000/160000] training loss: 569.8297212123871\n",
            "epoch 27 batch 5300 [53000/160000] training loss: 564.1263208389282\n",
            "epoch 27 batch 5400 [54000/160000] training loss: 570.558203458786\n",
            "epoch 27 batch 5500 [55000/160000] training loss: 553.328405380249\n",
            "epoch 27 batch 5600 [56000/160000] training loss: 574.550591468811\n",
            "epoch 27 batch 5700 [57000/160000] training loss: 567.1776350736618\n",
            "epoch 27 batch 5800 [58000/160000] training loss: 572.0853860378265\n",
            "epoch 27 batch 5900 [59000/160000] training loss: 541.3573217391968\n",
            "epoch 27 batch 6000 [60000/160000] training loss: 582.2389523983002\n",
            "epoch 27 batch 6100 [61000/160000] training loss: 561.5812835693359\n",
            "epoch 27 batch 6200 [62000/160000] training loss: 592.5146839618683\n",
            "epoch 27 batch 6300 [63000/160000] training loss: 566.522043466568\n",
            "epoch 27 batch 6400 [64000/160000] training loss: 547.6742366552353\n",
            "epoch 27 batch 6500 [65000/160000] training loss: 554.8149738311768\n",
            "epoch 27 batch 6600 [66000/160000] training loss: 532.0933618545532\n",
            "epoch 27 batch 6700 [67000/160000] training loss: 583.5611500740051\n",
            "epoch 27 batch 6800 [68000/160000] training loss: 530.7079081535339\n",
            "epoch 27 batch 6900 [69000/160000] training loss: 551.448735833168\n",
            "epoch 27 batch 7000 [70000/160000] training loss: 533.1119099855423\n",
            "epoch 27 batch 7100 [71000/160000] training loss: 532.3086395263672\n",
            "epoch 27 batch 7200 [72000/160000] training loss: 560.8809415102005\n",
            "epoch 27 batch 7300 [73000/160000] training loss: 549.082218170166\n",
            "epoch 27 batch 7400 [74000/160000] training loss: 534.9756898880005\n",
            "epoch 27 batch 7500 [75000/160000] training loss: 554.9513689279556\n",
            "epoch 27 batch 7600 [76000/160000] training loss: 563.6500380039215\n",
            "epoch 27 batch 7700 [77000/160000] training loss: 587.2271845340729\n",
            "epoch 27 batch 7800 [78000/160000] training loss: 582.5445413589478\n",
            "epoch 27 batch 7900 [79000/160000] training loss: 552.3896548748016\n",
            "epoch 27 batch 8000 [80000/160000] training loss: 554.636855840683\n",
            "epoch 27 batch 8100 [81000/160000] training loss: 540.908416390419\n",
            "epoch 27 batch 8200 [82000/160000] training loss: 535.5251045227051\n",
            "epoch 27 batch 8300 [83000/160000] training loss: 579.3109753131866\n",
            "epoch 27 batch 8400 [84000/160000] training loss: 571.8997135162354\n",
            "epoch 27 batch 8500 [85000/160000] training loss: 583.7669153213501\n",
            "epoch 27 batch 8600 [86000/160000] training loss: 550.8489408493042\n",
            "epoch 27 batch 8700 [87000/160000] training loss: 555.3087940216064\n",
            "epoch 27 batch 8800 [88000/160000] training loss: 573.0985591411591\n",
            "epoch 27 batch 8900 [89000/160000] training loss: 563.4957392215729\n",
            "epoch 27 batch 9000 [90000/160000] training loss: 585.1398005485535\n",
            "epoch 27 batch 9100 [91000/160000] training loss: 585.5839525461197\n",
            "epoch 27 batch 9200 [92000/160000] training loss: 523.6406002044678\n",
            "epoch 27 batch 9300 [93000/160000] training loss: 579.4684188365936\n",
            "epoch 27 batch 9400 [94000/160000] training loss: 532.0988111495972\n",
            "epoch 27 batch 9500 [95000/160000] training loss: 575.1516954898834\n",
            "epoch 27 batch 9600 [96000/160000] training loss: 555.4998363256454\n",
            "epoch 27 batch 9700 [97000/160000] training loss: 548.5100709199905\n",
            "epoch 27 batch 9800 [98000/160000] training loss: 550.1427102088928\n",
            "epoch 27 batch 9900 [99000/160000] training loss: 564.3541684150696\n",
            "epoch 27 batch 10000 [100000/160000] training loss: 584.1694036722183\n",
            "epoch 27 batch 10100 [101000/160000] training loss: 569.3140429258347\n",
            "epoch 27 batch 10200 [102000/160000] training loss: 543.5255932807922\n",
            "epoch 27 batch 10300 [103000/160000] training loss: 565.2870218753815\n",
            "epoch 27 batch 10400 [104000/160000] training loss: 570.2081120014191\n",
            "epoch 27 batch 10500 [105000/160000] training loss: 601.1434705257416\n",
            "epoch 27 batch 10600 [106000/160000] training loss: 589.0478495359421\n",
            "epoch 27 batch 10700 [107000/160000] training loss: 556.0427623987198\n",
            "epoch 27 batch 10800 [108000/160000] training loss: 543.4859127998352\n",
            "epoch 27 batch 10900 [109000/160000] training loss: 539.6078903675079\n",
            "epoch 27 batch 11000 [110000/160000] training loss: 561.3896591663361\n",
            "epoch 27 batch 11100 [111000/160000] training loss: 546.8710985183716\n",
            "epoch 27 batch 11200 [112000/160000] training loss: 566.7420164346695\n",
            "epoch 27 batch 11300 [113000/160000] training loss: 557.1016108989716\n",
            "epoch 27 batch 11400 [114000/160000] training loss: 567.5630638599396\n",
            "epoch 27 batch 11500 [115000/160000] training loss: 570.2504284381866\n",
            "epoch 27 batch 11600 [116000/160000] training loss: 592.3651354312897\n",
            "epoch 27 batch 11700 [117000/160000] training loss: 539.3489043712616\n",
            "epoch 27 batch 11800 [118000/160000] training loss: 548.812626004219\n",
            "epoch 27 batch 11900 [119000/160000] training loss: 545.8490320444107\n",
            "epoch 27 batch 12000 [120000/160000] training loss: 566.3135912418365\n",
            "epoch 27 batch 12100 [121000/160000] training loss: 584.6170239448547\n",
            "epoch 27 batch 12200 [122000/160000] training loss: 556.441333770752\n",
            "epoch 27 batch 12300 [123000/160000] training loss: 564.464921951294\n",
            "epoch 27 batch 12400 [124000/160000] training loss: 573.4686331748962\n",
            "epoch 27 batch 12500 [125000/160000] training loss: 553.2198716402054\n",
            "epoch 27 batch 12600 [126000/160000] training loss: 569.2477246522903\n",
            "epoch 27 batch 12700 [127000/160000] training loss: 546.8502204418182\n",
            "epoch 27 batch 12800 [128000/160000] training loss: 582.9458613395691\n",
            "epoch 27 batch 12900 [129000/160000] training loss: 573.3514595031738\n",
            "epoch 27 batch 13000 [130000/160000] training loss: 584.535151720047\n",
            "epoch 27 batch 13100 [131000/160000] training loss: 569.5281496047974\n",
            "epoch 27 batch 13200 [132000/160000] training loss: 590.4360103607178\n",
            "epoch 27 batch 13300 [133000/160000] training loss: 570.74347448349\n",
            "epoch 27 batch 13400 [134000/160000] training loss: 545.8290410041809\n",
            "epoch 27 batch 13500 [135000/160000] training loss: 562.7996692657471\n",
            "epoch 27 batch 13600 [136000/160000] training loss: 541.5554611682892\n",
            "epoch 27 batch 13700 [137000/160000] training loss: 600.010057926178\n",
            "epoch 27 batch 13800 [138000/160000] training loss: 584.0111162662506\n",
            "epoch 27 batch 13900 [139000/160000] training loss: 598.4455217123032\n",
            "epoch 27 batch 14000 [140000/160000] training loss: 569.6735415458679\n",
            "epoch 27 batch 14100 [141000/160000] training loss: 538.990348815918\n",
            "epoch 27 batch 14200 [142000/160000] training loss: 562.7620812654495\n",
            "epoch 27 batch 14300 [143000/160000] training loss: 560.607460975647\n",
            "epoch 27 batch 14400 [144000/160000] training loss: 580.5175002813339\n",
            "epoch 27 batch 14500 [145000/160000] training loss: 562.7723877429962\n",
            "epoch 27 batch 14600 [146000/160000] training loss: 558.1516218185425\n",
            "epoch 27 batch 14700 [147000/160000] training loss: 552.4113230705261\n",
            "epoch 27 batch 14800 [148000/160000] training loss: 564.0820318460464\n",
            "epoch 27 batch 14900 [149000/160000] training loss: 555.1466624736786\n",
            "epoch 27 batch 15000 [150000/160000] training loss: 594.3323427438736\n",
            "epoch 27 batch 15100 [151000/160000] training loss: 568.6255512237549\n",
            "epoch 27 batch 15200 [152000/160000] training loss: 596.8851108551025\n",
            "epoch 27 batch 15300 [153000/160000] training loss: 563.5098201036453\n",
            "epoch 27 batch 15400 [154000/160000] training loss: 583.6865983009338\n",
            "epoch 27 batch 15500 [155000/160000] training loss: 553.5776917934418\n",
            "epoch 27 batch 15600 [156000/160000] training loss: 539.102358341217\n",
            "epoch 27 batch 15700 [157000/160000] training loss: 542.8838287591934\n",
            "epoch 27 batch 15800 [158000/160000] training loss: 559.0686461925507\n",
            "epoch 27 batch 15900 [159000/160000] training loss: 551.6758284568787\n",
            "epoch 28 batch 0 [0/160000] training loss: 5.628626823425293\n",
            "epoch 28 batch 100 [1000/160000] training loss: 569.1083779335022\n",
            "epoch 28 batch 200 [2000/160000] training loss: 574.0706144571304\n",
            "epoch 28 batch 300 [3000/160000] training loss: 569.7561991214752\n",
            "epoch 28 batch 400 [4000/160000] training loss: 572.3752634525299\n",
            "epoch 28 batch 500 [5000/160000] training loss: 577.733208656311\n",
            "epoch 28 batch 600 [6000/160000] training loss: 564.7266894578934\n",
            "epoch 28 batch 700 [7000/160000] training loss: 548.8568524122238\n",
            "epoch 28 batch 800 [8000/160000] training loss: 590.8612965345383\n",
            "epoch 28 batch 900 [9000/160000] training loss: 569.7881557941437\n",
            "epoch 28 batch 1000 [10000/160000] training loss: 581.3293011188507\n",
            "epoch 28 batch 1100 [11000/160000] training loss: 557.0182476043701\n",
            "epoch 28 batch 1200 [12000/160000] training loss: 550.5721881389618\n",
            "epoch 28 batch 1300 [13000/160000] training loss: 589.672992348671\n",
            "epoch 28 batch 1400 [14000/160000] training loss: 563.2875022888184\n",
            "epoch 28 batch 1500 [15000/160000] training loss: 539.7976610660553\n",
            "epoch 28 batch 1600 [16000/160000] training loss: 572.7035474777222\n",
            "epoch 28 batch 1700 [17000/160000] training loss: 572.3296995162964\n",
            "epoch 28 batch 1800 [18000/160000] training loss: 563.5708233118057\n",
            "epoch 28 batch 1900 [19000/160000] training loss: 558.1353522539139\n",
            "epoch 28 batch 2000 [20000/160000] training loss: 541.0224200487137\n",
            "epoch 28 batch 2100 [21000/160000] training loss: 546.8576831817627\n",
            "epoch 28 batch 2200 [22000/160000] training loss: 588.5216325521469\n",
            "epoch 28 batch 2300 [23000/160000] training loss: 566.6405029296875\n",
            "epoch 28 batch 2400 [24000/160000] training loss: 557.1231100559235\n",
            "epoch 28 batch 2500 [25000/160000] training loss: 588.6772618293762\n",
            "epoch 28 batch 2600 [26000/160000] training loss: 551.9601149559021\n",
            "epoch 28 batch 2700 [27000/160000] training loss: 584.2695109844208\n",
            "epoch 28 batch 2800 [28000/160000] training loss: 528.2193731069565\n",
            "epoch 28 batch 2900 [29000/160000] training loss: 606.6865998506546\n",
            "epoch 28 batch 3000 [30000/160000] training loss: 577.364768743515\n",
            "epoch 28 batch 3100 [31000/160000] training loss: 583.5066840648651\n",
            "epoch 28 batch 3200 [32000/160000] training loss: 550.1322197914124\n",
            "epoch 28 batch 3300 [33000/160000] training loss: 569.5145605802536\n",
            "epoch 28 batch 3400 [34000/160000] training loss: 572.7495684623718\n",
            "epoch 28 batch 3500 [35000/160000] training loss: 576.269205570221\n",
            "epoch 28 batch 3600 [36000/160000] training loss: 565.9773259162903\n",
            "epoch 28 batch 3700 [37000/160000] training loss: 542.1053216457367\n",
            "epoch 28 batch 3800 [38000/160000] training loss: 560.503185749054\n",
            "epoch 28 batch 3900 [39000/160000] training loss: 576.3726983070374\n",
            "epoch 28 batch 4000 [40000/160000] training loss: 566.6334855556488\n",
            "epoch 28 batch 4100 [41000/160000] training loss: 573.4074950218201\n",
            "epoch 28 batch 4200 [42000/160000] training loss: 593.5038909912109\n",
            "epoch 28 batch 4300 [43000/160000] training loss: 569.9971815347672\n",
            "epoch 28 batch 4400 [44000/160000] training loss: 551.821583032608\n",
            "epoch 28 batch 4500 [45000/160000] training loss: 578.7087321281433\n",
            "epoch 28 batch 4600 [46000/160000] training loss: 534.3502279520035\n",
            "epoch 28 batch 4700 [47000/160000] training loss: 544.4438941478729\n",
            "epoch 28 batch 4800 [48000/160000] training loss: 565.5777243375778\n",
            "epoch 28 batch 4900 [49000/160000] training loss: 570.5203552246094\n",
            "epoch 28 batch 5000 [50000/160000] training loss: 559.4236263036728\n",
            "epoch 28 batch 5100 [51000/160000] training loss: 552.1777061223984\n",
            "epoch 28 batch 5200 [52000/160000] training loss: 570.131998538971\n",
            "epoch 28 batch 5300 [53000/160000] training loss: 563.8780951499939\n",
            "epoch 28 batch 5400 [54000/160000] training loss: 570.7275972366333\n",
            "epoch 28 batch 5500 [55000/160000] training loss: 553.0683671236038\n",
            "epoch 28 batch 5600 [56000/160000] training loss: 574.6424392461777\n",
            "epoch 28 batch 5700 [57000/160000] training loss: 567.1306743621826\n",
            "epoch 28 batch 5800 [58000/160000] training loss: 572.4769974946976\n",
            "epoch 28 batch 5900 [59000/160000] training loss: 541.3248629570007\n",
            "epoch 28 batch 6000 [60000/160000] training loss: 582.150554895401\n",
            "epoch 28 batch 6100 [61000/160000] training loss: 561.2852432727814\n",
            "epoch 28 batch 6200 [62000/160000] training loss: 592.6318122148514\n",
            "epoch 28 batch 6300 [63000/160000] training loss: 566.825975894928\n",
            "epoch 28 batch 6400 [64000/160000] training loss: 547.8567178249359\n",
            "epoch 28 batch 6500 [65000/160000] training loss: 554.6190189123154\n",
            "epoch 28 batch 6600 [66000/160000] training loss: 531.6984438896179\n",
            "epoch 28 batch 6700 [67000/160000] training loss: 583.6155952215195\n",
            "epoch 28 batch 6800 [68000/160000] training loss: 530.2133514881134\n",
            "epoch 28 batch 6900 [69000/160000] training loss: 551.8110148906708\n",
            "epoch 28 batch 7000 [70000/160000] training loss: 532.6481128931046\n",
            "epoch 28 batch 7100 [71000/160000] training loss: 532.4492123126984\n",
            "epoch 28 batch 7200 [72000/160000] training loss: 560.7846782207489\n",
            "epoch 28 batch 7300 [73000/160000] training loss: 549.1596471071243\n",
            "epoch 28 batch 7400 [74000/160000] training loss: 534.7564182281494\n",
            "epoch 28 batch 7500 [75000/160000] training loss: 554.9435772895813\n",
            "epoch 28 batch 7600 [76000/160000] training loss: 563.3894444704056\n",
            "epoch 28 batch 7700 [77000/160000] training loss: 587.1977889537811\n",
            "epoch 28 batch 7800 [78000/160000] training loss: 582.080570936203\n",
            "epoch 28 batch 7900 [79000/160000] training loss: 552.3324240446091\n",
            "epoch 28 batch 8000 [80000/160000] training loss: 554.7628650665283\n",
            "epoch 28 batch 8100 [81000/160000] training loss: 540.7490895986557\n",
            "epoch 28 batch 8200 [82000/160000] training loss: 535.3195424079895\n",
            "epoch 28 batch 8300 [83000/160000] training loss: 579.374493598938\n",
            "epoch 28 batch 8400 [84000/160000] training loss: 571.991525888443\n",
            "epoch 28 batch 8500 [85000/160000] training loss: 583.9867500066757\n",
            "epoch 28 batch 8600 [86000/160000] training loss: 551.1154434680939\n",
            "epoch 28 batch 8700 [87000/160000] training loss: 555.018447637558\n",
            "epoch 28 batch 8800 [88000/160000] training loss: 573.0195389986038\n",
            "epoch 28 batch 8900 [89000/160000] training loss: 563.0759127140045\n",
            "epoch 28 batch 9000 [90000/160000] training loss: 584.9088628292084\n",
            "epoch 28 batch 9100 [91000/160000] training loss: 585.5589147806168\n",
            "epoch 28 batch 9200 [92000/160000] training loss: 523.7078374624252\n",
            "epoch 28 batch 9300 [93000/160000] training loss: 579.5403237342834\n",
            "epoch 28 batch 9400 [94000/160000] training loss: 532.2658959627151\n",
            "epoch 28 batch 9500 [95000/160000] training loss: 575.2774384021759\n",
            "epoch 28 batch 9600 [96000/160000] training loss: 555.7278507947922\n",
            "epoch 28 batch 9700 [97000/160000] training loss: 548.140278339386\n",
            "epoch 28 batch 9800 [98000/160000] training loss: 549.6724565029144\n",
            "epoch 28 batch 9900 [99000/160000] training loss: 564.2830326557159\n",
            "epoch 28 batch 10000 [100000/160000] training loss: 583.8386449813843\n",
            "epoch 28 batch 10100 [101000/160000] training loss: 569.1941255331039\n",
            "epoch 28 batch 10200 [102000/160000] training loss: 543.905609369278\n",
            "epoch 28 batch 10300 [103000/160000] training loss: 564.8465086221695\n",
            "epoch 28 batch 10400 [104000/160000] training loss: 570.1816457509995\n",
            "epoch 28 batch 10500 [105000/160000] training loss: 601.2273766994476\n",
            "epoch 28 batch 10600 [106000/160000] training loss: 589.2747733592987\n",
            "epoch 28 batch 10700 [107000/160000] training loss: 555.7237244844437\n",
            "epoch 28 batch 10800 [108000/160000] training loss: 543.4706732034683\n",
            "epoch 28 batch 10900 [109000/160000] training loss: 539.3352768421173\n",
            "epoch 28 batch 11000 [110000/160000] training loss: 560.9312428236008\n",
            "epoch 28 batch 11100 [111000/160000] training loss: 546.3701298236847\n",
            "epoch 28 batch 11200 [112000/160000] training loss: 566.5596210956573\n",
            "epoch 28 batch 11300 [113000/160000] training loss: 557.2912991046906\n",
            "epoch 28 batch 11400 [114000/160000] training loss: 567.658162355423\n",
            "epoch 28 batch 11500 [115000/160000] training loss: 570.0845994949341\n",
            "epoch 28 batch 11600 [116000/160000] training loss: 592.3948240280151\n",
            "epoch 28 batch 11700 [117000/160000] training loss: 539.2941720485687\n",
            "epoch 28 batch 11800 [118000/160000] training loss: 548.5522503852844\n",
            "epoch 28 batch 11900 [119000/160000] training loss: 545.773609161377\n",
            "epoch 28 batch 12000 [120000/160000] training loss: 566.3362928628922\n",
            "epoch 28 batch 12100 [121000/160000] training loss: 584.4730415344238\n",
            "epoch 28 batch 12200 [122000/160000] training loss: 556.4812452793121\n",
            "epoch 28 batch 12300 [123000/160000] training loss: 564.5414550304413\n",
            "epoch 28 batch 12400 [124000/160000] training loss: 573.7469701766968\n",
            "epoch 28 batch 12500 [125000/160000] training loss: 553.2220125198364\n",
            "epoch 28 batch 12600 [126000/160000] training loss: 569.3416850566864\n",
            "epoch 28 batch 12700 [127000/160000] training loss: 546.4617002010345\n",
            "epoch 28 batch 12800 [128000/160000] training loss: 583.3411235809326\n",
            "epoch 28 batch 12900 [129000/160000] training loss: 573.1366686820984\n",
            "epoch 28 batch 13000 [130000/160000] training loss: 584.601972579956\n",
            "epoch 28 batch 13100 [131000/160000] training loss: 570.0259065628052\n",
            "epoch 28 batch 13200 [132000/160000] training loss: 590.2876334190369\n",
            "epoch 28 batch 13300 [133000/160000] training loss: 570.6316680908203\n",
            "epoch 28 batch 13400 [134000/160000] training loss: 545.680917263031\n",
            "epoch 28 batch 13500 [135000/160000] training loss: 562.5874109268188\n",
            "epoch 28 batch 13600 [136000/160000] training loss: 541.6020305156708\n",
            "epoch 28 batch 13700 [137000/160000] training loss: 600.0236268043518\n",
            "epoch 28 batch 13800 [138000/160000] training loss: 584.0505652427673\n",
            "epoch 28 batch 13900 [139000/160000] training loss: 598.4394985437393\n",
            "epoch 28 batch 14000 [140000/160000] training loss: 569.8365459442139\n",
            "epoch 28 batch 14100 [141000/160000] training loss: 538.5226805210114\n",
            "epoch 28 batch 14200 [142000/160000] training loss: 562.5084826946259\n",
            "epoch 28 batch 14300 [143000/160000] training loss: 560.4459308385849\n",
            "epoch 28 batch 14400 [144000/160000] training loss: 580.507607460022\n",
            "epoch 28 batch 14500 [145000/160000] training loss: 562.5129692554474\n",
            "epoch 28 batch 14600 [146000/160000] training loss: 558.0794752836227\n",
            "epoch 28 batch 14700 [147000/160000] training loss: 552.4990231990814\n",
            "epoch 28 batch 14800 [148000/160000] training loss: 563.8252345323563\n",
            "epoch 28 batch 14900 [149000/160000] training loss: 555.331268787384\n",
            "epoch 28 batch 15000 [150000/160000] training loss: 594.0208693742752\n",
            "epoch 28 batch 15100 [151000/160000] training loss: 568.596468091011\n",
            "epoch 28 batch 15200 [152000/160000] training loss: 596.5319769382477\n",
            "epoch 28 batch 15300 [153000/160000] training loss: 563.5532479286194\n",
            "epoch 28 batch 15400 [154000/160000] training loss: 583.568570971489\n",
            "epoch 28 batch 15500 [155000/160000] training loss: 553.561363697052\n",
            "epoch 28 batch 15600 [156000/160000] training loss: 539.0835225582123\n",
            "epoch 28 batch 15700 [157000/160000] training loss: 542.8086354732513\n",
            "epoch 28 batch 15800 [158000/160000] training loss: 559.0780715942383\n",
            "epoch 28 batch 15900 [159000/160000] training loss: 552.0338780879974\n",
            "epoch 29 batch 0 [0/160000] training loss: 5.63555908203125\n",
            "epoch 29 batch 100 [1000/160000] training loss: 568.9504631757736\n",
            "epoch 29 batch 200 [2000/160000] training loss: 574.3337604999542\n",
            "epoch 29 batch 300 [3000/160000] training loss: 570.0341973304749\n",
            "epoch 29 batch 400 [4000/160000] training loss: 572.390932559967\n",
            "epoch 29 batch 500 [5000/160000] training loss: 577.6796679496765\n",
            "epoch 29 batch 600 [6000/160000] training loss: 564.829509139061\n",
            "epoch 29 batch 700 [7000/160000] training loss: 548.835047006607\n",
            "epoch 29 batch 800 [8000/160000] training loss: 591.3016157150269\n",
            "epoch 29 batch 900 [9000/160000] training loss: 569.8937513828278\n",
            "epoch 29 batch 1000 [10000/160000] training loss: 581.5413997173309\n",
            "epoch 29 batch 1100 [11000/160000] training loss: 557.2356456518173\n",
            "epoch 29 batch 1200 [12000/160000] training loss: 550.4311773777008\n",
            "epoch 29 batch 1300 [13000/160000] training loss: 589.2017568349838\n",
            "epoch 29 batch 1400 [14000/160000] training loss: 563.3345044851303\n",
            "epoch 29 batch 1500 [15000/160000] training loss: 540.0065667629242\n",
            "epoch 29 batch 1600 [16000/160000] training loss: 572.8156335353851\n",
            "epoch 29 batch 1700 [17000/160000] training loss: 572.0873782634735\n",
            "epoch 29 batch 1800 [18000/160000] training loss: 563.4292142391205\n",
            "epoch 29 batch 1900 [19000/160000] training loss: 558.2271581888199\n",
            "epoch 29 batch 2000 [20000/160000] training loss: 540.9197814464569\n",
            "epoch 29 batch 2100 [21000/160000] training loss: 547.02982878685\n",
            "epoch 29 batch 2200 [22000/160000] training loss: 588.4248738288879\n",
            "epoch 29 batch 2300 [23000/160000] training loss: 566.3867038488388\n",
            "epoch 29 batch 2400 [24000/160000] training loss: 557.3683992624283\n",
            "epoch 29 batch 2500 [25000/160000] training loss: 588.6313672065735\n",
            "epoch 29 batch 2600 [26000/160000] training loss: 551.8147943019867\n",
            "epoch 29 batch 2700 [27000/160000] training loss: 584.3578717708588\n",
            "epoch 29 batch 2800 [28000/160000] training loss: 528.2502822875977\n",
            "epoch 29 batch 2900 [29000/160000] training loss: 606.6005326509476\n",
            "epoch 29 batch 3000 [30000/160000] training loss: 577.2273285388947\n",
            "epoch 29 batch 3100 [31000/160000] training loss: 583.5249962806702\n",
            "epoch 29 batch 3200 [32000/160000] training loss: 549.7364869117737\n",
            "epoch 29 batch 3300 [33000/160000] training loss: 569.368973493576\n",
            "epoch 29 batch 3400 [34000/160000] training loss: 572.9064002037048\n",
            "epoch 29 batch 3500 [35000/160000] training loss: 576.267326593399\n",
            "epoch 29 batch 3600 [36000/160000] training loss: 566.0174791812897\n",
            "epoch 29 batch 3700 [37000/160000] training loss: 541.8881276845932\n",
            "epoch 29 batch 3800 [38000/160000] training loss: 560.307507276535\n",
            "epoch 29 batch 3900 [39000/160000] training loss: 576.4521751403809\n",
            "epoch 29 batch 4000 [40000/160000] training loss: 566.8804662227631\n",
            "epoch 29 batch 4100 [41000/160000] training loss: 573.2299284934998\n",
            "epoch 29 batch 4200 [42000/160000] training loss: 593.7439205646515\n",
            "epoch 29 batch 4300 [43000/160000] training loss: 569.8536013364792\n",
            "epoch 29 batch 4400 [44000/160000] training loss: 551.7411930561066\n",
            "epoch 29 batch 4500 [45000/160000] training loss: 578.821679353714\n",
            "epoch 29 batch 4600 [46000/160000] training loss: 534.3831448554993\n",
            "epoch 29 batch 4700 [47000/160000] training loss: 544.4023013114929\n",
            "epoch 29 batch 4800 [48000/160000] training loss: 565.6354349851608\n",
            "epoch 29 batch 4900 [49000/160000] training loss: 570.7698268890381\n",
            "epoch 29 batch 5000 [50000/160000] training loss: 559.5159567594528\n",
            "epoch 29 batch 5100 [51000/160000] training loss: 552.1638585329056\n",
            "epoch 29 batch 5200 [52000/160000] training loss: 570.4515604972839\n",
            "epoch 29 batch 5300 [53000/160000] training loss: 563.6240849494934\n",
            "epoch 29 batch 5400 [54000/160000] training loss: 570.8778959512711\n",
            "epoch 29 batch 5500 [55000/160000] training loss: 552.8123315572739\n",
            "epoch 29 batch 5600 [56000/160000] training loss: 574.7481673955917\n",
            "epoch 29 batch 5700 [57000/160000] training loss: 567.0862408876419\n",
            "epoch 29 batch 5800 [58000/160000] training loss: 572.8560492992401\n",
            "epoch 29 batch 5900 [59000/160000] training loss: 541.3083183765411\n",
            "epoch 29 batch 6000 [60000/160000] training loss: 582.073341012001\n",
            "epoch 29 batch 6100 [61000/160000] training loss: 561.0028151273727\n",
            "epoch 29 batch 6200 [62000/160000] training loss: 592.7355595827103\n",
            "epoch 29 batch 6300 [63000/160000] training loss: 567.120628118515\n",
            "epoch 29 batch 6400 [64000/160000] training loss: 548.0367125272751\n",
            "epoch 29 batch 6500 [65000/160000] training loss: 554.4281380176544\n",
            "epoch 29 batch 6600 [66000/160000] training loss: 531.3096899986267\n",
            "epoch 29 batch 6700 [67000/160000] training loss: 583.6681710481644\n",
            "epoch 29 batch 6800 [68000/160000] training loss: 529.7366616725922\n",
            "epoch 29 batch 6900 [69000/160000] training loss: 552.1849901676178\n",
            "epoch 29 batch 7000 [70000/160000] training loss: 532.20589864254\n",
            "epoch 29 batch 7100 [71000/160000] training loss: 532.5818153619766\n",
            "epoch 29 batch 7200 [72000/160000] training loss: 560.6944320201874\n",
            "epoch 29 batch 7300 [73000/160000] training loss: 549.2479248046875\n",
            "epoch 29 batch 7400 [74000/160000] training loss: 534.5544939041138\n",
            "epoch 29 batch 7500 [75000/160000] training loss: 554.9784883260727\n",
            "epoch 29 batch 7600 [76000/160000] training loss: 563.1271209716797\n",
            "epoch 29 batch 7700 [77000/160000] training loss: 587.1693215370178\n",
            "epoch 29 batch 7800 [78000/160000] training loss: 581.6265912055969\n",
            "epoch 29 batch 7900 [79000/160000] training loss: 552.2697161436081\n",
            "epoch 29 batch 8000 [80000/160000] training loss: 554.8962965011597\n",
            "epoch 29 batch 8100 [81000/160000] training loss: 540.5918469429016\n",
            "epoch 29 batch 8200 [82000/160000] training loss: 535.1175756454468\n",
            "epoch 29 batch 8300 [83000/160000] training loss: 579.4318764209747\n",
            "epoch 29 batch 8400 [84000/160000] training loss: 572.0965478420258\n",
            "epoch 29 batch 8500 [85000/160000] training loss: 584.2090624570847\n",
            "epoch 29 batch 8600 [86000/160000] training loss: 551.3842587471008\n",
            "epoch 29 batch 8700 [87000/160000] training loss: 554.7309358119965\n",
            "epoch 29 batch 8800 [88000/160000] training loss: 572.9479665756226\n",
            "epoch 29 batch 8900 [89000/160000] training loss: 562.6781395673752\n",
            "epoch 29 batch 9000 [90000/160000] training loss: 584.6820113658905\n",
            "epoch 29 batch 9100 [91000/160000] training loss: 585.5170351266861\n",
            "epoch 29 batch 9200 [92000/160000] training loss: 523.7771019935608\n",
            "epoch 29 batch 9300 [93000/160000] training loss: 579.6134566068649\n",
            "epoch 29 batch 9400 [94000/160000] training loss: 532.4434430599213\n",
            "epoch 29 batch 9500 [95000/160000] training loss: 575.3805814981461\n",
            "epoch 29 batch 9600 [96000/160000] training loss: 555.9687702655792\n",
            "epoch 29 batch 9700 [97000/160000] training loss: 547.7678537368774\n",
            "epoch 29 batch 9800 [98000/160000] training loss: 549.230259180069\n",
            "epoch 29 batch 9900 [99000/160000] training loss: 564.1960005760193\n",
            "epoch 29 batch 10000 [100000/160000] training loss: 583.5157008171082\n",
            "epoch 29 batch 10100 [101000/160000] training loss: 569.0764904022217\n",
            "epoch 29 batch 10200 [102000/160000] training loss: 544.2822642326355\n",
            "epoch 29 batch 10300 [103000/160000] training loss: 564.4219696521759\n",
            "epoch 29 batch 10400 [104000/160000] training loss: 570.1447459459305\n",
            "epoch 29 batch 10500 [105000/160000] training loss: 601.3272581100464\n",
            "epoch 29 batch 10600 [106000/160000] training loss: 589.5086599588394\n",
            "epoch 29 batch 10700 [107000/160000] training loss: 555.4071708917618\n",
            "epoch 29 batch 10800 [108000/160000] training loss: 543.4728300571442\n",
            "epoch 29 batch 10900 [109000/160000] training loss: 539.0837531089783\n",
            "epoch 29 batch 11000 [110000/160000] training loss: 560.472079873085\n",
            "epoch 29 batch 11100 [111000/160000] training loss: 545.8873581886292\n",
            "epoch 29 batch 11200 [112000/160000] training loss: 566.3705018758774\n",
            "epoch 29 batch 11300 [113000/160000] training loss: 557.4753361940384\n",
            "epoch 29 batch 11400 [114000/160000] training loss: 567.7701025009155\n",
            "epoch 29 batch 11500 [115000/160000] training loss: 569.8953349590302\n",
            "epoch 29 batch 11600 [116000/160000] training loss: 592.4549256563187\n",
            "epoch 29 batch 11700 [117000/160000] training loss: 539.260320186615\n",
            "epoch 29 batch 11800 [118000/160000] training loss: 548.3006300926208\n",
            "epoch 29 batch 11900 [119000/160000] training loss: 545.7043610811234\n",
            "epoch 29 batch 12000 [120000/160000] training loss: 566.3561308383942\n",
            "epoch 29 batch 12100 [121000/160000] training loss: 584.3494329452515\n",
            "epoch 29 batch 12200 [122000/160000] training loss: 556.5194549560547\n",
            "epoch 29 batch 12300 [123000/160000] training loss: 564.6096000671387\n",
            "epoch 29 batch 12400 [124000/160000] training loss: 574.0334252119064\n",
            "epoch 29 batch 12500 [125000/160000] training loss: 553.2391731739044\n",
            "epoch 29 batch 12600 [126000/160000] training loss: 569.435173034668\n",
            "epoch 29 batch 12700 [127000/160000] training loss: 546.0810860395432\n",
            "epoch 29 batch 12800 [128000/160000] training loss: 583.7233946323395\n",
            "epoch 29 batch 12900 [129000/160000] training loss: 572.9257614612579\n",
            "epoch 29 batch 13000 [130000/160000] training loss: 584.6799399852753\n",
            "epoch 29 batch 13100 [131000/160000] training loss: 570.5079879760742\n",
            "epoch 29 batch 13200 [132000/160000] training loss: 590.1324963569641\n",
            "epoch 29 batch 13300 [133000/160000] training loss: 570.5131797790527\n",
            "epoch 29 batch 13400 [134000/160000] training loss: 545.52263712883\n",
            "epoch 29 batch 13500 [135000/160000] training loss: 562.3541553020477\n",
            "epoch 29 batch 13600 [136000/160000] training loss: 541.6558105945587\n",
            "epoch 29 batch 13700 [137000/160000] training loss: 600.0350338220596\n",
            "epoch 29 batch 13800 [138000/160000] training loss: 584.0653702020645\n",
            "epoch 29 batch 13900 [139000/160000] training loss: 598.4389779567719\n",
            "epoch 29 batch 14000 [140000/160000] training loss: 570.0169451236725\n",
            "epoch 29 batch 14100 [141000/160000] training loss: 538.0811667442322\n",
            "epoch 29 batch 14200 [142000/160000] training loss: 562.2557501792908\n",
            "epoch 29 batch 14300 [143000/160000] training loss: 560.286142706871\n",
            "epoch 29 batch 14400 [144000/160000] training loss: 580.5127733945847\n",
            "epoch 29 batch 14500 [145000/160000] training loss: 562.2604649066925\n",
            "epoch 29 batch 14600 [146000/160000] training loss: 558.0113544464111\n",
            "epoch 29 batch 14700 [147000/160000] training loss: 552.6088440418243\n",
            "epoch 29 batch 14800 [148000/160000] training loss: 563.5622578859329\n",
            "epoch 29 batch 14900 [149000/160000] training loss: 555.5342121124268\n",
            "epoch 29 batch 15000 [150000/160000] training loss: 593.6984614133835\n",
            "epoch 29 batch 15100 [151000/160000] training loss: 568.5614666938782\n",
            "epoch 29 batch 15200 [152000/160000] training loss: 596.1712713241577\n",
            "epoch 29 batch 15300 [153000/160000] training loss: 563.6108360290527\n",
            "epoch 29 batch 15400 [154000/160000] training loss: 583.4749788045883\n",
            "epoch 29 batch 15500 [155000/160000] training loss: 553.5591430664062\n",
            "epoch 29 batch 15600 [156000/160000] training loss: 539.0493543148041\n",
            "epoch 29 batch 15700 [157000/160000] training loss: 542.7456406354904\n",
            "epoch 29 batch 15800 [158000/160000] training loss: 559.1029576063156\n",
            "epoch 29 batch 15900 [159000/160000] training loss: 552.3775217533112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the evolution of the training loss throughout the epochs :"
      ],
      "metadata": {
        "id": "-NK5aXjIj6gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/final_training_epoch_losses_no_pretraining', 'rb') as f:\n",
        "  training_losses = pickle.load(f)\n",
        "\n",
        "plt.scatter(x = [k for k in range(len(training_losses))], y = training_losses)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross entropy loss for classif.')\n",
        "plt.title('Evolution of training loss through epochs ; no pretraining')"
      ],
      "metadata": {
        "id": "JQY9LHpGjqrT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "1803cce2-3e6f-449c-cad2-cf34173438c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Evolution of training loss through epochs ; no pretraining')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqNklEQVR4nO3de5wcVZn/8c+XEDAiGu7mBkESboIEGAFdWBFZEhAkZgVk2SUgCyuCwqoocUUQQW4KCiprEBZY5aaEi4CGiID+VAIJmSUBDEQIkuEmhnCNkITn98c5I5XJdE/NpbszPd/369Wv7j51quqp6svTder0KUUEZmZmtbZGowMwM7OBwQnHzMzqwgnHzMzqwgnHzMzqwgnHzMzqwgnHzMzqwgmnj0kKSWN6OO8ekub3dUwl1ruVpFZJL0v6XI3W8RVJP+rruj2Io8evTzOsvxDH6BzLmo2OpRJJCyXt3eg4+jNJh0m6va/r9jiegfo/HEkLgU2AFYXiyyPi+F4uN4CxEbGgL+vWkqRLgZci4j8rTL8L+HFE1CQJ1FM993ln+201es1HA48DgyNieSNjqSR/Rv89In7V6FgaQdLlwKKI+GqjY+krq+2vmzo5YKC+mTvYDLimpzNLWnN1/dLqz7xfm5ckkX7wv9mLZfS/90dEDMgbsBDYu5PytYElwHaFso2ApcDG+fnRwAJgMXAzMLxQN4Ax+fFdpF9o7dOOAP5ffvybXPdV4BXgEGBP0i+a9vrb5GUsAR4EPlaYdjnwfeBW4GVgJrBFle39WF7GkrzMbXL5r0lHeX/LcWzZYb4zO0z/XmE7jwMeBR7PZd8FngReAmYDexSWcxrp1z7A6Dz/ZODPwPPAf/Ww7hDgCuAF4GHgS8V92Ml+KL4+7wKuBP4CPAF8FVgjTxsD3A28mNd5bS4XcAHwXN7OuRTeKyX326fzfluSX8P2loYjgN/l5f8VOKOLGP++nzrsqzXz881J77OXgV/ldZXarxU+F9/KdZ8F/hsYkqftCSwCvpKXsxA4rDBvxW0ofJ4eznE+BOxU+Ix+EXggvw7XAm/L0zYEbsn7cDHw2+IyO8R+C3ByhWlV90Pe7u8AT+Xbd4C1Kyyr/fX7Xo73j8BHCtPvyu+L35G+T8YAWwMz8jbMBw7OdY8BlgFvkN4/Py/sky/nffI66aDhZOBPhf338c6+c0q+/8rWHQR8O++vx4HjKbz3Kr6PavFl3h9uVEg4edplwJmF58cBv8yP98o7eaf8ZrwI+E2HF6nLhNOxbvGDmx8PJiW1rwBr5fW+DGyVp19O+lLaJb/pfgJcU2F7tiQltn/Ky/1SXvZancXZyfyrTM+xzwDW560vnn8FNsjxfAF4hre+IE5j1S+7S0gJYwfSh2ebHtQ9m5QY1gNGkj6IZRPOlcBNwLp5PY8AR+VpVwP/RTrP+TZg91w+npRMh5KSzzbAsG7ut1vy/JuSvoQnFN4fy4HP5n04pIsY/76fOuyr9oTzB1KSWAvYnZQgS+3XTrblAtKPq/VzLD8Hziq8b5cD55M+Ex8ivd+2KrGfDwLagPfn/TkG2KzwGb0XGJ7X+zDw6TztLFLSG5xve5C/DLv5PVB1PwCnA/cAG5N+eP4e+EaFZbW/fv+ZYzqElHjWL7wf/gy8N7++7yL9QDsyP9+R9N2ybeEzfkYn31utwCje+twdlPfRGnmdr5Lfk3T+nVPt/Ve27qdJyW0k6bP3K5xwukw4r5Ayd/vt6Dxtb+BPhbq/Aw7Pjy8Fzi1Mewfpl8jowovUFwlnD9IXdvGX4NXAaYU3448K0/YD/lhhW08Bris8X4P0Id+zszg7mX+V6Tn2vbrYxy8AO+THp7Hql93IQt17gU/2oO5jwPjCtH+nRMIh/UJ7g/zhztP+A7grP74SmFpcby7fi/SFuRsVflGX2G+7F55fR/71nd8ffy5M6yrGv++nDvtqTdIXxHLg7YXpPy67XzvELNKX2BaFsg/w1pHtnnld63TYrlNKbMN04IQqn9F/LTw/F/jv/Ph0UhIb09m8ZW8l3l9/AvYrTBsPLKywrCNIR0HqsKx/K7wfTi9MOwT4bYdl/BA4Nd76jHeWcD7VxTa1AgcWYur4nVPt/Ve27q+B/yhM25sSCWeg91KbGBFDC7dLcvmdwNsl7ZpPro4DbsjThpOaBQCIiFdIRxoj+ji24cCTsXIb7xMd1vNM4fFrpORXaVnFmN8k/bLqbcxPFp9I+qKkhyW9KGkJ6RfchlXmLxt/tbrDO8SxUkxVbEj6FfpEoay4f79E+qK9V9KDkj4FEBG/JjWZfB94TtJUSe8suc521ba7GH9XMVYzHFgcEa9VWHaZWNptBLwdmC1pSX5tf5nL270QEa92iHN4iW0YRfpSr6RSfOeRjtJvl/SYpJOrLKOMau+vjrEPr7KctsjfwBXqF1+DzYBd2/dp3q+HAe/uItaOn7vDcy/T9mVsx2r6uRvoCadTEbGClM0PzbdbIuLlPPkp0hsFAEnrkJqR2jpZ1KukD2q7rt5IRU8BoyQVX6NNK6ynzLKKMYv0QS+7rOiqXNIepC/pg4H1ImIoqTlBPYi3O54mHda3G1VyvudJR6abFcr+vn8j4pmIODoihpN+kf+gvTtzRFwYETsD25KaK0+qsI5K+62a4jxVY6T6++tpYH1Jxell901Hz5POOby38OPsXRFR/KJaL38WinE+VWIbngS26G5AEfFyRHwhIt5DOj/5eUkf6e5ySljps8Nb21XJiPz5qlS/+Po+Cdzd4UfvOyLi2E7qFhU/d5uRmgOPBzbIn7t5rKafOyecyq4iHfIelh+3uxo4UtI4SWsD3wRmRsTCTpbRCkyS9Pb8ZXVUh+nPAu+psP6ZpF8UX5I0WNKewAH0rDfZdcBHJX1E0mDS+ZXXSe3RZVSLs926pGaVvwBrSvoa0N1f/j1xHTBF0nqSRpA+eF0q/Kg4U9K6+YP7eVKzE5IOktT+gXqB9CF/U9L785HvYNIX/t+ASj2Nyuy3HsdIen/9o6RNJb0LmFKY9wlgFnCapLUkfYD0/ulJHG+SvtQukLQxgKQRksZ3qPr1vK49gP2Bn5bYhh8BX5S0s5IxuU5VkvbPdUX6YbOCCq+DpLskndbtDU+uBr4qaSNJGwJfK8TemY2Bz+XP7EGkc3y3Vah7C7ClpH/L9Qfn99c2eXqZ9886pPfmXwAkHUk6wqm164AT8vtgKKkjQ5cGesL5uaRXCrf2ZjMiYibpC2U48ItC+a9IbdPXk7L8FsAnKyz/AlL79bOknlQ/6TD9NOCKfCh8cHFCRLxB+oLYl/Qr8Qek80h/7O5GRsR80gn9i/KyDiB1CX+j5CK+C3xC0guSLqxQZzqpmeURUjPC3yjfvNUbp5N6SD1OOnH5M1IyLeOzpNf4MeD/kX5YXJanvR+YKekV0snyEyLiMVISvYSUhJ4gNaeeV2H5ZfZbj2OMiBmknlsPkDoy3NJh3sNI51rae7xdS/l909GXSU1Y90h6ibSvtypMf4a0T54ivc8/XXivVtuGn5J6bl1F6hRzI6mDQFfG5hheIXWO+EFE3Fmh7ijSedieOIOUuB8g9Ui8P5dVMjPH9jxpuz4REX/trGJuNdmH9P3xFGkfnkPqeAHpfPG2+fvhxgrLeIjUW+wPpO+Z7en5tnbHJcDtpP0yh5RUl7Py/xpXMWD/+GnNSdKxpBO+H2p0LKsbSdeSOpac2sfL3ZPUGWFkF1XrLh+lXhcRH6zDuo4gdRLZvdbrWt1I2pfUoaPq0elAP8Kxfk7SMEn/IGkNSVuRmgtv6Gq+gSA3z2yR980E4EDSEcSAERGL6pFsBhpJQyTtJ2nN3JR9KiU+dwN9pAHr/9YidSXdnNS1/RpS86OlTgTTSJ1aFgHHRsScxoZkTULA10nNtEtJf0D/WpczuUnNzMzqwU1qZmZWF25SyzbccMMYPXp0o8MwM+tXZs+e/XxEbNR1TSecvxs9ejSzZs1qdBhmZv2KpCe6rpW4Sc3MzOrCCcfMzOrCCcfMzOrCCcfMzOrCCcfMzOrCvdR64cY5bZw3fT5PLVnK8KFDOGn8Vkzcsa8vi2Nm1hxqdoQj6TJJz0maVyhbX9IMSY/m+/VyuSRdKGmBpAck7VSYZ3Ku/6ikyYXynSXNzfNc2H4Nikrr6Gs3zmljyrS5tC1ZSgBtS5YyZdpcbpzTk8vVmJk1v1o2qV0OTOhQdjJwR0SMBe7IzyENwT82344BLoaUPEiDwu0K7AKcWkggFwNHF+ab0MU6+tR50+ezdNnKI3EvXbaC86bPr8XqzMz6vZolnIj4DbC4Q/GBpOvCkO8nFsqvjOQeYKikYaTrh8+IiMUR8QIwA5iQp70zIu7Jl3O9ssOyOltHn3pqydJulZuZDXT1PoezSUQ8nR8/A2ySH49g5Yt1Lcpl1coXdVJebR2rkHQM6YiKYcOG0draWnpDjthavPrG8lXK11lrzW4tx8xsoGhYp4GICEk1Haq6q3VExFRgKkBLS0uMGzeu9LIXxkZMmTZ3pWa1IYMHcdak7Rk3zh0HzMw6qne36Gdzcxj5/rlc3ka6DGy7kbmsWvnITsqrraNPTdxxBGdN2p4RQ4cgYMTQIZw1aXv3UjMzq6DeRzg3A5OBs/P9TYXy4yVdQ+og8GJEPC1pOvDNQkeBfYApEbFY0kuSdiNdQ/xw4KIu1tHnJu44wgnGzKykmiUcSVcDewIbSlpE6m12NnCdpKOAJ4CDc/XbgP2ABcBrwJEAObF8A7gv1zs9Ito7InyG1BNuCPCLfKPKOszMrIF8xc+spaUlfHkCM7PukTQ7IlrK1PXQNmZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhdOOGZmVhc9SjiS3t3XgZiZWXPr6RHOpX0ahZmZNb2KCUfSHfn+nI7TIuKjtQzKzMyaz5pVpg2T9EHgY5KuAVScGBH31zQyMzNrKtUSzteAU4CRwPkdpgWwV62CMjOz5lMx4UTEz4CfSTolIr5Rx5jMzKwJVUw4kraOiD8Ct0raqeN0N6mZmVl3VGtS+zxwDPDtTqa5Sc3MzLqlWpPaMfn+w/ULx8zMmlWX/8ORdJCkdfPjr0qaJmnH2odmZmbNpMwfP0+JiJcl7Q7sTfrT53/3ZqWSTpA0T9KDkk7MZeMk3SOpVdIsSbvkckm6UNICSQ8UzydJmizp0XybXCjfWdLcPM+FkrRKEGZmVldlEs6KfP9RYGpE3Aqs1dMVStoOOBrYBdgB2F/SGOBc4OsRMY7UJfvcPMu+wNh8Owa4OC9nfeBUYNe8rFMlrZfnuTivo32+CT2N18zM+kaZhNMm6YfAIcBtktYuOV8l2wAzI+K1iFgO3A1MInVEeGeu8y7gqfz4QODKSO4BhkoaBowHZkTE4oh4AZgBTMjT3hkR90REAFcCE3sRr5mZ9YFqvdTaHUw6QvhWRCzJX+gn9WKd84AzJW0ALAX2A2YBJwLTJX2LlNA+mOuPAJ4szL8ol1UrX9RJ+SokHUM6amLYsGG0trb2YrPMzKyaMglnGHBrRLwuaU/gfaSjhh6JiIfz+Gy3A68CraRmu2OB/4yI6yUdTDpXtHdP11MylqnAVICWlpYYN25cLVdnZjaglWkaux5Ykc+zTAVGAVf1ZqURcWlE7BwR/wi8ADwCTAam5So/JZ2XAWjL62w3MpdVKx/ZSbmZmTVQmYTzZj7XMgm4KCJOIh319JikjfP9pnm5V5HO2XwoV9kLeDQ/vhk4PPdW2w14MSKeBqYD+0haL3cW2AeYnqe9JGm33DvtcOCm3sRrZma9V6ZJbZmkQ0lf3AfkssG9XO/1+RzOMuC4fG7oaOC7ktYE/kY+twLcRjrPswB4DTgSICIWS/oGcF+ud3pELM6PPwNcDgwBfpFvZmbWQEoduapUkLYFPg38ISKulrQ5cHBErHKdnP6spaUlZs2a1egwzMz6FUmzI6KlTN0uj3Ai4iHgc4XnjwNNlWzMzKz2ukw4ksYCZwHbAm9rL4+I99QwLjMzazJlOg38D+mf+8uBD5O6RP+4lkGZmVnzKZNwhkTEHaTzPU9ExGmkYW7MzMxKK9NL7XVJawCPSjqe9J+Wd9Q2LDMzazZljnBOAN5O6jiwM/BvpD9pmpmZlVaml1r7/1xeIf8HxszMrLsqJhxJPyeN4NypiPhYTSIyM7OmVO0I51t1i8LMzJpexYQTEXcDSFoHWBoRb+bng4C16xOemZk1izKdBu4gdRpoNwT4VW3CMTOzZlUm4bwtIl5pf5Ifv71KfTMzs1WUSTivStqp/YmknUlX6jQzMyutzB8/TwR+KukpQMC7gUNqGZSZmTWfUv/DkbQ1sFUumh8Ry2oblpmZNZsyRzjkBDOvxrGYmVkTK3MOx8zMrNeqJhwlo+oVjJmZNa+qCSfS9advq1MsZmbWxMo0qd0v6f01j8TMzJpamU4DuwKHSXoCeJXUNToi4n01jczMzJpKmYQzvuZRmJlZ0+uySS0ingCGAgfk29BcZmZmVlqXCUfSCcBPgI3z7ceSPlvrwMzMrLmUaVI7Ctg1Il4FkHQO8AfgoloGZmZmzaVMLzUBKwrPV+QyMzOz0soc4fwPMFPSDfn5RODSmkVkZmZNqWLCkbR5RDweEedLugvYPU86MiLm1CU6MzNrGtWOcH4G7Czpjoj4CHB/nWIyM7MmVC3hrCHpK8CWkj7fcWJEnF+7sMzMrNlU6zTwSVIHgTWBdTu5mZmZlVbxCCci5gPnSHogIn5Rx5jMzKwJlRlpoM+TjaQTJM2T9KCkEwvln5X0x1x+bqF8iqQFkuZLGl8on5DLFkg6uVC+uaSZufxaSWv19TaYmVn31P0CbJK2A44GdgF2APaXNEbSh4EDgR0i4r3At3L9bUnNe+8FJgA/kDRI0iDg+8C+wLbAobkuwDnABRExBniB9OdVMzNroEZc8XMbYGZEvBYRy4G7gUnAscDZEfE6QEQ8l+sfCFwTEa9HxOPAAlKy2gVYEBGPRcQbwDXAgZIE7EXqZQdwBem/Q2Zm1kBd/vFT0kHALyPiZUlfBXYCzoiInnaTngecKWkDYCmwHzAL2BLYQ9KZwN+AL0bEfcAI4J7C/ItyGcCTHcp3BTYAluRk1rF+x207BjgGYNiwYbS2tvZwk8zMrCtlRho4JSJ+Kml3YG/gPOBi0pd7t0XEw3k8tttJ19dp5a3ecOsDuwHvB66T9J6erKMbsUwFpgK0tLTEuHHjark6M7MBrUyTWvs4ah8FpkbErUCvTsJHxKURsXNE/CPpHMsjpCORaZHcC7wJbAi0AaMKs4/MZZXK/woMlbRmh3IzM2ugMgmnTdIPgUOA2yStXXK+iiRtnO83JZ2/uQq4EfhwLt+SlNSeB24GPilpbUmbA2OBe4H7gLG5R9papI4FN0dEAHcCn8irmwzc1Jt4zcys98o0qR1M6h32rYhYImkYcFIv13t9PoezDDguL/cy4DJJ84A3gMk5eTwo6TrgIWB5rr8CQNLxwHRgEHBZRDyYl/9l4BpJZwBz8GCjZmYNp/SdXqWCtAWwKCJel7Qn8D7gyohYUvPo6qilpSVmzZrV6DDMzPoVSbMjoqVM3TJNY9cDKySNIZ1gH0VqAjMzMyutTMJ5M3cxngRcFBEnAcNqG5aZmTWbMglnmaRDgcOBW3LZ4NqFZGZmzahMwjkS+ABwZkQ8nnuK/W9twzIzs2ZTZvDOh4AvAnPzOGiLIuKcmkdmZmZNpczQNnuSxiNbCAgYJWlyRPymppGZmVlTKfM/nG8D++Tr47T/KfNqYOdaBmZmZs2lzDmcwe3JBiAiHsGdBszMrJvKHOHMkvQj4Mf5+WGk0Z3NzMxKK5NwjgWOAz6Xn/8W+EHNIjIzs6bUZcLJF0Q7P9/MzMx6pGLCkTQXqDjQWkS8ryYRmZlZU6p2hLN/3aIwM7OmVzHhRMQT9QzEzMyaW68upGZmZlaWE46ZmdVFlwlH0gGSnJjMzKxXyiSSQ4BHJZ0raetaB2RmZs2pzGjR/wrsCPwJuFzSHyQdI2ndmkdnZmZNo1RTWUS8BPwMuIZ0tc+PA/dL+mwNYzMzsyZS5hzOxyTdANxFGrRzl4jYF9gB+EJtwzMzs2ZRZiy1fwYu6Hj9m4h4TdJRtQnLzMyaTZmx1CZLerekj5GGurkvIp7J0+6odYBmZtYcyjSpHQXcC0wCPgHcI+lTtQ7MzMyaS5kmtS8BO0bEXwEkbQD8HrisloGZmVlzKdNL7a/Ay4XnL+cyMzOz0soc4SwAZkq6iXQO50DgAUmfB4gIXyfHzMy6VCbh/Cnf2t2U7/3HTzMzK61ML7WvA0h6R37+Sq2DMjOz5lOml9p2kuYADwIPSpot6b21D83MzJpJmU4DU4HPR8RmEbEZaXSBS2oblpmZNZsy53DWiYg7259ExF2S1qlhTE3pxjltnDd9Pk8tWcrwoUM4afxWTNxxRKPDMjOrmzJHOI9JOkXS6Hz7KvBYb1Yq6QRJ8yQ9KOnEDtO+ICkkbZifS9KFkhZIekDSToW6kyU9mm+TC+U7S5qb57lQknoTb2/dOKeNKdPm0rZkKQG0LVnKlGlzuXFOWyPDMjOrqzIJ51PARsA04Hpgw1zWI5K2A44GdiENALq/pDF52ihgH+DPhVn2Bcbm2zHAxbnu+sCpwK55WadKWi/Pc3FeR/t8E3oab184b/p8li5bsVLZ0mUrOG/6/AZFZGZWf1Wb1CQNAqZFxIf7cJ3bADMj4rW8jrtJw+acC1xAGtngpkL9A4ErIyJIw+oMlTQM2BOYERGL83JmABMk3QW8MyLuyeVXAhOBX/ThNnTLU0uWdqvczKwZVU04EbFC0puS3hURL/bROucBZ+YhcpYC+wGzJB0ItEXE/3VoARsBPFl4viiXVStf1En5KiQdQzpqYtiwYbS2tvZ8q6o4Ymvx6hvLVylfZ601a7ZOM7PVTZlOA68Ac/MRxKvthRHxuZ6sMCIelnQOcHteXiuwNvAVUnNa3UTEVFIvPFpaWmLcuHE1Wc/C2Igp0+au1Kw2ZPAgzpq0PePGueOAmQ0MZRLOtHwrit6sNCIuBS4FkPRN4FlSs1f70c1I0hVFdwHagFGF2UfmsjZSs1qx/K5cPrKT+g3T3hvNvdTMbCArk3CGRsR3iwWSTujNSiVtHBHPSdqUdP5mt+I6JC0EWiLieUk3A8dLuobUQeDFiHha0nTgm4WOAvsAUyJisaSXJO0GzAQOBy7qTbx9YeKOI5xgzGxAK9NLbXInZUf0cr3XS3oI+DlwXEQsqVL3NlI37AWkP5x+BiB3FvgGcF++nd7egSDX+VGe5080sMOAmZklSp2/OpkgHQr8C7A78NvCpHWBNyPiI7UPr35aWlpi1qxZjQ7DzKxfkTQ7IlrK1K3WpPZ74GnS/26+XSh/GXig5+GZmdlAVDHhRMQTwBPAB+oXjpmZNasyo0VPykPHvJhPxr8s6aV6BGdmZs2jTC+1c4EDIuLhWgdjZmbNq0wvtWedbMzMrLfKHOHMknQtcCPwenthRHT8M6iZmVlFZRLOO4HXWHnYmWDV0QfMzMwq6jLhRMSR9QjEzMyaW5lealtKukPSvPz8ffkibGZmZqWV6TRwCTAFWAYQEQ8An6xlUGZm1nzKJJy3R8S9HcpWvbiLmZlZFWUSzvOStiBfkkDSJ0hD3piZmZVWppfacaSLlG0tqQ14HDisplGZmVnTKdNL7TFgb0nrAGtExMu1D8vMzJpNmSMcACLi1a5rmZmZda7MORwzM7Nec8IxM7O66LJJTdJBwC8j4uX8h8+dgDMi4v6aRzdA3TinjfOmz+epJUsZPnQIJ43fiok7jmh0WGZmvVLmCOeUnGx2B/YGLgUurm1YA9eNc9qYMm0ubUuWEkDbkqVMmTaXG+e0NTo0M7NeKZNwVuT7jwJTI+JWYK3ahTSwnTd9PkuXrVipbOmyFZw3fX6DIjIz6xtlEk6bpB8ChwC3SVq75HzWA08tWdqtcjOz/qJM4jgYmA6Mj4glwPrASbUMaiAbPnRIt8rNzPqLMglnGHBrRDwqaU/gIKDj2GrWR04avxVDBg9aqWzI4EGcNH6rBkVkZtY3yiSc64EVksaQhrgZBVxV06gGsIk7juCsSdszYugQBIwYOoSzJm3vXmpm1u+VGWngzYhYLmkScFFEXCRpTq0DG8gm7jjCCcbMmk6ZI5xlkg4FDgduyWWDaxeSmZk1ozIJ50jgA8CZEfG4pM2B/61tWGZm1my6TDgR8RDwRWCupO2ARRFxTs0jMzOzplJmaJs9gSuAhYCAUZImR8RvahqZmZk1lTKdBr4N7BMR8wEkbQlcDexcy8DMzKy5lDmHM7g92QBExCO404CZmXVTmSOc2ZJ+BPw4Pz8MmFW7kKw7PLK0mfUXZY5wPg08BHwu3x4Cju3NSiWdIGmepAclnZjLzpP0R0kPSLpB0tBC/SmSFkiaL2l8oXxCLlsg6eRC+eaSZubyayU15WCjHlnazPqTqglH0iDg/yLi/IiYlG8XRMTrPV1h7ul2NLALsAOwfx7FYAawXUS8D3gEmJLrbwt8EngvMAH4gaRBObbvA/sC2wKH5roA5wAXRMQY4AXgqJ7GuzrzyNJm1p9UTTgRsQKYL2nTPlznNsDMiHgtIpYDdwOTIuL2/BzgHmBkfnwgcE1EvB4RjwMLSMlqF2BBRDwWEW8A1wAHShKwF/CzPP8VwMQ+jH+14ZGlzaw/KXMOZz3gQUn3Aq+2F0bEx3q4znnAmZI2AJYC+7HqOaFPAdfmxyNICajdolwG8GSH8l2BDYAlheRVrL8SSccAxwAMGzaM1tbWHmxO4xyxtXj1jeWrlK+z1pr9blvMrPmVSTin9OUKI+JhSecAt5MSWCtvXeQNSf8FLAd+0pfrrRDLVNKApLS0tMS4ceNqvco+tTA2Ysq0uSs1qw0ZPIizJm3PuHHuOGBmq5eKCSefV9kkIu7uUL478HRvVhoRl5IuVY2kb5KOQpB0BLA/8JGIiFy9jTRCdbuRuYwK5X8FhkpaMx/lFOs3lfbeaO6lZmb9QbUjnO+QT9x38GKedkBPVypp44h4Lp8bmgTsJmkC8CXgQxHxWqH6zcBVks4HhgNjSdfjETA2j+3WRupY8C8REZLuBD5BOq8zGbipp7Gu7jyytJn1F9USziYRMbdjYUTMlTS6l+u9Pp/DWQYcFxFLJH0PWBuYkc77c09EfDoiHpR0Hak79vJcfwWApONJVyMdBFwWEQ/m5X8ZuEbSGcAc8tGUmZk1jt5queowQXo0IsZWmLYgdzluGi0tLTFrVnP/n9V/EjWzviZpdkS0lKlbrVv0LElHd7Lwfwdm9zQ4awz/SdTMGq1ak9qJwA2SDuOtBNMCrAV8vMZxWR+r9idRH+WYWT1UTDgR8SzwQUkfBrbLxbdGxK/rEpn1Kf9J1Mwarcv/4UTEncCddYjFamj40CG0dZJchg8d0oBozGwgKjN4pzWBk8ZvxZDBg1YqGzJ4ECeN36pBEZnZQFNmpAFrAt35k6h7s5lZLTjhDCBl/iTa3putvYNBe2+29vnNzHrKTWq2El/ywMxqxQnHVuLebGZWK25Ss5V0tzebz/eYWVk+wrGVdKc3m0cvMLPucMKxlUzccQRnTdqeEUOHIGDE0CGcNWn7To9afL7HzLrDTWq2irKXPOju+R43v5kNbD7CsR6rdF6ns3I3v5mZE471WHfO97j5zczcpGY91p3RC9z8ZmZOONYrZc/3dKe7tUc7MGtOblKzuqhl89uNc9r4h7N/zeYn38o/nP1rnxcyW035CMfqolbNb909GnJTnVnjOOFY3dSi+a07VzJ1cjJrLDep2WqnO81v3Tka6k5Tnbtxm/U9H+HYaqc7zW/dORrqq+TU2ZFT2SMhHzXZQOaEY6ulss1vJ43faqVmMqh8NFSL5NSdZrrVpUnPSc8axQnH+rXuHA3VIjl150hodTjfVMukV4u6jV5/LesORE441u+VPRqqRXLqTjNdLZv0yiaRWi23FnUbvf5a1m2v31+SeV9xpwEbUCbuOILfnbwXj5/9UX538l4VP2BlR83uznhy3albq84QtVpuLeo2ev21rNudTim1qNuoTjFOOGYVlElO3elR1526tUpOtVpuLeo2ev21rNvopNeosQ2dcMx6oTvXD+pO3Volp1ottxZ1G73+WtZtdNJr1KXknXDMeqlsM1136tYqOdVqubWo2+j117Juo5Ned5bZl9xpwGw1VYvOELVabi3qNnr9tazbnR6TtajbnWX2JUVETVfQX7S0tMSsWbMaHYaZDRDN0ktN0uyIaClV1wknccIxM+u+7iSchpzDkXSCpHmSHpR0Yi5bX9IMSY/m+/VyuSRdKGmBpAck7VRYzuRc/1FJkwvlO0uam+e5UJLqvpFmZraSuiccSdsBRwO7ADsA+0saA5wM3BERY4E78nOAfYGx+XYMcHFezvrAqcCueVmntiepXOfownwTar9lZmZWTSOOcLYBZkbEaxGxHLgbmAQcCFyR61wBTMyPDwSujOQeYKikYcB4YEZELI6IF4AZwIQ87Z0RcU+k9sIrC8syM7MGaUQvtXnAmZI2AJYC+wGzgE0i4ulc5xlgk/x4BPBkYf5Fuaxa+aJOylch6RjSURPDhg2jtbW1xxtlZmbV1T3hRMTDks4BbgdeBVqBFR3qhKSa92aIiKnAVEidBsaNG1frVZqZDVgN+R9ORFwKXAog6Zuko5BnJQ2LiKdzs9hzuXobMKow+8hc1gbs2aH8rlw+spP6Vc2ePft5SU/0ZHuADYHnezjv6qxZtwuad9u8Xf1Pf9+2zcpWbEjCkbRxRDwnaVPS+ZvdgM2BycDZ+f6mXP1m4HhJ15A6CLyYk9J04JuFjgL7AFMiYrGklyTtBswEDgcu6iqmiNioF9szq2y3wP6kWbcLmnfbvF39TzNvW0eNGmng+nwOZxlwXEQskXQ2cJ2ko4AngINz3dtI53kWAK8BRwLkxPIN4L5c7/SIWJwffwa4HBgC/CLfzMysgRrVpLZHJ2V/BT7SSXkAx1VYzmXAZZ2UzwK2632kZmbWVzx4Z9+Y2ugAaqRZtwuad9u8Xf1PM2/bSjy0jZmZ1YWPcMzMrC6ccMzMrC6ccHpJ0gRJ8/NAoSd3PUf/IGlhHgC1VVK/HkZb0mWSnpM0r1DW6WCx/UmF7TpNUlt+3Vol7dfIGHtC0ihJd0p6KA/we0Iu79evWZXt6vevWVk+h9MLkgYBjwD/RPrz6n3AoRHxUEMD6wOSFgItEdGf/5AGgKR/BF4hjcm3XS47F1gcEWfnHwrrRcSXGxlnd1XYrtOAVyLiW42MrTfyH7+HRcT9ktYFZpPGQzyCfvyaVdmug+nnr1lZPsLpnV2ABRHxWES8AVxDGmzUViMR8RtgcYfiSoPF9hsVtqvfi4inI+L+/Phl4GHSeIj9+jWrsl0DhhNO71QaQLQZBHC7pNl5kNNmU2mw2GZwfL521GX9rdmpI0mjgR1Jo4Y0zWvWYbugiV6zapxwrJLdI2In0vWIjsvNN00p/7m4WdqWLwa2AMYBTwPfbmg0vSDpHcD1wIkR8VJxWn9+zTrZrqZ5zbrihNM7lQYW7fcioi3fPwfcQGo+bCbP5jb19rb157qo3y9ExLMRsSIi3gQuoZ++bpIGk76UfxIR03Jxv3/NOtuuZnnNynDC6Z37gLGSNpe0FvBJ0mCj/ZqkdfJJTSStQxoYdV71ufqdm0mDxMLKg8X2a+1fyNnH6YevW74k/KXAwxFxfmFSv37NKm1XM7xmZbmXWi/lLozfAQYBl0XEmY2NqPckvYd0VANpvL2r+vN2SbqadCmLDYFnSZcmvxG4DtiUPFhsYfDXfqHCdu1JapoJYCHwH4XzHv2CpN2B3wJzgTdz8VdI5zv67WtWZbsOpZ+/ZmU54ZiZWV24Sc3MzOrCCcfMzOrCCcfMzOrCCcfMzOrCCcfMzOrCCcesTiStKIwI3NqXo4tLGl0cNdpsdbRmowMwG0CWRsS4Rgdh1ig+wjFrsHztoXPz9YfulTQml4+W9Os8qOMdkjbN5ZtIukHS/+XbB/OiBkm6JF9r5XZJQxq2UWadcMIxq58hHZrUDilMezEitge+Rxq5AuAi4IqIeB/wE+DCXH4hcHdE7ADsBDyYy8cC34+I9wJLgH+u6daYdZNHGjCrE0mvRMQ7OilfCOwVEY/lwR2fiYgNJD1PumDXslz+dERsKOkvwMiIeL2wjNHAjIgYm59/GRgcEWfUYdPMSvERjtnqISo87o7XC49X4HO0tppxwjFbPRxSuP9Dfvx70gjkAIeRBn4EuAM4FtJlziW9q15BmvWGfwGZ1c8QSa2F57+MiPau0etJeoB0lHJoLvss8D+STgL+AhyZy08Apko6inQkcyzpwl1mqzWfwzFrsHwOpyUinm90LGa15CY1MzOrCx/hmJlZXfgIx8zM6sIJx8zM6sIJx8zM6sIJx8zM6sIJx8zM6uL/A4XiT+04mni0AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the accuracy of the classification model after 30 training epochs but WITHOUT PRETRAINING :"
      ],
      "metadata": {
        "id": "xo0q6KASkKvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing by bootstrapping 1000 batches in test dataset (= 10 000 reviews in total) :\n",
        "final_model = torch.load('./drive/MyDrive/final_model_no_pretraining')\n",
        "final_model.eval()\n",
        "\n",
        "bootstrap_number = 50\n",
        "bootstrap_size = 20\n",
        "bootstrapped_accuracies = []\n",
        "\n",
        "for boot in range(bootstrap_number):\n",
        "\n",
        "  test_loader = DataLoader(dataset = cinema_reviews['test'], batch_size = 10,\n",
        "                            num_workers=2, shuffle=True, drop_last=True)\n",
        "  right_answers = 0\n",
        "  true_test_size = 0 # few KeyError so we discount them\n",
        "\n",
        "  for i in range(bootstrap_size):\n",
        "    try:\n",
        "      batch = next(iter(test_loader))\n",
        "      true_label = batch['label'][0]\n",
        "      probas = F.softmax(final_model(preprocessing(batch, max_dialog_size, max_size_of_utterance)[0]), dim = 1)\n",
        "      votes = torch.sum(probas, axis=0)\n",
        "\n",
        "      if votes[0] < votes[1]:\n",
        "        pred = 1 \n",
        "      else:\n",
        "        pred = 0\n",
        "\n",
        "      if pred == true_label:\n",
        "        right_answers += 1\n",
        "      \n",
        "      true_test_size += 1\n",
        "    \n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "  bootstrapped_accuracies.append(right_answers/true_test_size)\n",
        "\n",
        "\n",
        "# average accuracy over bootstrapped sets of reviews :\n",
        "print('Average accuracy over bootstrapped sets of samples WITHOUT PRETRAINING = ', int(np.mean(bootstrapped_accuracies)*100), '%')"
      ],
      "metadata": {
        "id": "iNBI8lwbTft2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09554b77-fc8e-4db6-94d3-6dc60cd8c8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy over bootstrapped sets of samples WITHOUT PRETRAINING =  68 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "tUStaRWZ7xh4",
        "BjAP-ugmMUkB"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3de82a3ca99e44728071fadb22f8951c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfb2bed0049d474bbf4d67d256dda461",
              "IPY_MODEL_a19b695adce140dd9feec386b04117fa",
              "IPY_MODEL_15b18c9b05df4814ac4066ae4057e21b"
            ],
            "layout": "IPY_MODEL_f0db876de8664c85a7f551f5fe9be03c"
          }
        },
        "cfb2bed0049d474bbf4d67d256dda461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e852bc1bafd246debf415d3e05364ba8",
            "placeholder": "​",
            "style": "IPY_MODEL_d1c4540c55af40799b77b634e4cb8789",
            "value": "100%"
          }
        },
        "a19b695adce140dd9feec386b04117fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7e5fb29c6eb43f28c1a9bc51eaea4f0",
            "max": 10576129,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c2077a27a1c4ddba981cb6c8e7e0e5c",
            "value": 10576129
          }
        },
        "15b18c9b05df4814ac4066ae4057e21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96d4b9dd59fd4efaa117fb3afb489eb1",
            "placeholder": "​",
            "style": "IPY_MODEL_5e3368b5b602408ebb2515526483920c",
            "value": " 10576129/10576129 [00:09&lt;00:00, 1154668.25it/s]"
          }
        },
        "f0db876de8664c85a7f551f5fe9be03c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e852bc1bafd246debf415d3e05364ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1c4540c55af40799b77b634e4cb8789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7e5fb29c6eb43f28c1a9bc51eaea4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c2077a27a1c4ddba981cb6c8e7e0e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96d4b9dd59fd4efaa117fb3afb489eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3368b5b602408ebb2515526483920c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc9f2fb4f098428db16136f2937a6015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43d86c5d283f4c7b9d9e1bd6cda18d40",
              "IPY_MODEL_2ce2cfb9a9ad42b58192f3bc596cbfd0",
              "IPY_MODEL_d75e3266bd09473c95183b11bf78c98b"
            ],
            "layout": "IPY_MODEL_bce500f1fd2047c098385b163334805d"
          }
        },
        "43d86c5d283f4c7b9d9e1bd6cda18d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cec64f065a540a69cab0d097d2276a5",
            "placeholder": "​",
            "style": "IPY_MODEL_7a5a3f2e67464e939e19006fbf86d8b8",
            "value": "100%"
          }
        },
        "2ce2cfb9a9ad42b58192f3bc596cbfd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a0e0480aa5c46b2968bf0cac17a4110",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_015e9ff1cc24471f861985627618bb68",
            "value": 7
          }
        },
        "d75e3266bd09473c95183b11bf78c98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91bf7c332ece4936bf205e56eeed1b82",
            "placeholder": "​",
            "style": "IPY_MODEL_63ce71453d1d4054999c2e00a0aaaf96",
            "value": " 7/7 [3:21:41&lt;00:00, 1733.90s/it]"
          }
        },
        "bce500f1fd2047c098385b163334805d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cec64f065a540a69cab0d097d2276a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a5a3f2e67464e939e19006fbf86d8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a0e0480aa5c46b2968bf0cac17a4110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "015e9ff1cc24471f861985627618bb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91bf7c332ece4936bf205e56eeed1b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ce71453d1d4054999c2e00a0aaaf96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "247c629cd0d645bfa0c617fa40b690b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_747f06ec34944bddbdab51974193c36e",
              "IPY_MODEL_346b6bf1dbbf4ff78515416cb794e783",
              "IPY_MODEL_0b7ae10eceac4769b28be91d4293af61"
            ],
            "layout": "IPY_MODEL_7f409689d6cd42d482cd5b321dc28df5"
          }
        },
        "747f06ec34944bddbdab51974193c36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66adb3120d4a455d86b62318bc3e9843",
            "placeholder": "​",
            "style": "IPY_MODEL_bc48cd5f291a4720acceea7eae28f9b4",
            "value": " 53%"
          }
        },
        "346b6bf1dbbf4ff78515416cb794e783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19c1d7dccd174698af59751954df1354",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_895e418a88da4f8d91d9cc774119e9c5",
            "value": 16
          }
        },
        "0b7ae10eceac4769b28be91d4293af61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4ff8af86ad4ea190d8e28b09921ce8",
            "placeholder": "​",
            "style": "IPY_MODEL_cc50bff4f6d3438a8ade9daf0598725a",
            "value": " 16/30 [8:00:17&lt;6:54:45, 1777.52s/it]"
          }
        },
        "7f409689d6cd42d482cd5b321dc28df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66adb3120d4a455d86b62318bc3e9843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc48cd5f291a4720acceea7eae28f9b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19c1d7dccd174698af59751954df1354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "895e418a88da4f8d91d9cc774119e9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f4ff8af86ad4ea190d8e28b09921ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc50bff4f6d3438a8ade9daf0598725a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c53603b1a09459dbbdd21a089c81567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b06cf1b9bc9b4563bf19895cf284d115",
              "IPY_MODEL_15f5074dc19a478f808cf43118b322c9",
              "IPY_MODEL_85ff2ab9e8b842d49203e59a4f5aa119"
            ],
            "layout": "IPY_MODEL_d45a7078abb740ffa6579007a8e15ac1"
          }
        },
        "b06cf1b9bc9b4563bf19895cf284d115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17364cbfc6a40ad88574efb3cb7f1fa",
            "placeholder": "​",
            "style": "IPY_MODEL_deb6c143365d42a19d82de7320142e56",
            "value": "100%"
          }
        },
        "15f5074dc19a478f808cf43118b322c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3512e41fd7c345789574d8f7fbed7200",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aff798a186694d679c9f4f069a798f69",
            "value": 14
          }
        },
        "85ff2ab9e8b842d49203e59a4f5aa119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_831d920986d54f8788f5feb323907cb0",
            "placeholder": "​",
            "style": "IPY_MODEL_02b7d169840d4ecbbbc5779dc2a0be3f",
            "value": " 14/14 [6:57:08&lt;00:00, 1824.44s/it]"
          }
        },
        "d45a7078abb740ffa6579007a8e15ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d17364cbfc6a40ad88574efb3cb7f1fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb6c143365d42a19d82de7320142e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3512e41fd7c345789574d8f7fbed7200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aff798a186694d679c9f4f069a798f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "831d920986d54f8788f5feb323907cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02b7d169840d4ecbbbc5779dc2a0be3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63f96d2001154a5c8cdc1d120e08232c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12c48669f71f421990374157ea0ade2e",
              "IPY_MODEL_cef68fc041af4902bf115a538bae6a1c",
              "IPY_MODEL_7b92c3608ea046c9a21a12039902e4ea"
            ],
            "layout": "IPY_MODEL_c5879c3753a644ec964dfb8e35f7c796"
          }
        },
        "12c48669f71f421990374157ea0ade2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e63bbb439e14dffbf4e6fa987534dbe",
            "placeholder": "​",
            "style": "IPY_MODEL_dff231d5300e45bb8c44201cc7b2cb64",
            "value": "100%"
          }
        },
        "cef68fc041af4902bf115a538bae6a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a95572802c6343369a8b7a12e618bc74",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9859b9b925e43ca82abe0372e85432f",
            "value": 3
          }
        },
        "7b92c3608ea046c9a21a12039902e4ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e35b4a667aa4a9ab3d214228157fb53",
            "placeholder": "​",
            "style": "IPY_MODEL_93c92597b7a84aedaee49549a617a0f3",
            "value": " 3/3 [00:00&lt;00:00, 93.70it/s]"
          }
        },
        "c5879c3753a644ec964dfb8e35f7c796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e63bbb439e14dffbf4e6fa987534dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff231d5300e45bb8c44201cc7b2cb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a95572802c6343369a8b7a12e618bc74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9859b9b925e43ca82abe0372e85432f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e35b4a667aa4a9ab3d214228157fb53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c92597b7a84aedaee49549a617a0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}