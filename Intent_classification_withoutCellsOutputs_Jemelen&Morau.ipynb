{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilienJemelen/NLP_project2023_MORAU-JEMELEN/blob/main/Intent_classification_withoutCellsOutputs_Jemelen%26Morau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING PACKAGES"
      ],
      "metadata": {
        "id": "tUStaRWZ7xh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install xmltodict\n",
        "!pip install tokenizers\n",
        "!pip install torchinfo\n",
        "!pip install datasets\n",
        "!pip install datasets\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "from torchtext.data import get_tokenizer\n",
        "import transformers\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import pickle\n",
        "import xmltodict\n",
        "import xml.etree.ElementTree as ET\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torchtext\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from itertools import islice\n",
        "import torch.optim as opt\n",
        "from IPython.display import clear_output \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from gensim.models import FastText\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils as utils\n",
        "import re\n",
        "from nltk import tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "id": "ytD6OWiP71Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjAP-ugmMUkB"
      },
      "source": [
        "## SECTION 1 / LOADING OPENSUBTITLES DATA AND TRAINING OUR CUSTOM TOKENIZER"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data we have are .zip formatted (from https://opus.nlpl.eu/OpenSubtitles-alt-v2018.php), and .xml at the lower level, so some formatting is required (cells below)"
      ],
      "metadata": {
        "id": "RWsvEuxA8ixK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eGbnbMprAcH"
      },
      "outputs": [],
      "source": [
        "# creating folder for data storage\n",
        "try:\n",
        "  os.mkdir('./opensbtitles_files')\n",
        "except FileExistsError:\n",
        "  print('folder exists already')\n",
        "\n",
        "\n",
        "# loading the .zip and creating a zip object\n",
        "with ZipFile('./drive/MyDrive/fr.zip', 'r') as zObject:\n",
        "    # Extracting all the .xml of the zip into a specific location.\n",
        "    zObject.extractall(\n",
        "        path=\"./opensbtitles_files\")\n",
        "    \n",
        "# making a list of all the .xml files paths\n",
        "general_folder_path = './opensbtitles_files/OpenSubtitles/raw/fr/'\n",
        "xml_files_paths = []\n",
        "for subfolder_path in os.listdir(general_folder_path):\n",
        "  for subsubfolder_path in os.listdir(general_folder_path + subfolder_path):\n",
        "    for file_name in os.listdir(general_folder_path + subfolder_path + '/' + subsubfolder_path):\n",
        "      xml_files_paths.append(general_folder_path + subfolder_path + '/' + subsubfolder_path + '/' + file_name)\n",
        "\n",
        "\n",
        "# defining function to save as pickle at path\n",
        "def save_data(data, path):\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "\n",
        "# initializing dictionaries to store the data:\n",
        "try:\n",
        "  xmltree = ET.parse(xml_files_paths[0])\n",
        "  xmlroot = xmltree.getroot()\n",
        "  #defining an xml string\n",
        "  xml_data = ET.tostring(xmlroot)\n",
        "  #converting xml to dictionary\n",
        "  doc_dict = xmltodict.parse(xml_data)\n",
        "  dictionaries = doc_dict['document']['s']\n",
        "except ET.ParseError:\n",
        "  print('corrupt file, skipping it')\n",
        "\n",
        "# Due to the size of the data and our limited storage + computation capacities,\n",
        "# We will only training tokenizer on a certain proportion of the whole data\n",
        "propor = 1/2\n",
        "for k in range(1, int(len(xml_files_paths)*propor)):\n",
        "  \n",
        "  try:\n",
        "    xmltree = ET.parse(xml_files_paths[k])\n",
        "    xmlroot = xmltree.getroot()\n",
        "    # defining an xml string\n",
        "    xml_data = ET.tostring(xmlroot)\n",
        "    # converting xml to dictionary\n",
        "    doc_dict = xmltodict.parse(xml_data)\n",
        "    dictionaries = dictionaries + doc_dict['document']['s']\n",
        "  except (ET.ParseError, TypeError) as error:\n",
        "    print('corrupt file, skipping it')\n",
        "\n",
        "  if int((k/int(len(xml_files_paths)*propor))*100) != int(((k-1)/int(len(xml_files_paths)*propor))*100):\n",
        "    print(int((k/int(len(xml_files_paths)*propor))*100), ' %')\n",
        "\n",
        "for i in range(len(dictionaries)):\n",
        "  dictionaries[i]['@id'] = i\n",
        "\n",
        "save_data(dictionaries, \"./drive/MyDrive/opensbtitles_compiled_XXL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39wQ8XQccEBB"
      },
      "outputs": [],
      "source": [
        "with open('./drive/MyDrive/opensbtitles_compiled_XXL', 'rb') as f:\n",
        "  dictionaries = pickle.load(f)\n",
        "\n",
        "dictionaries[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mduO16qkcNj7"
      },
      "source": [
        "PREPARING A LIST OF DICTIONARIES FOR MODEL PRETRAINING /\n",
        "\n",
        "> List of lists (each sublist represents a dialog) of dictionaries (one dictionary = one utterance)\n",
        "\n",
        "> Dialogs are defined as lists of utterances with less than 6 seconds between the utterances.\n",
        "\n",
        "> Concretely, 3 types of 'time' values in each utterance :\n",
        "\n",
        "- First type / 'time': [{'@id': 'T60S', '@value': '00:05:55,780'},\n",
        "   {'@id': 'T60E', '@value': '00:05:58,408'}]\n",
        "=> in this case, we compare the utterance with the previous one with the first time value and the utterance with the following one with the second time value\n",
        "\n",
        "- Second type / no time value\n",
        "We keep the utterance, since it's often a very short one\n",
        "\n",
        "- Third type (majority) / 'time': {'@id': 'T45E', '@value': '00:05:00,615'}\n",
        "We use the time value to compare with previous and following utterances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir1ulVS7cW0h"
      },
      "outputs": [],
      "source": [
        "voc = dictionaries.copy()\n",
        "dialogs = []\n",
        "dialog = []\n",
        "time_0 = voc[0]['time']\n",
        "if type(time_0) == list:\n",
        "  dialog.append(voc[0]['#text'])\n",
        "  time_previous = datetime.strptime(time_0[1]['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "if type(time_0) == dict:\n",
        "  dialog.append(voc[0]['#text'])\n",
        "  time_previous = datetime.strptime(time_0['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "\n",
        "  \n",
        "for k in range(1, len(voc)):\n",
        "\n",
        "  try:\n",
        "    time_k = voc[k]['time']\n",
        "    if type(time_k) == list:\n",
        "      time_k_value = datetime.strptime(time_k[0]['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "      if abs((time_k_value - time_previous).total_seconds()) <= 6:\n",
        "        dialog.append(voc[k]['#text'])\n",
        "      else:\n",
        "        dialogs.append(dialog)\n",
        "        dialog = []\n",
        "      time_previous = datetime.strptime(time_k[1]['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "\n",
        "    if type(time_k) == dict:\n",
        "      time_k_value = datetime.strptime(time_k['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "      if abs((time_k_value - time_previous).total_seconds()) <= 6:\n",
        "        dialog.append(voc[k]['#text'])\n",
        "      else:\n",
        "        dialogs.append(dialog)\n",
        "        dialog = []\n",
        "      time_previous = datetime.strptime(time_k['@value'].replace(' ', '').replace('.', ','), '%H:%M:%S,%f')\n",
        "\n",
        "  except :\n",
        "    try:\n",
        "      dialog.append(voc[k]['#text'])\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "\n",
        "dialogs = [l for l in dialogs if l != []]\n",
        "with open(\"./drive/MyDrive/dialogs_XXL\", \"wb\") as f:\n",
        "        pickle.dump(dialogs, f)\n",
        "\n",
        "dialogs[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUjgDqw44yLM"
      },
      "source": [
        "MAKING A TOKENIZER SPECIFIC TO OPENSUBTITLES DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBuiRLBb5ezL"
      },
      "outputs": [],
      "source": [
        "# in this cell we compile all the text we have and split it in smaller files to facilitate tokenizer training\n",
        "try:\n",
        "  os.mkdir('./textFiles4tokenizer')\n",
        "except FileExistsError:\n",
        "  print('folder exists already')\n",
        "\n",
        "text_data = []\n",
        "file_count = 0\n",
        "\n",
        "for sample in tqdm(dictionaries):\n",
        "  try:\n",
        "    sample = sample['#text'].replace('\\n', ' ')\n",
        "    text_data.append(sample)\n",
        "  except KeyError:\n",
        "    pass\n",
        "  \n",
        "  if len(text_data) == 5_000:\n",
        "    with open(f'./textFiles4tokenizer/file_{file_count}.txt', 'w') as fp:\n",
        "      fp.write('\\n'.join(text_data))\n",
        "    text_data = []\n",
        "    file_count += 1\n",
        "with open(f'./textFiles4tokenizer/file_{file_count}.txt', 'w') as fp:\n",
        "  fp.write('\\n'.join(text_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqRHl2mX9EL1"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text = True, # removes unwanted characters\n",
        "    strip_accents = False, # Accents are important in French\n",
        "    lowercase = True # upper case letters are not really important in our case\n",
        ")\n",
        "\n",
        "general_folder_path = './textFiles4tokenizer/'\n",
        "paths = []\n",
        "for file_name in os.listdir(general_folder_path):\n",
        "  paths.append(general_folder_path + file_name)\n",
        "\n",
        "# training the tokenizer\n",
        "tokenizer.train(files = paths,\n",
        "                vocab_size = 50_000, # number of tokens that we can have\n",
        "                min_frequency = 2, # min frequency so that a sequence be taken into account as a token\n",
        "                special_tokens = ['[PAD]', # \"padding\" token\n",
        "                                  '[UNK]', # \"unknown\" token\n",
        "                                  '[CLS]', # \"classifier\" token\n",
        "                                  '[SEP]', # \"separator\" token\n",
        "                                  '[MASK]'], # \"mask\" token\n",
        "                limit_alphabet = 5000,\n",
        "                wordpieces_prefix = \"##\")\n",
        "\n",
        "try:\n",
        "  os.mkdir('./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining')\n",
        "except FileExistsError:\n",
        "  print('folder exists already')\n",
        "\n",
        "tokenizer.save_model('./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DDIS3CyAMAd"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining')\n",
        "tokenizer('Je suis enthousiaste !')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kijzgqGvCF4x"
      },
      "source": [
        "## SECTION 2 / DEFINING THE ARCHITECTURE OF THE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Creating our FastText model (compared to word 2 vec, FastText has the advantage for us of taking into account punctuation marks)"
      ],
      "metadata": {
        "id": "2Zhca8D5GMm5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLvrwmAF2aOf"
      },
      "outputs": [],
      "source": [
        "# only run this cell the first time ! then just LOAD the FastText model from Drive (next cell)\n",
        "sentences = []\n",
        "for dialog in dialogs:\n",
        "  for sentence in dialog:\n",
        "    sentences.append(sentence.split(' '))\n",
        "\n",
        "model = FastText(sentences, min_count = 3)\n",
        "model.save('./drive/MyDrive/FastTextModel/custom_FastTextModel')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading our FastText model"
      ],
      "metadata": {
        "id": "b2vgFAzCF3q9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l87CXMWyvfL1"
      },
      "outputs": [],
      "source": [
        "fastText_model = FastText.load('./drive/MyDrive/FastTextModel/custom_FastTextModel')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the embedder to feed input as embeddings of size 100 (small because of limited computation capacity)"
      ],
      "metadata": {
        "id": "GznM0NVtGgV5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSfMqjF4BJul"
      },
      "outputs": [],
      "source": [
        "def embedder(token_ids):\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_ids)    \n",
        "    return torch.stack([torch.tensor(fastText_model.wv[w]) for w in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming our tokenized data by padding the utterances and conv and building Dataset class"
      ],
      "metadata": {
        "id": "aLz-wo3Q6a3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenSubtitlesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      item = {\n",
        "          \"conversation\": self.data[idx]['conversation']\n",
        "      }\n",
        "   \n",
        "      return item"
      ],
      "metadata": {
        "id": "DvW3zalJysL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only run that cell the first time to pad dialogs ! then just run the next cell\n",
        "\n",
        "Padding dialogs (as token ids lists for the moment):"
      ],
      "metadata": {
        "id": "QyrUFch0VGo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/opensbtitles_dialogs4pretraining', 'rb') as f:\n",
        "  dialogs4pretraining = pickle.load(f)\n",
        "\n",
        "# we initialize max_size_of_utterance and max_dialog_size values:\n",
        "max_size_of_utterance = 0\n",
        "max_dialog_size = 0\n",
        "\n",
        "dialogs4pretraining_padded = dialogs4pretraining.copy()\n",
        "\n",
        "# computing maximum sizes for padding (dialogs WILL be sparse with many utterances set to [])\n",
        "for d in range(len(dialogs4pretraining_padded)):\n",
        "  if max_dialog_size < len(dialogs4pretraining_padded[d]):\n",
        "    max_dialog_size = len(dialogs4pretraining_padded[d])\n",
        "\n",
        "  for u in range(len(dialogs4pretraining_padded[d])):\n",
        "    if max_size_of_utterance < len(dialogs4pretraining_padded[d][u]):\n",
        "      max_size_of_utterance = len(dialogs4pretraining_padded[d][u])\n",
        "\n",
        "\n",
        "print('max dialog size : ', max_dialog_size, 'max size of utterance : ', max_size_of_utterance)\n",
        "# Comments : \n",
        "    # max_dialog_size is way too big for the RAM : we are going to have to cut longer dialogs\n",
        "    # max_size_of_utterance is reasonable, but since except a few utterances most of them are below 30 we cut to 30"
      ],
      "metadata": {
        "id": "X8e_uBe_7Fs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell only if padded dialogs do not already exist in Drive (else go to next cell)"
      ],
      "metadata": {
        "id": "RYmuQxnfrkYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_dialog_size = 10\n",
        "max_size_of_utterance = 30\n",
        "\n",
        "dialogs4pretraining_padded = dialogs4pretraining.copy()\n",
        "padding_utterance = [0 for i in range(max_size_of_utterance)]\n",
        "\n",
        "for d in range(len(dialogs4pretraining_padded)):\n",
        "\n",
        "  if len(dialogs4pretraining_padded[d]) < max_dialog_size:\n",
        "\n",
        "    # <PAD> token is n°0 for utterances\n",
        "    for u in range(len(dialogs4pretraining_padded[d])):\n",
        "      if len(dialogs4pretraining_padded[d][u]) < max_size_of_utterance:\n",
        "        dialogs4pretraining_padded[d][u] = dialogs4pretraining_padded[d][u] + [0 for i in range(len(dialogs4pretraining_padded[d][u]), max_size_of_utterance)]\n",
        "    # <PAD> token is [padding_utterance, padding_utterance, ...] for dialogs\n",
        "    dialogs4pretraining_padded[d] = dialogs4pretraining_padded[d] + [padding_utterance for i in range(len(dialogs4pretraining_padded[d]), max_dialog_size)] \n",
        "  \n",
        "  elif len(dialogs4pretraining_padded[d]) > max_dialog_size: \n",
        "    \n",
        "    # in this case we cut the dialog to max_dialog_size # of utterances\n",
        "    dialogs4pretraining_padded[d] = dialogs4pretraining_padded[d][:max_dialog_size]\n",
        "    # <PAD> token is n°0 for remaining utterances\n",
        "    for u in range(len(dialogs4pretraining_padded[d])):\n",
        "      if len(dialogs4pretraining_padded[d][u]) < max_size_of_utterance:\n",
        "        dialogs4pretraining_padded[d][u] = dialogs4pretraining_padded[d][u] + [0 for i in range(len(dialogs4pretraining_padded[d][u]), max_size_of_utterance)]\n",
        "\n",
        "with open(\"./drive/MyDrive/opensbtitles_dialogs4pretraining_padded_small\", \"wb\") as f:\n",
        "        pickle.dump(dialogs4pretraining_padded, f)"
      ],
      "metadata": {
        "id": "9WFw_vPDXe4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the autoencoder layers model"
      ],
      "metadata": {
        "id": "G4Db0MhuJDRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwWRNJimGZo5"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, nhead, dim_feedforward, vocab_size):\n",
        "\n",
        "        super(Model, self).__init__()   \n",
        "        self.first_layer_encoders = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)   \n",
        "        self.second_layer_encoder = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.decoder = nn.TransformerDecoderLayer(d_model, nhead)\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "\n",
        "  def forward(self, conversation, tgt):\n",
        "\n",
        "      conversation = [embedder(token_ids).to(device) for token_ids in conversation]                  #First each token of each utterance is embedded with word2vec\n",
        "      tgt = embedder(tgt).to(device)                                                                 #We do the same for the target, which corresponds to one given utterance\n",
        "\n",
        "      sentence_hidden_states = [self.first_layer_encoders(sentence) for sentence in conversation]    #Then each utterance is feeded into seperate transformer encoders in the first layer of the NN\n",
        "      corpus_hidden_states = torch.cat(sentence_hidden_states, dim=0)                                #We retrieve and concatenate the outputs\n",
        "      second_layer = self.second_layer_encoder(corpus_hidden_states)                                 #The hidden representation of the conversation is then feeded into one last transformer encoder\n",
        "      output = self.decoder(tgt, second_layer)                                                       #We decode the output of the previous layer using the target\n",
        "      pre_logits = self.linear(output)                                                               #Finally, we train a Dense layer to perform classification\n",
        "\n",
        "      return pre_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qZFPXMKcYk4u"
      },
      "outputs": [],
      "source": [
        "model_not_pretrained = Model(100, 10, 100, tokenizer.vocab_size).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a function which is going to mask part of conversation to allow the autoencoder to learn to predict parts of conversations"
      ],
      "metadata": {
        "id": "uEua7traJMna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxzPEbA6LjR_"
      },
      "outputs": [],
      "source": [
        "def masker(conversation, mask_ratio):\n",
        "  \"\"\" \n",
        "  This function is used to return a masked version of the conversation at the token level as well as a masked version at the utterance level\n",
        "\n",
        "  Input: \n",
        "       - conversation: the conversation to mask, as a list of a list of tokens (a list of tokenized utterances)\n",
        "       - mask_ratio: the probability to mask a given token / utterance\n",
        "  \n",
        "  Output:\n",
        "       - masked_input is the conversation masked at the tokens level (at least one word per utterance)\n",
        "       - masked_dialogue is the conversation masked at the utterance level (at least one utterance)\n",
        "       - target_conv is the original conversation\n",
        "       - masks is a list of the indices of the masked tokens for each utterance\n",
        "       - n_masked is the total number of tokens (including the tokens from the masked utterance) that were masked\n",
        "  \"\"\"\n",
        "  target_conv = []\n",
        "  masked_input = []\n",
        "  masks = []\n",
        "  n_masked = 0\n",
        "\n",
        "\n",
        "  #First, we mask conversations at the utterance level\n",
        "\n",
        "  #We do not want to mask padded utterance (it wouldn't be interested to decode them), so we start to identify relevant utterance\n",
        "  non_padded_indexes = [i for i in range(len(conversation)) if conversation[i] != padding_utterance]\n",
        "\n",
        "  #Then, using the mask_ratio as a probability, we pick utterances to mask fully\n",
        "  mask_uterance = torch.rand(len(non_padded_indexes)) < mask_ratio  \n",
        "  mask_uterance = [non_padded_indexes[i] for i in range(len(non_padded_indexes)) if i in torch.nonzero(mask_uterance)]\n",
        "\n",
        "  #If no utterance was picked (it can happen when mask_ratio is low), we make sur to mask at least one by picking it randomly\n",
        "  if len(mask_uterance) == 0:\n",
        "    mask_uterance = torch.randint(len(non_padded_indexes), (1, 1))\n",
        "    mask_uterance = [non_padded_indexes[mask_uterance]]\n",
        "\n",
        "  #Finally, we fully mask the tokens of the selected utterances\n",
        "  masked_dialogue = conversation.copy()\n",
        "  for index in mask_uterance:\n",
        "    masked_dialogue[index] = [tokenizer.mask_token_id for uterance in masked_dialogue[index]]\n",
        "    n_masked += len(masked_dialogue[index])\n",
        "\n",
        "  #Then, we mask conversations at the token level\n",
        "\n",
        "  for uterance in conversation:\n",
        "\n",
        "    #We do not want to mask padded tokens (it wouldn't be interested to decode them), so we start to identify relevant tokens for each utterance\n",
        "    non_padded_indexes = [i for i in range(len(uterance)) if uterance[i] != 0]\n",
        "\n",
        "    #Then, using the mask_ratio as a probability, we pick tokens to mask fully for each utterance\n",
        "    mask_indices = torch.rand(len(non_padded_indexes)) < mask_ratio  \n",
        "    mask_indices = [non_padded_indexes[i] for i in range(len(non_padded_indexes)) if i in torch.nonzero(mask_indices)]\n",
        "\n",
        "    #If no tokens was picked (it can happen when mask_ratio is low), we make sur to mask at least one by picking it randomly\n",
        "    if len(mask_indices) == 0 and uterance != padding_utterance:\n",
        "      mask_indices = torch.randint(len(non_padded_indexes), (1, 1))\n",
        "      mask_indices = [non_padded_indexes[mask_indices]]\n",
        "    n_masked += len(mask_indices)\n",
        "    input_tensor = torch.tensor(uterance)\n",
        "    masks += [torch.tensor(mask_indices).to(device)]\n",
        "\n",
        "    #Finally, we mask the selected tokens for each utterance\n",
        "    masked_input_tensor = input_tensor.clone()\n",
        "    masked_input_tensor[mask_indices] = tokenizer.mask_token_id\n",
        "    masked_input += [masked_input_tensor.to(device)]\n",
        "\n",
        "    target_tensor = input_tensor.clone()\n",
        "    target_conv += [target_tensor.to(device)]\n",
        "  \n",
        "  \n",
        "  return masked_input, target_conv, masks, masked_dialogue, n_masked\n",
        "\n",
        "\n",
        "def base_mlm_loss(logits, target, mask):\n",
        "  \"\"\" \n",
        "  This function defines the base MLM loss for masked parts prediction. It is an implementation of the usual MLM loss for one given utterance (the one corresponding to the target)\n",
        "\n",
        "  Input: \n",
        "    - logits: the logits of the model when it's called with tgt = target\n",
        "    - target: the unmasked utterance of interest for which we want to compute the MLM loss\n",
        "    - mask: the indices of the masked tokens for this utterance of interest\n",
        "\n",
        "  Ouput: \n",
        "    - mlm_loss_val: the MLM loss on this utterance\n",
        "  \"\"\"\n",
        "\n",
        "  #The usual MLM loss can be seen as a cross-entropy loss that focuses on the masked tokens. We use reduction = sum because we will normalize it later\n",
        "  #We filter the target on the masked tokens and compute the cross-entropy loss on these tokens\n",
        "  mlm_loss = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "  logits = torch.squeeze(logits[mask], dim=1)\n",
        "  target = target[mask]\n",
        "  if target.shape[0] != 1:\n",
        "    target = torch.squeeze(target, dim=-1)\n",
        "  if len(logits) > 0:\n",
        "    mlm_loss_val = mlm_loss(logits, target)\n",
        "  else: \n",
        "    print('this should no longer be possible')\n",
        "  return mlm_loss_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hZs2-stXKoQ"
      },
      "outputs": [],
      "source": [
        "# That part of the code is inspired by the hierarchical loss defined in the paper\n",
        "# Hierarchical Pre-training for Sequence Labelling in Spoken Dialog, Chapuis, Colombo et al. \n",
        "\n",
        "# defining hyperparameters. \n",
        "#mask_ratio is the proportion of tokens/utterances we want to mask, lamb_u and lamb_c weight the contribution of the loss for masked tokens and the loss for masked utterances\n",
        "mask_ratio = 0.2\n",
        "lamb_u = 0.5\n",
        "lamb_c = 0.5\n",
        "\n",
        "# using parameters used for padding the opensubtitles data previously\n",
        "max_dialog_size = 10\n",
        "max_size_of_utterance = 30\n",
        "padding_utterance = [0 for i in range(max_size_of_utterance)]\n",
        "padding_dialog = [padding_utterance for k in range(max_dialog_size)]\n",
        "\n",
        "\n",
        "def mlm_loss(model, conversation, mask_ratio, lamb_u, lamb_c ):\n",
        "  \"\"\"\n",
        "  This function is in charge of computing a weighted mean between the usual MLM loss at the masked tokens level and its generalization to the masked utterance level. \n",
        "  It is the loss that our model will use for its training.\n",
        "\n",
        "  Input: \n",
        "    - model: an instance of our hierarchical autoencoder Model\n",
        "    - conversation: a sample of conversation from our database, under the format of a list of list of tokens_ids (a list of tokenized utterances)\n",
        "    - mask_ratio: the proportion of tokens/utterances that we want to mask\n",
        "    - lamb_u: the importance of the MLM loss computed at the masked tokens level\n",
        "    - lamb_c: the importance of the MLM loss computed at the masked utterance level\n",
        "  \n",
        "  Output: \n",
        "    - a weighted mean of the MLM loss computed at the masked tokens level and the MLM loss computed at the masked utterance level\n",
        "  \"\"\"\n",
        "\n",
        "  if conversation == padding_dialog:\n",
        "    return 0\n",
        "\n",
        "\n",
        "  #First, we compute the MLM loss at the masked tokens level. \n",
        "  #This is the usual defintion of the MLM loss and we just need to apply the base MLM loss function we defined to all the utterances\n",
        "\n",
        "  #We begin by calling our masker function in order to have a version of the conversation where at least one tokens per utterance was masked\n",
        "  masked_input, target_conv, masks, masked_dialogue, n_masked = masker(conversation, 0.15)\n",
        "  losses = []\n",
        "\n",
        "  for index in range(len(conversation)):\n",
        "    #We iterate over all the utterances to focus on different targets and we identify the tokens that were masked to each utterance\n",
        "    target = target_conv[index]\n",
        "    mask = masks[index]\n",
        "    \n",
        "    #We train the model for each target of interest and get the probabilites associated with each token\n",
        "    logits = model(masked_input, target.to(device))   # shape: number of words of uterance of interest * vocab => a prediction is available for each word\n",
        "    logits = logits.view(-1, tokenizer.vocab_size)\n",
        "    target = target.view(-1).to(device)\n",
        "\n",
        "    #We feed the logits, the indices of the masked utterance and the original words into our MLM loss function to get the error\n",
        "    try:\n",
        "      mlm_loss_val = base_mlm_loss(logits, target, mask) # cross entropy loss while only considering the masked word compared to true value\n",
        "      losses += [mlm_loss_val]\n",
        "    except IndexError:\n",
        "      pass\n",
        "  \n",
        "  # the MLM loss at the masked token level is the sum of the losses for each utterance.\n",
        "  base_MLM = sum(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Second, we compute the MLM loss at the masked utterance level. In this scenario, we'll try to predict all the tokens from fully masked utterances\n",
        "  \n",
        "  #In that scenario, we'll consider the conversation as one single utterance where some tokens (all the tokens from some given utterances were masked)\n",
        "  target_conv_full = torch.cat(target_conv)\n",
        "\n",
        "  #We train the model on the full dialogue and get the logits\n",
        "  logits_full = model(masked_dialogue, target_conv_full.to(device))\n",
        "  logits_full = logits_full.view(-1, tokenizer.vocab_size)\n",
        "  target_conv_full = target_conv_full.view(-1).to(device)\n",
        "\n",
        "  #We get the indices of the masked tokens (all the tokens from some given utterances) and compute the MLM loss on it\n",
        "  mask_conv = (torch.cat([torch.tensor(el) for el in masked_dialogue]) == tokenizer.mask_token_id)\n",
        "  mlm_loss_full = base_mlm_loss(logits_full, target_conv_full, mask_conv)\n",
        "\n",
        "  #We compute the weighted loss between the two\n",
        "  mlm_loss = lamb_u * base_MLM + lamb_c * mlm_loss_full\n",
        "  return mlm_loss/n_masked"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [[2054, 2003, 2605, 3007], [1045, 2228, 2009, 2003, 3000]]\n",
        "mlm_loss(model_not_pretrained, conversation, mask_ratio, lamb_u, lamb_c)"
      ],
      "metadata": {
        "id": "HSom7lKnZ6yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 3 / PRETRAINING OF THE AUTOENCODER"
      ],
      "metadata": {
        "id": "BMJyKFVey_-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we define a few functions to be able to handle batches of conversations:"
      ],
      "metadata": {
        "id": "IVtIy9b27ttv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converts conversation as list of strings to list of token ids, as required by the model\n",
        "def conv_converter(conversation_):\n",
        "\n",
        "  conversation = []\n",
        "  for sentence in conversation_ :\n",
        "    conversation.append(tokenizer(sentence)['input_ids'])\n",
        "    \n",
        "  return conversation\n",
        "\n",
        "\n",
        "# batch treatment to feed into the model\n",
        "def from_batch_to_conv(batch, max_dialog_size = max_dialog_size, max_size_of_utterance = max_size_of_utterance):\n",
        "\n",
        "  batch_size = len(batch[0][0])\n",
        "  conversations = []\n",
        "\n",
        "  for k in range(batch_size):\n",
        "    conversation = []\n",
        "    for u in range(max_dialog_size):\n",
        "      utterance = []\n",
        "      for n in range(max_size_of_utterance):\n",
        "        utterance.append(int(batch[u][n][k]))\n",
        "\n",
        "      conversation.append(utterance)\n",
        "    conversations.append(conversation)\n",
        "\n",
        "  return conversations\n",
        "\n",
        "\n",
        "# wrapped up computation of MLM loss for a whole batch, we sum the losses obtained on each conversation\n",
        "def batch_loss(batch):\n",
        "\n",
        "  loss = 0\n",
        "  for conversation in batch:\n",
        "    loss += mlm_loss(model, conversation, mask_ratio, lamb_u, lamb_c)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "m0AxTJrjbCY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define the function in charge of the training of the model.\n",
        "\n",
        "We only do the pretraining on one epoch at once because colab will never allow us to do multiple epochs without interrupting the execution\n",
        "\n",
        "We configure the function to be able to get back to pretraining after interruption thanks to intermediate savings"
      ],
      "metadata": {
        "id": "QneAM25ZOwxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_epoch_pretrain(model, optimizer, begin = 0, epoch_loss = 0):\n",
        "\n",
        "  # set the model into a training mode : the model's weights and parameters WILL BE updated!\n",
        "  model.train()\n",
        "\n",
        "  # define the iterator over batches defined by train_loader\n",
        "  it = iter(train_loader)\n",
        "\n",
        "  # bringing the iterator to where we where before shutdown (thanks colab)\n",
        "  if begin > 0:\n",
        "    try:\n",
        "      next(islice(it, begin, begin + 1))\n",
        "    except RuntimeError:\n",
        "      pass\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "\n",
        "\n",
        "  # for each valid batch (very rare RunTimeError so try condition)\n",
        "  for i in range(begin, train_loader.__len__()):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    if int((i/train_loader.__len__())*100) != int(((i-1)/train_loader.__len__())*100):\n",
        "      # intermediate saving ! colab is unstable...\n",
        "      with open(\"./drive/MyDrive/pretraining_params_saving\", \"wb\") as f:\n",
        "        pickle.dump({'epoch_loss' : epoch_loss, 'iter' : i}, f)\n",
        "      torch.save(model, './drive/MyDrive/model')\n",
        "      clear_output()\n",
        "      print(int((i/train_loader.__len__())*100), ' % of epoch')\n",
        "\n",
        "    try:\n",
        "\n",
        "      batch = next(it)\n",
        "      batch = from_batch_to_conv(batch['conversation'])\n",
        "      train_loss = batch_loss(batch)\n",
        "        \n",
        "      # compute accumulated gradients\n",
        "      train_loss.backward()\n",
        "      # perform parameter update based on current gradients\n",
        "      optimizer.step()\n",
        "      # add the mini-batch training loss to epoch_loss  \n",
        "      epoch_loss += train_loss.item()\n",
        "\n",
        "      print(train_loss.item())\n",
        "\n",
        "    # sometimes RuntimeError happen due to batch_size parameter in train_loader ! \n",
        "    # Rarely with batch_size = 3, and all the time with batch_size = 10 for instance\n",
        "    except RuntimeError:\n",
        "      pass\n",
        "      # KeyError skipped on this batch (very special character)\n",
        "    except KeyError:\n",
        "      pass"
      ],
      "metadata": {
        "id": "CDNSnrO8MFmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the pretraining data and convert it to a Dataloader object for batch-wise training:"
      ],
      "metadata": {
        "id": "3X_AEZPMPO4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/opensbtitles_dialogs4pretraining_padded_small', 'rb') as f:\n",
        "  dialogs4pretraining_padded = pickle.load(f)\n",
        "\n",
        "dialogs4pretraining_padded = pd.DataFrame({\n",
        "     \"conversation\" : dialogs4pretraining_padded\n",
        "})\n",
        "\n",
        "# only training on 1/3 of dialogs, too big otherwise\n",
        "dialogs4pretraining_padded = dialogs4pretraining_padded[dialogs4pretraining_padded.index < \n",
        "                                                        np.percentile(dialogs4pretraining_padded.index, 33)]\n",
        "\n",
        "from datasets import Dataset\n",
        "dialogs4pretraining_padded = OpenSubtitlesDataset(Dataset.from_dict(dialogs4pretraining_padded))\n",
        "\n",
        "\n",
        "# Creating DataLoader for OpenSubtitles dataset \n",
        "# loading train_loader, please define batch_size!\n",
        "train_loader = DataLoader(dataset = dialogs4pretraining_padded, batch_size = 3,\n",
        "                          num_workers=2, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "JW2a1N__chsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If an intermediary saved version exist, we load it with its weights:"
      ],
      "metadata": {
        "id": "mwfcnLqoSw6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('./drive/MyDrive/model')\n",
        "\n",
        "with open('./drive/MyDrive/pretraining_params_saving', 'rb') as f:\n",
        "  params_b4_stop = pickle.load(f)\n",
        "loss = params_b4_stop['epoch_loss']\n",
        "begin_mark = params_b4_stop['iter']"
      ],
      "metadata": {
        "id": "EDFa0z7PaZxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we run the training, using the stochastic gradient descent as an optimizer:"
      ],
      "metadata": {
        "id": "8E-yx9i-9B5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an optimizer object for SGD\n",
        "optimizer = opt.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# getting back to where we where before colab decided to stop the session\n",
        "one_epoch_pretrain(model, optimizer, begin = begin_mark, epoch_loss = loss)"
      ],
      "metadata": {
        "id": "mXi9iQygeb6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1st epoch cumul. MLM loss : __119 397__\n",
        "* 2nd epoch cumul. MLM loss : __52 462__\n",
        "* 3rd epoch cumul. MLM loss : __34 029__\n",
        "* Estimated cumul. MLM loss for epoch 4 (not enough computation units to run the epoch, estimation based on 5% of pretrain set) : __25 180__"
      ],
      "metadata": {
        "id": "rBNc0jQkTpiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 4 / TRAINING OF THE MODEL FOR MOVIES REVIEWS CLASSIFICATION"
      ],
      "metadata": {
        "id": "KvaX0tUH9IQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we fine-tune the model on movie reviews to see how well the pretraining on dialogues generalizes to intent classification on written paragraphs.\n",
        "\n",
        "We use data from allociné since our pretraining was made on French dialogues. This dataset is available on the PyTorch library."
      ],
      "metadata": {
        "id": "LCS6g-h49LgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cinema_reviews = load_dataset(\"allocine\")\n",
        "cinema_reviews"
      ],
      "metadata": {
        "id": "Uvv1svnG7oB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset = cinema_reviews['train'], batch_size = 10,\n",
        "                          num_workers=2, shuffle=True, drop_last=True)\n",
        "\n",
        "example_batch = next(iter(train_loader))\n",
        "example_batch['review'][0]"
      ],
      "metadata": {
        "id": "5gbwXtdDKp5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to preprocess this new data. We use the same techniques as for the pretraining: tokenization with the same pretrained tokenizer, padding to have consistent sizes in the reviews and storing of the reviews as lists of lists of tokens (list of tokenized sentences)"
      ],
      "metadata": {
        "id": "DhdQ6iXn-L6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('./drive/MyDrive/custom_opensbtitles_tokenizer_XXLtraining')\n",
        "max_dialog_size = 5\n",
        "max_size_of_utterance = 30\n",
        "\n",
        "def preprocessing(batch, max_dialog_size, max_size_of_utterance):\n",
        "\n",
        "  batch_preprocessed = []\n",
        "  for review in batch['review']:\n",
        "\n",
        "    # We begin by splitting the reviews into a list of sentences, using the nltk library\n",
        "    correct_spacing_after_dot = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', review)                           # making sure a whitespace separate sentences\n",
        "    list_of_sentences = nltk.tokenize.sent_tokenize(correct_spacing_after_dot, language='french')\n",
        "\n",
        "    # We tokenize each sentence using our pretrained tokenizer\n",
        "    tokenized_list = [tokenizer(el.lower())['input_ids'] for el in list_of_sentences]\n",
        "    padding_utterance = [0 for i in range(max_size_of_utterance)]\n",
        "    for i in range(len(tokenized_list)):\n",
        "      if len(tokenized_list[i]) >= max_size_of_utterance:\n",
        "        tokenized_list[i] = tokenized_list[i][:max_size_of_utterance]\n",
        "      else: \n",
        "        tokenized_list[i] = tokenized_list[i] + [0 for i in range(len(tokenized_list[i]), max_size_of_utterance)]\n",
        "        \n",
        "    # Finally, we use padding to have the same number of sentences in each review and the same number of tokens in each sentences\n",
        "    padded_list = tokenized_list[:max_dialog_size] if len(tokenized_list) >= max_dialog_size else tokenized_list + [padding_utterance for i in range(len(tokenized_list), max_dialog_size)]\n",
        "    batch_preprocessed += [padded_list]\n",
        "\n",
        "  return batch_preprocessed"
      ],
      "metadata": {
        "id": "12JoXz9cJJgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's now time to define our new model to perform classification on movie reviews. We reuse the encoding layers from the pretrained model and add a MLP on top of it:"
      ],
      "metadata": {
        "id": "G8hFlXNS_XOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Version of the pretrained model with classification layer\n",
        "\n",
        "class FinalModel(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(FinalModel, self).__init__()\n",
        "\n",
        "    self.first_layer_encoders = model.first_layer_encoders\n",
        "    self.second_layer_encoder = model.second_layer_encoder\n",
        "    self.classif_layer = nn.Linear(100 * max_size_of_utterance, 2)\n",
        "  \n",
        "  def forward(self, conversation):\n",
        "    \n",
        "    #We feed our new data into the pretrained model\n",
        "    conversation = [embedder(token_ids).to(device) for token_ids in conversation]\n",
        "    sentence_hidden_states = [self.first_layer_encoders(sentence) for sentence in conversation]\n",
        "    corpus_hidden_states = torch.cat(sentence_hidden_states, dim=0)\n",
        "    second_layer = self.second_layer_encoder(corpus_hidden_states)\n",
        "\n",
        "    #We finetune the model with an additionnal MLP\n",
        "    second_layer = second_layer.reshape(max_dialog_size, max_size_of_utterance*100)    #After retrieving the hidden embedding produced by the pretrain model, we reshape it to have one line per sentence\n",
        "    output = self.classif_layer(second_layer)                                          #Then we use a Dense layer to have an output of dimension \"number of sentences x 2\" so that each sentence can contribute to the probability\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "fBTfnZrUKlSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "final_model = FinalModel(model)\n",
        "final_model.to(device)"
      ],
      "metadata": {
        "id": "NutqYxObNKku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We finally define our loss and the training function to optimize the model"
      ],
      "metadata": {
        "id": "S9G1pdf1_4j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def batch_loss_classif(batch):\n",
        "  \"\"\"\n",
        "  A function to define our loss at the batch level.\n",
        "  It is defined as the sum of the losses obtained on each review. \n",
        "\n",
        "  The model outputs a matrix of size \"number of sentences x 2\" that contains the votes of each sentence for the opinion of the review (0 for the first column, 1 in the second)\n",
        "  We turn the votes into probabilities, sum them and apply the cross-entropy loss\n",
        "  \"\"\"\n",
        "\n",
        "  target = batch['label']\n",
        "\n",
        "  # We turn the reviews from teh batches into the format expected by the model\n",
        "  x = preprocessing(batch, max_dialog_size, max_size_of_utterance)\n",
        "\n",
        "  # initialize batch loss\n",
        "  loss = 0\n",
        "\n",
        "  # We iterate over each review in the batch\n",
        "  for el, targ in zip(x, target): \n",
        "    \n",
        "    try:\n",
        "      torch.manual_seed(0)\n",
        "\n",
        "      #We run the model with each review\n",
        "      out = final_model(el)\n",
        "\n",
        "      #We make the columns sum to one (each line / sentence gives a proba for 0 and a proba for 1)\n",
        "      probas = F.softmax(out, dim=1)                \n",
        "\n",
        "      #We sum the votes, use the cross-entropy loss (the PyTorch version takes care of applying the softmax) and add it to batch loss\n",
        "      votes = torch.sum(probas, axis=0).to(device)\n",
        "      loss += loss_fn(votes, torch.tensor(targ).to(device)) \n",
        "\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "7lVwsqIttQ4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimization of parameters, using SGD and the loss defined\n",
        "\n",
        "def classif_trainer(final_model, num_epoch = 10, max_norm = 1, epoch_b4_stopping = 0, epoch_losses = [], classif_layer_only = True, clipping = True):\n",
        "\n",
        "  if epoch_b4_stopping != 0:\n",
        "    epoch_losses = epoch_losses\n",
        "  else:\n",
        "    epoch_losses = [0 for i in range(num_epoch)]\n",
        "\n",
        "  if classif_layer_only == True:\n",
        "    for param in final_model.parameters():\n",
        "      param.requires_grad = False\n",
        "    for param in final_model.classif_layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "  optimizer = opt.SGD(final_model.parameters(), lr=0.001, momentum=0.9)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  # training mode \"train\" only changes the \"dropout\" or \"batchnorm\" layers\n",
        "  final_model.train()\n",
        "\n",
        "  # main loop (train+test)\n",
        "  for epoch in tqdm(range(epoch_b4_stopping, num_epoch)):\n",
        "\n",
        "      if epoch == epoch_b4_stopping:\n",
        "        epoch_losses[epoch] = 0 \n",
        "\n",
        "      loss_100_batches = 0\n",
        "      \n",
        "      for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "          # Clear previous gradients in the graph\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # compute batch loss\n",
        "          loss = batch_loss_classif(batch)\n",
        "\n",
        "          # Compute clipped gradients\n",
        "          loss.backward()\n",
        "          if clipping == True:\n",
        "            utils.clip_grad_norm(final_model.parameters(), max_norm)\n",
        "\n",
        "          epoch_losses[epoch] += loss.item()\n",
        "          loss_100_batches += loss.item()\n",
        "\n",
        "          # Apply Gradient descent\n",
        "          optimizer.step()\n",
        "          \n",
        "          if batch_idx %100 ==0:\n",
        "              print('epoch {} batch {} [{}/{}] training loss: {}'.format(epoch,batch_idx,batch_idx*10,\n",
        "                      len(train_loader.dataset),loss_100_batches))\n",
        "              loss_100_batches = 0\n",
        "              \n",
        "          if batch_idx %1000 == 0:\n",
        "            with open(\"./drive/MyDrive/final_training_params_saving\", \"wb\") as f:\n",
        "              pickle.dump({'epoch_losses': epoch_losses, 'epoch_b4_stopping' : epoch}, f)\n",
        "            torch.save(final_model, './drive/MyDrive/final_model')\n",
        "\n",
        "  with open(\"./drive/MyDrive/final_training_epoch_losses\", \"wb\") as f:\n",
        "              pickle.dump(epoch_losses, f)\n",
        "  torch.save(final_model, './drive/MyDrive/final_model')"
      ],
      "metadata": {
        "id": "EDBC9cMRrSaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "final_model = torch.load('./drive/MyDrive/final_model')"
      ],
      "metadata": {
        "id": "9qZpJBw_nyfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classif_trainer(final_model, num_epoch = 30)"
      ],
      "metadata": {
        "id": "MHfsUVPPHVAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /!\\ run this cell if training was interrupted by colab /\n",
        "with open('./drive/MyDrive/final_training_params_saving', 'rb') as f:\n",
        "  final_training_params_saving = pickle.load(f)\n",
        "\n",
        "final_model = torch.load('./drive/MyDrive/final_model')\n",
        "classif_trainer(final_model, num_epoch = 30, epoch_b4_stopping = final_training_params_saving['epoch_b4_stopping'],\n",
        "                epoch_losses = final_training_params_saving['epoch_losses'], classif_layer_only = True)"
      ],
      "metadata": {
        "id": "QVZz1xEcmKsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/final_training_epoch_losses', 'rb') as f:\n",
        "  training_losses = pickle.load(f)\n",
        "\n",
        "plt.scatter(x = [k for k in range(len(training_losses))], y = training_losses)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross entropy loss for classif.')\n",
        "plt.title('Evolution of training loss through epochs')"
      ],
      "metadata": {
        "id": "6_SiLupRSDK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 5 / EVALUATION OF THE MODEL ON MOVIES REVIEWS CLASSIFICATION TASK"
      ],
      "metadata": {
        "id": "pbbiYKUPP-IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing by bootstrapping 1000 batches in test dataset (= 10 000 reviews in total) :\n",
        "final_model.eval()\n",
        "\n",
        "bootstrap_number = 50\n",
        "bootstrap_size = 20\n",
        "bootstrapped_accuracies = []\n",
        "\n",
        "for boot in range(bootstrap_number):\n",
        "\n",
        "  test_loader = DataLoader(dataset = cinema_reviews['test'], batch_size = 10,\n",
        "                            num_workers=2, shuffle=True, drop_last=True)\n",
        "  right_answers = 0\n",
        "  true_test_size = 0 # few KeyError so we discount them\n",
        "\n",
        "  for i in range(bootstrap_size):\n",
        "    try:\n",
        "      batch = next(iter(test_loader))\n",
        "      true_label = batch['label'][0]\n",
        "      probas = F.softmax(final_model(preprocessing(batch, max_dialog_size, max_size_of_utterance)[0]), dim = 1)\n",
        "      votes = torch.sum(probas, axis=0)\n",
        "\n",
        "      if votes[0] < votes[1]:\n",
        "        pred = 1 \n",
        "      else:\n",
        "        pred = 0\n",
        "\n",
        "      if pred == true_label:\n",
        "        right_answers += 1\n",
        "      \n",
        "      true_test_size += 1\n",
        "    \n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "  bootstrapped_accuracies.append(right_answers/true_test_size)"
      ],
      "metadata": {
        "id": "015qNAeEXV1y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n, bins, patches = plt.hist(x = bootstrapped_accuracies, bins = 'auto', color = '#0504aa',\n",
        "                            alpha = 0.7, rwidth = 0.85)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Bootstrapped accuracy values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of bootstrapped accuracies')\n",
        "maxfreq = n.max()\n",
        "# Set a clean upper y-axis limit.\n",
        "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
      ],
      "metadata": {
        "id": "GVV5AUenGi6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average accuracy over bootstrapped sets of reviews :\n",
        "print('Average accuracy over bootstrapped sets of samples = ', int(np.mean(bootstrapped_accuracies)*100), '%')"
      ],
      "metadata": {
        "id": "3Zq78P0-g6SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 6 / ASSESSMENT OF THE IMPACT OF PRETRAINING ON CLASSIFICATION "
      ],
      "metadata": {
        "id": "enZfEO5NPg2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we want to know how the classification model would have behaved if no pretraining (hierarchical model with MLM loss) had been performed before training the classification layer weights:\n",
        "* In the following cells we instanciate the pretraining model without any MLM loss training\n",
        "* After that we train the classification layer weights for 30 epochs, as we did for our pretrained model, and then we evaluate the accuracy of this not pretrained classification model"
      ],
      "metadata": {
        "id": "pen19vG6PxbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "final_model = FinalModel(model_not_pretrained)\n",
        "final_model.to(device)"
      ],
      "metadata": {
        "id": "fSq_tAaaPwZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimization of parameters of classif layer \n",
        "\n",
        "def classif_trainer_2(final_model, num_epoch = 10, max_norm = 1, epoch_b4_stopping = 0, epoch_losses = [], classif_layer_only = True, clipping = True):\n",
        "\n",
        "  if epoch_b4_stopping != 0:\n",
        "    epoch_losses = epoch_losses\n",
        "  else:\n",
        "    epoch_losses = [0 for i in range(num_epoch)]\n",
        "\n",
        "  if classif_layer_only == True:\n",
        "    for param in final_model.parameters():\n",
        "      param.requires_grad = False\n",
        "    for param in final_model.classif_layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "  optimizer = opt.SGD(final_model.parameters(), lr=0.001, momentum=0.9)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  # training mode \"train\" only changes the \"dropout\" or \"batchnorm\" layers\n",
        "  final_model.train()\n",
        "\n",
        "  # main loop (train+test)\n",
        "  for epoch in tqdm(range(epoch_b4_stopping, num_epoch)):\n",
        "\n",
        "      if epoch == epoch_b4_stopping:\n",
        "        epoch_losses[epoch] = 0 \n",
        "\n",
        "      loss_100_batches = 0\n",
        "      \n",
        "      for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "          # Clear previous gradients in the graph\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # compute batch loss\n",
        "          loss = batch_loss_classif(batch)\n",
        "\n",
        "          # Compute clipped gradients\n",
        "          loss.backward()\n",
        "          if clipping == True:\n",
        "            utils.clip_grad_norm(final_model.parameters(), max_norm)\n",
        "\n",
        "          epoch_losses[epoch] += loss.item()\n",
        "          loss_100_batches += loss.item()\n",
        "\n",
        "          # Apply Gradient descent\n",
        "          optimizer.step()\n",
        "          \n",
        "          if batch_idx %100 ==0:\n",
        "              print('epoch {} batch {} [{}/{}] training loss: {}'.format(epoch,batch_idx,batch_idx*10,\n",
        "                      len(train_loader.dataset),loss_100_batches))\n",
        "              loss_100_batches = 0\n",
        "              \n",
        "          if batch_idx %1000 == 0:\n",
        "            with open(\"./drive/MyDrive/final_training_params_saving_no_pretraining\", \"wb\") as f:\n",
        "              pickle.dump({'epoch_losses': epoch_losses, 'epoch_b4_stopping' : epoch}, f)\n",
        "            torch.save(final_model, './drive/MyDrive/final_model_no_pretraining')\n",
        "\n",
        "  with open(\"./drive/MyDrive/final_training_epoch_losses_no_pretraining\", \"wb\") as f:\n",
        "              pickle.dump(epoch_losses, f)\n",
        "  torch.save(final_model, './drive/MyDrive/final_model_no_pretraining')"
      ],
      "metadata": {
        "id": "4k7YfUfHRwfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classif_trainer_2(final_model, num_epoch = 30)"
      ],
      "metadata": {
        "id": "Mri2ITF2STRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /!\\ run this cell if training was interrupted by colab /\n",
        "with open('./drive/MyDrive/final_training_params_saving_no_pretraining', 'rb') as f:\n",
        "  final_training_params_saving = pickle.load(f)\n",
        "\n",
        "final_model = torch.load('./drive/MyDrive/final_model_no_pretraining')\n",
        "classif_trainer_2(final_model, num_epoch = 30, epoch_b4_stopping = final_training_params_saving['epoch_b4_stopping'],\n",
        "                epoch_losses = final_training_params_saving['epoch_losses'], classif_layer_only = True)"
      ],
      "metadata": {
        "id": "cXomWTveUt5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the evolution of the training loss throughout the epochs :"
      ],
      "metadata": {
        "id": "-NK5aXjIj6gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/final_training_epoch_losses_no_pretraining', 'rb') as f:\n",
        "  training_losses = pickle.load(f)\n",
        "\n",
        "plt.scatter(x = [k for k in range(len(training_losses))], y = training_losses)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross entropy loss for classif.')\n",
        "plt.title('Evolution of training loss through epochs ; no pretraining')"
      ],
      "metadata": {
        "id": "JQY9LHpGjqrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the accuracy of the classification model after 30 training epochs but WITHOUT PRETRAINING :"
      ],
      "metadata": {
        "id": "xo0q6KASkKvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing by bootstrapping 1000 batches in test dataset (= 10 000 reviews in total) :\n",
        "final_model = torch.load('./drive/MyDrive/final_model_no_pretraining')\n",
        "final_model.eval()\n",
        "\n",
        "bootstrap_number = 50\n",
        "bootstrap_size = 20\n",
        "bootstrapped_accuracies = []\n",
        "\n",
        "for boot in range(bootstrap_number):\n",
        "\n",
        "  test_loader = DataLoader(dataset = cinema_reviews['test'], batch_size = 10,\n",
        "                            num_workers=2, shuffle=True, drop_last=True)\n",
        "  right_answers = 0\n",
        "  true_test_size = 0 # few KeyError so we discount them\n",
        "\n",
        "  for i in range(bootstrap_size):\n",
        "    try:\n",
        "      batch = next(iter(test_loader))\n",
        "      true_label = batch['label'][0]\n",
        "      probas = F.softmax(final_model(preprocessing(batch, max_dialog_size, max_size_of_utterance)[0]), dim = 1)\n",
        "      votes = torch.sum(probas, axis=0)\n",
        "\n",
        "      if votes[0] < votes[1]:\n",
        "        pred = 1 \n",
        "      else:\n",
        "        pred = 0\n",
        "\n",
        "      if pred == true_label:\n",
        "        right_answers += 1\n",
        "      \n",
        "      true_test_size += 1\n",
        "    \n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "  bootstrapped_accuracies.append(right_answers/true_test_size)\n",
        "\n",
        "\n",
        "# average accuracy over bootstrapped sets of reviews :\n",
        "print('Average accuracy over bootstrapped sets of samples WITHOUT PRETRAINING = ', int(np.mean(bootstrapped_accuracies)*100), '%')"
      ],
      "metadata": {
        "id": "iNBI8lwbTft2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}